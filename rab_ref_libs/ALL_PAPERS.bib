@InProceedings{ison2022,
  author   = {ison and because the two work processes‚Äîindex operation and may be erroneously attributed to a weakness in the index and output scanning‚Äîcan be worked on separately. The and but is actually a weakness or misuse of the display. and frequent confounding of these operations by the user In summary and the two steps of index using and display and is because the index-couples and which are at the heart of scanning are often confused and particularly in subordinate and index using and frequently carry the display also and as with indexes. It is submitted that distinguishing between and card catalogs or subject heading indexes such as Chemical and these two work processes results in better system design. and Abstracts. Thus and the user subconsciously proceeds from and using the index-couples to using the display and to which and the former have guided him. This and intertwining of the REFERENCES},
  title    = {The Generation of a Unique Machine Description for Chemical Structures-A Technique Developed at Chemical Abstracts Service.},
  year     = {2022},
  file     = {:/home/fabrice/Documents/library/papers/1965_Morgan_CONF.pdf:PDF},
  keywords = {article doi: 10.1021/c160017a018, Article metadata: Journal of Chemical Documentation_5_2_10.1021/c160017a018_107_113},
}

@InProceedings{WEISFEILER,
  author   = {B.YU. WEISFEILER and A.A. LEMAN},
  title    = {THE REDUCTION OF A GRAPH TO CANONICAL FORM AND THE ALGEBRA WHICH APPEARS THEREIN},
  abstract = {We consider an algorithm for the reduction of a given finite multigraph Œì to
canonical form. Therein the new invariant of a graph appears ‚Äî the algebra A(Œì). The
study of properties of the algebra A(Œì) turns out to be helpful in solving a number of
graph-theoretic problems. We pose and discuss some conjectures on the relation between
properties of the algebra A(Œì) and the automorphism group Aut(Œì) of a graph Œì. We give
an example of undirected graph Œì whose algebra A(Œì) coincides with the group algebra of
some noncommutative group.},
  file     = {:/home/fabrice/Documents/library/papers/1968_Weisfeiler_CONF.pdf:PDF},
}

@InProceedings{ULLMANN1976,
  author   = {J. R. ULLMANN},
  title    = {An Algorithm for Subgraph Isomorphism},
  year     = {1976},
  abstract = {Subgraph isomorphism can be determined by means of a brute-force tree-search enu-
meration procedure. In this paper a new algorithm is introduced that attains efficiencyb y inferentially 
eliminatings uccessor nodes in the tree search. To assess the time actually taken by the new algomthm, 
subgraph isomorphism, chque detection, graph isomorphism, and directed graph isomorphism ex- 
periments have been carried out with random and with various nonrandom graphs.},
  file     = {:/home/fabrice/Documents/library/papers/1976_Ullmann_CONF.pdf:PDF},
}

@InProceedings{LaszloBabai1980,
  author   = {L√°szl√≥ Babai, Paul ErdoÀùs, and Stanley M. Selkow},
  title    = {Random Graph Isomorphism | SIAM Journal on Computing | Vol. 9, No. 3 | Society for Industrial and Applied Mathematics},
  year     = {1980},
  abstract = {SIAM J. Comput. 1980.9:628-635},
  file     = {:/home/fabrice/Documents/library/papers/1980_Babai_SIAM.pdf:PDF},
  keywords = {graph,isomorphism testing,canonical labeling,random graph,naive algorithm,average-case analysis,linear time,degree sequence of a graph},
}

@Misc{Mitchell,
  author   = {Tom M. Mitchell},
  title    = {Machine Learning},
  abstract = {Machine Learning},
  file     = {:/home/fabrice/Documents/library/papers/1983_Mitchell_BOOK.pdf:PDF},
}

@InProceedings{ScaleSpace1990,
  author    = {Scale-Space and Edge and Detection Using and Anisotropic and Diffusion},
  title     = {Scale-space and edge detection using anisotropic diffusion - Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  year      = {1990},
  publisher = {IEEE},
  file      = {:/home/fabrice/Documents/library/papers/1990_Perona_IEEE.pdf:PDF},
}

@InProceedings{1646mg1991,
  author   = {dryness to give 16 (46 mg and 85%). The solid was recrystallized from cubator at 37 ¬∞C for 72 h (or for 96 h for Table III and IV) in and EtOH-EtjO: mp and ¬∞C (sintered and from and ¬∞C); MS and m/z and 1 mL of medium containing various concentrations of test com},
  title    = {Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity},
  year     = {1991},
  file     = {:/home/fabrice/Documents/library/papers/1991_Debnath_CONF.pdf:PDF},
  keywords = {article doi: 10.1021/jm00106a046, Article metadata: Journal of Medicinal Chemistry_34_2_10.1021/jm00106a046_786_797},
}

@InProceedings{Graph8603,
  author = {Variables for Graph and Identification},
  title  = {An Optimal Lower Bound on the Number of},
  year   = {8603},
  file   = {:/home/fabrice/Documents/library/papers/1992_Cai_IEEE.pdf:PDF},
}

@InProceedings{Wilkinson1992,
  author = {Leland Wilkinson},
  title  = {Tree Structured Data Analysis},
  year   = {1992},
  file   = {:/home/fabrice/Documents/library/papers/1992_Wilkinson_CONF.pdf:PDF},
}

@InProceedings{NorthHolland1992,
  author   = {North-Holland},
  title    = {PII: 0020-0190(92)90136-J},
  year     = {1992},
  file     = {:/home/fabrice/Documents/library/papers/1992_Zhang_CONF.pdf:PDF},
  keywords = {Computational complexity, unordered trees},
}

@InProceedings{Zhang0046,
  author = {Kaizhong Zhang},
  title  = {A new editing based distance between unordered labeled trees},
  year   = {0046},
  file   = {:/home/fabrice/Documents/library/papers/1993_Zhang_CONF.pdf:PDF},
}

@InProceedings{TimeSeries,
  author = {TimeSeries},
  title  = {convo.dvi},
  file   = {:/home/fabrice/Documents/library/papers/1995_LeCun_CONF.pdf:PDF},
}

@InProceedings{Learning2985,
  author    = {Gradient-Based Learning and Applied to Document and Recognition},
  title     = {See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication},
  year      = {2985},
  publisher = {IEEE},
  file      = {:/home/fabrice/Documents/library/papers/1998_Lecun_IEEE.pdf:PDF},
}

@InProceedings{eeepmrartmeDnatvoidf,
  author   = {eeepmrartmeDnatvoidf and : htstiaSptiya:l/:n/othwfaaCwuCaswrslui.lf},
  title    = {Convolution UKCerSnCel-sCoRnLD-9i9sc-1re0te Structures UnDURLiv},
  abstract = {July 8, 1.uc9sc9es9.cu.ecdscu./edhuaussler
agGoa
Wrefpenerpaelididrneiaitsdatroibbs pcolrrirbet},
  file     = {:/home/fabrice/Documents/library/papers/1999_Haussler_CONF.pdf:PDF},
}

@InProceedings{intersectionkernelforimageclassification4044,
  author    = {Histogram intersection kernel for image classification},
  title     = {See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication},
  year      = {4044},
  publisher = {IEEE},
  file      = {:/home/fabrice/Documents/library/papers/2003_Barla_CONF.pdf:PDF},
}

@InProceedings{nupeoicneehaiwetrsrgaet2002,
  author = {nupeoicneehaiwetrsrgaet and of the ce and atsBttdteeioohhioenqedepvreol},
  title  = {LaRpleadcuiacntioEn nitioOn MikhaiilgaBenenldkminDapastafoPRrareDtphairmeNsieeynnogstiiayotnioanlity tLccfotDtohiooonaorremnpancasrwllphieautld},
  year   = {2002},
  file   = {:/home/fabrice/Documents/library/papers/2003_Belkin_IEEE.pdf:PDF},
}

@InProceedings{Report2003,
  author = {Technical Report},
  title  = {Frequent Sub-Structure-Based Approaches for Classifying Chemical Compounds},
  year   = {2003},
  file   = {:/home/fabrice/Documents/library/papers/2003_Deshpande_TECH_REPORT.pdf:PDF},
}

@InProceedings{Enzyme1016,
  author   = {Distinguishing Enzyme and Structures from Non-enzymes and Without Alignments},
  title    = {doi:10.1016/S0022-2836(03)00628-4},
  year     = {1016},
  doi      = {g@umist.ac.uk},
  file     = {:/home/fabrice/Documents/library/papers/2003_Dobson_CONF.pdf:PDF},
  keywords = {protein function prediction; structure; enzyme; machine *Corresponding author learning; structural genomics},
}

@InProceedings{ThomasGaertner2214,
  author   = {Thomas G√§rtner, Peter Flach, and Stefan Wrobel},
  title    = {LNAI 2777 - On Graph Kernels: Hardness Results and Efficient Alternatives},
  year     = {2214},
  abstract = {Learning Theory and Kernel Machines},
  file     = {:/home/fabrice/Documents/library/papers/2003_Gaertner_CONF.pdf:PDF},
}

@InProceedings{Hsu1995,
  author   = {Chih-Wei Hsu and Chih-Chung Chang and Chih-Jen Lin},
  title    = {A Practical Guide to Support Vector Classification},
  year     = {1995},
  abstract = {The support vector machine (SVM) is a popular classification technique.},
  file     = {:/home/fabrice/Documents/library/papers/2003_Hsu_CONF.pdf:PDF},
}

@InProceedings{HisashiKashima2003,
  author    = {Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi},
  title     = {Marginalized Kernels Between Labeled Graphs},
  year      = {2003},
  publisher = {IEEE},
  abstract  = {Proceedings of the Twentieth International Conference on Machine Learning},
  file      = {:/home/fabrice/Documents/library/papers/2003_Kashima_CONF.pdf:PDF},
  keywords  = {Compilation copyright ¬©2003, American Association for Artificial Intelligence. All rights reserved.},
}

@InProceedings{Nijssen2333,
  author   = {Siegfried Nijssen and Joost N. Kok},
  title    = {Efficient Discovery of Frequent Unordered Trees},
  year     = {2333},
  abstract = {Recently, an algorithm called Freqt was introduced which
enumerates all frequent induced subtrees in an ordered data tree. We
propose a new algorithm for mining unordered frequent induced sub-
trees. We show that the complexity of enumerating unordered trees is
not higher than the complexity of enumerating ordered trees; a strategy
for determining the frequency of unordered trees is introduced.},
  file     = {:/home/fabrice/Documents/library/papers/2003_Nijssen_CONF.pdf:PDF},
}

@InProceedings{1,
  author = {1},
  title  = {graph_kernels.dvi},
  file   = {:/home/fabrice/Documents/library/papers/2003_Ramon_CONF.pdf:PDF},
}

@InProceedings{Genetic1906,
  author = {Spline-Fitting with a Genetic and Algorithm: A and Method for Developing and Classification and Structure-Activity Relationships},
  title  = {No Job Name},
  year   = {1906},
  file   = {:/home/fabrice/Documents/library/papers/2003_Sutherland_CONF.pdf:PDF},
}

@InProceedings{Pattern2004,
  author   = {Cyclic Pattern and Kernels for Predictive and Graph Mining},
  title    = {p234_horvath.dvi},
  year     = {2004},
  abstract = {1. INTRODUCTION
With applications in biology, the world-wide web, and sev- In recent years, data mining has moved far beyond the},
  file     = {:/home/fabrice/Documents/library/papers/2004_Horvath_KDD.pdf:PDF},
  keywords = {tured graphs while being efficient enough to be applied to},
}

@InProceedings{Efficient9972,
  author    = {An Efficient and Algorithm for and Discovering Frequent and Subgraphs},
  title     = {To appear in IEEE Transactions on Knowledge and Data Engineering},
  year      = {9972},
  publisher = {IEEE},
  abstract  = {Over the years, frequent itemset discovery algo- formulating the frequent pattern discovery problem is as that
rithms have been used to find interesting patterns in various of discovering subgraphs that occur frequently over the entire
application areas. However, as data mining techniques are being set of graphs.
increasingly applied to non-traditional domains, existing frequent
pattern discovery approach cannot be used. This is because The power of graphs to model complex datasets has been},
  file      = {:/home/fabrice/Documents/library/papers/2004_Kuramochi_IEEE.pdf:PDF},
}

@InProceedings{Mahe2004,
  author   = {Pierre Mahe and pierre.mahe@ensmp.fr},
  title    = {Extensions of Marginalized Graph Kernels},
  year     = {2004},
  abstract = {tial drugs, require the analysis, comparison, and clas-
sification of these graphs. Among the many different},
  file     = {:/home/fabrice/Documents/library/papers/2004_Mahe_ICML.pdf:PDF},
}

@{Nijssen2333a,
  abstract = {'  Q 	'I"=/
,-5"$
 %U0'/ ,'7/ '  v'	n  	 %b-, 
3 ,%5,   	   
  
 	
 			   	
  	! 	
#"$ ! 	%& '('
)" *	+'-, 
  	 ="C	< 3
 S[  ,   < P	%&  ' (' 
 $" ¬î7D 0 I m¬î:' L  '      FLu 
'm '&
.  # 
/  0'/
,' 
/ '  1 '  	+2 '	 2' 	 ' 0.34, &0   	  ' 2  /, 4 	 2 "$5 76 L\ !     0' &	 'P' 
L?&  % ('  ¬î I¬ï¬ñ ¬ä¬í¬ë¬ó¬ì'¬åh¬ò¬é b¬å ¬äm¬ã ¬ô!' P 	 ',= m  
 8" / 9" :'*<; / 
 ,3<
 "$ =" /
"9, 
>?7 ,&@ "$5
 =" /A"  ' -' 0 . '  ;
/   < ! 0'/,-' /
'   !< 3$ < '     	+' $
	 . ' *;</  P 	+'v'  # 	  
	 B C" 	D7=" / "E:'*;</  
,3<FHGID
	$" 
 % J  = /0 '/
,'/
'K 
6 	 K "$
1 '8 &
 
	%& '  ' 
 "¬ö,*	 % %*R{@|U}~?U ¬ÄNF
,%5/
L 	
   '   I	8 	' NF MO' 
  P/
0 '
/ , ' 
/ '  Q"C	 <3 uL 
"C	 5i-, 	% %  @K '
 
  %5 "$ < 1O ' 	 %& '( '
"
"$ 
 5
 R 	%&' '
"$S 	*C?  T 7 '&?*U WF VXW 7  S'4"C	Y  
+\¬õ'C %(n/K'
@
 , +3J7 '< ,  25 < 'C   - 	N% 
  	 
 	 
Z "$  
$ $" '@ $[  ,   < * ?\L8
 '?&8'] / 
'  R 0^ <; 
/ 5,<Y 6 $[ ,5 '(% <3 
F Vg
* 	 %5%(3<j ' = 	 % & '( '¬ú"   /5% 4 ?   *	 @ %5&Y S 	d?-6
0 	+4 
 ' , 5
 % _`D7%  '
 a'  Rb 	,J'+	 J 'c 	'  /J /7 6  , 5	% * C. '  ' $" ¬î 1\    $ m	  , * C\1(' ] :'   2 '   +	  		   
0 ' 
/ ,' /
'  d +	 ' J , &<	5 
 *e  ` *	-, e '  - *FfVga'
 J* 	',e .  7/ = 
 &/
:% ¬ù 	%5Z ?i	% C ' c 7 * 	 P% 1\ (' T  	  = 	+	  	   S : 6

/ 
 6h0 '/ -, '
/ '   1>'0i +	 ' d	 'R, & 
 :7'*j 'k 	'
 i	+' >,5 ' %(3<¬ÉF VX`' K 	 ?*2\Lc
* ¬û\ *q\ '
5 i,*	 B?c 7 &
 & 
 	0 .' "$*` 'l '   i	 e> 	%%3a'  i	'J	
0m' "$ k' 	T \PK* \¬É'	+$ 'i	
% 5 , 	 '  &`n' K¬ü/,7Y 0 	+$
 '76
	 
  4F VgZ'  @ L\ 	 3l& 
C*, 	 W/
 ]"$ '$ [$ ,  <=	% &'(' $"  ,  %@,*	4 7  * K"C	 Y@	]7 (¬†j' ,8J ?  .'C" 	
 , &Fol@/
.# ' 
8  $" %@/ 
 0  '/ ,'7/ '  n	J& 
% K3 /8 ' 
= 	
	
, *4	 (% 6 @{ |U} ~?U ¬Ä$'n$"   P'
L x#¬°PV¬¢ ¬£¬£n	+	 	 !m   :' * ;</  <I 
/ 0 '/,-6
 '( '"$]1\   k ' 
  f3 	' 4 '* 	5% %(3f
  *
   UpF olR 5 "$
%   $"  < '*  ' /
'!	] 
/ , ,  0 ./% (% 3d, &"$ +	 ' 2'
 '$" $[ ,    
 ,C3  Q & /
!	 (% 6
	l
\q2  2r 
?s&t * ;</  
, + s uv'  J D	,-'. w zx Xy @{ U| }~?U v¬Ä ¬Å$	5%  6 &' ( '  "¬É\1( '4	= /"=?- 1 I 0	 '-X6 .¬í6 ' -6g+	 0 '/ ,'7/ '#$" 5

'('
"p	 *8 &S'I
 ' 
  ,  % 	@\L1
 '  '  
/ % ' &="$57 6 	 % '('$"  F ol
   *\¬§' 	2 & /7n 	5%   ' (' "H,*	 R	5%  ]* 	5%(34? 
 
 $=	 5% 	' @$" %  , / 5% 	+ 
 		 	F /    $' 8
d¬è ¬ä ¬•2¬ç-¬é'¬ç-¬ã ¬ë ¬çP$" 5
  #D7?  '"$ <' $ 1\  ,-CL\ 2, &"S6},
  author   = {Siegfried Nijssen and Joost N. Kok and LIACS and Leiden University LIACS and Leiden University},
  file     = {:/home/fabrice/Documents/library/papers/2004_Nijssen_CONF.pdf:PDF},
  keywords = {2r 8 -, Y  '/T Y *\1%*7 & d. C  / S ?  = \Li,&/%5¬ù $"   6 ¬Ç  J"$  7 I 0' / , ' 7/ '    "$(X6 0 ' / , ' / ' I     I.' ; /  < ' &W d%: +' &C/"8?#L/  %5 , '&  FiVXZ '# ? \LS\1 %% ('  ¬É" -' &  (% 3d'.  1 ' ='  , 1  /  %5 ,*+ '& ¬îF M},
  title    = {A Quickstart in Frequent Structure Mining can make a Difference},
  year     = {2333},
}

@InProceedings{BRENDA2004,
  author   = {BRENDA and the enzyme database: updates and major and new developments and Ida Schomburg and Antje Chang and Christian Ebeling and Marion Gremse and Christian Heldt},
  title    = {gkh081 431..433},
  year     = {2004},
  abstract = {progress of projects of structural and functional genomics and
metabolomics, the systematic collection, accessibility and},
  file     = {:/home/fabrice/Documents/library/papers/2004_Schomburg_CONF.pdf:PDF},
}

@Article{Bille2005,
  author    = {Philip Bille},
  journal   = {Theoretical Computer Science},
  title     = {A survey on tree edit distance and related problems},
  year      = {2005},
  month     = {jun},
  number    = {1-3},
  pages     = {217--239},
  volume    = {337},
  doi       = {10.1016/j.tcs.2004.12.030},
  file      = {:/home/fabrice/Documents/library/papers/2005_Bille_TCS.pdf:PDF},
  keywords  = {Tree matching; Tree edit distance; Tree alignment; Tree inclusion},
  publisher = {Elsevier {BV}},
}

@Article{Borgwardt2005,
  author    = {K. M. Borgwardt and C. S. Ong and S. Schonauer and S. V. N. Vishwanathan and A. J. Smola and H.-P. Kriegel},
  journal   = {Bioinformatics},
  title     = {Protein function prediction via graph kernels},
  year      = {2005},
  month     = {jun},
  number    = {Suppl 1},
  pages     = {i47--i56},
  volume    = {21},
  abstract  = {known function is consequently the basis of current function
Motivation: Computational approaches to protein function prediction (Whisstock and Lesk, 2003). A newly discovered
prediction infer protein function by finding proteins with sim- protein is predicted to exert the same function as the most
ilar sequence, structure, surface clefts, chemical properties, similar proteins in a database of known proteins. This simil-
amino acid motifs, interaction partners or phylogenetic pro- arity among proteins can be defined in a multitude of ways:
files. We present a new approach that combines sequential, two proteins can be regarded to be similar, if their sequences
structural and chemical information into one graph model of align well [e.g. PSI-BLAST (Altschul et al., 1997)], if their
proteins. We predict functional class membership of enzymes structures match well [e.g. DALI (Holm and Sander, 1996)],
and non-enzymes using graph kernels and support vector if both have common surface clefts or bindings sites [e.g.
machine classification on these protein graphs. CASTp (Binkowski et al., 2003)], similar chemical fea-
Results: Our graph model, derivable from protein sequence tures or common interaction partners [e.g. DIP (Xenarios
and structure only, is competitive with vector models that et al., 2002)], if both contain certain motifs of amino acids
require additional protein information, such as the size of (AAs) [e.g. Evolutionary Trace (Yao et al., 2003)] or if both
surface pockets. If we include this extra information into our appear in the same range of species (Pellegrini et al., 1999).
graph model, our classifier yields significantly higher accuracy An armada of protein function prediction systems that meas-
levels than the vector models. Hyperkernels allow us to select ure protein similarity by one of the conditions above has been
and to optimally combine the most relevant node attributes in developed. Each of these conditions is based on a biological
our protein graphs. We have laid the foundation for a protein hypothesis; e.g. structural similarity implies that two proteins
function prediction system that integrates protein information could share a common ancestor and that they both could per-
from various sources efficiently and effectively. form the same function as this common ancestor (Bartlett
Availability: More information available via www.dbs.ifi.lmu. et al., 2003).
de/Mitarbeiter/borgwardt.html. These assumptions are not universally valid. Hegyi and
Contact: borgwardt@dbs.ifi.lmu.de Gerstein (1999) showed that proteins with similar function},
  doi       = {10.1093/bioinformatics/bti1007},
  file      = {:/home/fabrice/Documents/library/papers/2005_Borgwardt_CONF.pdf:PDF},
  publisher = {Oxford University Press ({OUP})},
}

@InProceedings{Borgwardt8053,
  author   = {Karsten M. Borgwardt and Hans-Peter Kriegel and Institute for Computer and Science},
  title    = {Shortest-path kernels on graphs},
  year     = {8053},
  abstract = {this purpose. However, kernels on these substructures are
either computationally expensive, sometimes even NP-hard},
  file     = {:/home/fabrice/Documents/library/papers/2005_Borgwardt_IEEE.pdf:PDF},
}

@InProceedings{Histogram4186,
  author    = {Generalized Histogram and Intersection Kernel and for Image and Recognition},
  title     = {See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication},
  year      = {4186},
  publisher = {IEEE},
  file      = {:/home/fabrice/Documents/library/papers/2005_Boughorbel_IEEE.pdf:PDF},
}

@InProceedings{Assignment2213,
  author = {Optimal Assignment and Kernels for Attributed and Molecular Graphs},
  title  = {See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication},
  year   = {2213},
  file   = {:/home/fabrice/Documents/library/papers/2005_Froehlich_ICML.pdf:PDF},
}

@InProceedings{Gaston2333,
  author = {The Gaston and Tool for Frequent and Subgraph and Mining},
  title  = {GraBaTs‚Äô04 Preliminary Version},
  year   = {2333},
  file   = {:/home/fabrice/Documents/library/papers/2005_Nijssen_ENTCS.pdf:PDF},
}

@InProceedings{Yang2005,
  author   = {Rui Yang and Panos Kalnis and Anthony K. H. Tung},
  title    = {binaryFilterSeparate.dvi},
  year     = {2005},
  abstract = {and the linear representation of data. They allow the expres-},
  file     = {:/home/fabrice/Documents/library/papers/2005_Yang_CONF.pdf:PDF},
}

@Misc{Bishop,
  author = {Christopher M. Bishop},
  title  = {Pattern Recognition and Machine Learning},
  file   = {:/home/fabrice/Documents/library/papers/2006_Bishop_BOOK.pdf:PDF},
}

@InProceedings{Borgwardt2007,
  author = {Karsten Michael Borgwardt},
  title  = {Graph Kernels},
  year   = {2007},
  file   = {:/home/fabrice/Documents/library/papers/2007_Borgwardt_BOOK.pdf:PDF},
}

@InProceedings{Classification4260,
  author    = {Image Classification and with Segmentation and Graph Kernels},
  title     = {See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication},
  year      = {4260},
  publisher = {IEEE},
  file      = {:/home/fabrice/Documents/library/papers/2007_Harchaoui_IEEE.pdf:PDF},
}

@Article{Wale2007,
  author    = {Nikil Wale and Ian A. Watson and George Karypis},
  journal   = {Knowledge and Information Systems},
  title     = {Comparison of descriptor spaces for chemical compound retrieval and classification},
  year      = {2007},
  month     = {aug},
  number    = {3},
  pages     = {347--375},
  volume    = {14},
  abstract  = {In recent years the development of computational techniques that build models
to correctly assign chemical compounds to various classes or to retrieve potential drug-like
compounds has been an active area of research. Many of the best-performing techniques for
these tasks utilize a descriptor-based representation of the compound that captures various
aspects of the underlying molecular graph‚Äôs topology. In this paper we compare five dif-
ferent set of descriptors that are currently used for chemical compound classification. We
also introduce four different descriptors derived from all connected fragments present in the
molecular graphs primarily for the purpose of comparing them to the currently used des-
criptor spaces and analyzing what properties of descriptor spaces are helpful in providing
effective representation for molecular graphs. In addition, we introduce an extension to exis-
ting vector-based kernel functions to take into account the length of the fragments present
in the descriptors. We experimentally evaluate the performance of the previously introduced
and the new descriptors in the context of SVM-based classification and ranked-retrieval on
28 classification and retrieval problems derived from 18 datasets. Our experiments show that
for both of these tasks, two of the four descriptors introduced in this paper along with the
extended connectivity fingerprint based descriptors consistently and statistically outperform
previously developed schemes based on the widely used fingerprint- and Maccs keys-based
descriptors, as well as recently introduced descriptors obtained by mining and analyzing the
structure of the molecular graphs.},
  doi       = {10.1007/s10115-007-0103-5},
  file      = {:/home/fabrice/Documents/library/papers/2007_Wale_CONF.pdf:PDF},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Bach2008,
  author   = {Francis R. Bach and FRANCIS.BACH@MINES.ORG},
  title    = {Graph Kernels between Point Clouds},
  year     = {2008},
  abstract = {point clouds, with applications to classification of line
drawings‚Äîsuch as handwritten digits (LeCun et al., 1998)},
  file     = {:/home/fabrice/Documents/library/papers/2008_Bach_ICML.pdf:PDF},
}

@InProceedings{Hofmann1171,
  author   = {Thomas Hofmann, Bernhard Scholkopf, Alexander J. Smola},
  title    = {Kernel methods in machine learning},
  year     = {1171},
  abstract = {The Annals of Statistics, 2008, Vol.36, No.3, 1171-1220},
  doi      = {ng},
  file     = {:/home/fabrice/Documents/library/papers/2008_Hofmann_CONF.pdf:PDF},
}

@InProceedings{Kondor2008,
  author   = {Risi Kondor and risi@gatsby.ucl.ac.uk},
  title    = {The Skew Spectrum of Graphs},
  year     = {2008},
  abstract = {Given a graph G, the two main lines of research that
have emerged to address the above problem focus re-},
  file     = {:/home/fabrice/Documents/library/papers/2008_Kondor_ICML.pdf:PDF},
}

@Article{Mahe2008,
  author    = {Pierre Mah{\'{e}} and Jean-Philippe Vert},
  journal   = {Machine Learning},
  title     = {Graph kernels based on tree patterns for molecules},
  year      = {2008},
  month     = {oct},
  number    = {1},
  pages     = {3--35},
  volume    = {75},
  abstract  = {Motivated by chemical applications, we revisit and extend a family of positive
definite kernels for graphs based on the detection of common subtrees, initially proposed by
Ramon and G√§rtner (Proceedings of the first international workshop on mining graphs, trees
and sequences, pp. 65‚Äì74, 2003). We propose new kernels with a parameter to control the
complexity of the subtrees used as features to represent the graphs. This parameter allows to
smoothly interpolate between classical graph kernels based on the count of common walks,
on the one hand, and kernels that emphasize the detection of large common subtrees, on the
other hand. We also propose two modular extensions to this formulation. The first extension
increases the number of subtrees that define the feature space, and the second one removes
noisy features from the graph representations. We validate experimentally these new kernels
on problems of toxicity and anti-cancer activity prediction for small molecules with support
vector machines.},
  doi       = {10.1007/s10994-008-5086-2},
  file      = {:/home/fabrice/Documents/library/papers/2008_Mah√©_CONF.pdf:PDF},
  keywords  = {Graph kernels ¬∑ Support vector machines ¬∑ Chemoinformatics},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Kondor2009,
  author   = {Risi Kondor, Nino Shervashidze, Karsten M. Borgwardt},
  title    = {The graphlet spectrum},
  year     = {2009},
  abstract = {In this paper, we overcome these two limitations by},
  file     = {:/home/fabrice/Documents/library/papers/2009_Kondor_ICML.pdf:PDF},
}

@Article{Kulis2008,
  author    = {Brian Kulis and Sugato Basu and Inderjit Dhillon and Raymond Mooney},
  journal   = {Machine Learning},
  title     = {Semi-supervised graph clustering: a kernel approach},
  year      = {2008},
  month     = {sep},
  number    = {1},
  pages     = {1--22},
  volume    = {74},
  abstract  = {Semi-supervised clustering algorithms aim to improve clustering results using
limited supervision. The supervision is generally given as pairwise constraints; such con-
straints are natural for graphs, yet most semi-supervised clustering algorithms are designed
for data represented as vectors. In this paper, we unify vector-based and graph-based ap-
proaches. We first show that a recently-proposed objective function for semi-supervised
clustering based on Hidden Markov Random Fields, with squared Euclidean distance and
a certain class of constraint penalty functions, can be expressed as a special case of the
weighted kernel k-means objective (Dhillon et al., in Proceedings of the 10th International
Conference on Knowledge Discovery and Data Mining, 2004a). A recent theoretical con-
nection between weighted kernel k-means and several graph clustering objectives enables
us to perform semi-supervised clustering of data given either as vectors or as a graph. For
graph data, this result leads to algorithms for optimizing several new semi-supervised graph
clustering objectives. For vector data, the kernel approach also enables us to find clusters
with non-linear boundaries in the input data space. Furthermore, we show that recent work
on spectral learning (Kamvar et al., in Proceedings of the 17th International Joint Confer-
ence on Artificial Intelligence, 2003) may be viewed as a special case of our formulation.
We empirically show that our algorithm is able to outperform current state-of-the-art semi-
supervised algorithms on both vector-based and graph-based data sets.},
  doi       = {10.1007/s10994-008-5084-4},
  file      = {:/home/fabrice/Documents/library/papers/2009_Kulis_SpringerMachineLearning.pdf:PDF},
  keywords  = {Semi-supervised clustering ¬∑ Kernel k-means ¬∑ Graph clustering ¬∑ Spectral learning},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{FrancoScarselli2009,
  author = {Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini},
  title  = {The graph neural network model},
  year   = {2009},
  file   = {:/home/fabrice/Documents/library/papers/2009_Scarselli_IEEE.pdf:PDF},
}

@InProceedings{NinoShervashidze,
  author   = {Nino Shervashidze, Karsten M. Borgwardt},
  title    = {Fast subtree kernels on graphs},
  abstract = {Neural Information Processing Systems http://nips.cc/},
  file     = {:/home/fabrice/Documents/library/papers/2009_Shervashidze_NIPS.pdf:PDF},
}

@InProceedings{Shervashidze2003,
  author   = {Nino Shervashidze and S.V.N. Vishwanathan and Tobias H. Petri and MPI for Biological and Cybernetics Department and of Statistics and Institute for Computer and Science and MPI for Developmental and Biology Purdue and University LMU and MuÃànchen},
  title    = {Efficient graphlet kernels for large graph comparison},
  year     = {2003},
  abstract = {Frequent subgraph mining algorithms, on the other
hand, aim to detect subgraphs that are frequent in
a given dataset of graphs. Afterwards, feature selec-},
  file     = {:/home/fabrice/Documents/library/papers/2009_Shervashidze_PMLR.pdf:PDF},
}

@InProceedings{tmatsui2207,
  author = {tmatsui},
  title  = {Microsoft Word - icassp_tomoko_081002.docx},
  year   = {2207},
  file   = {:/home/fabrice/Documents/library/papers/2009_Vert_IEEE.pdf:PDF},
}

@InProceedings{FabrizioCosta2010,
  author   = {Fabrizio Costa, Kurt De Grave},
  title    = {Fast Neighborhood Subgraph Pairwise Distance Kernel},
  year     = {2010},
  abstract = {performance (Ben-David et al., 2002). Possible reme-},
  file     = {:/home/fabrice/Documents/library/papers/2010_Costa_ICML.pdf:PDF},
  keywords = {graph kernel, chemoinformatics, molecular graph},
}

@InProceedings{Fakultaet4422,
  author = {Fakult√§t and f√ºr Informatik and Algorithm Engineering and (LS 11) and 44221 Dortmund and / Germany and http://ls11-www.cs.uni-dortmund.de/},
  title  = {Erweiterte Substruktursuche in Molek√ºldatenbanken und ihre Integration in Scaffold Hunter Nils Kriege Algorithm Engineering Report TR10-1-001 Februar 2010 ISSN},
  year   = {4422},
  file   = {:/home/fabrice/Documents/library/papers/2010_Kriege_DISSERTATION.pdf:PDF},
}

@InProceedings{Bonneel2011,
  author   = {Nicolas Bonneel, Michiel van de Panne, Sylvain Paris, Wolfgang Heidrich},
  title    = {Displacement Interpolation Using Lagrangian Mass Transport},
  year     = {2011},
  abstract = {1 Introduction},
  doi      = {=},
  file     = {:/home/fabrice/Documents/library/papers/2011_Bonneel_SIGGRAPH.pdf:PDF},
  keywords = {displacement interpolation, mass transport rection is, subjectively speaking, better defined as having a single},
}

@Article{Chang2011,
  author    = {Chih-Chung Chang and Chih-Jen Lin},
  journal   = {{ACM} Transactions on Intelligent Systems and Technology},
  title     = {{LIBSVM}},
  year      = {2011},
  month     = {apr},
  number    = {3},
  pages     = {1--27},
  volume    = {2},
  doi       = {10.1145/1961189.1961199},
  file      = {:/home/fabrice/Documents/library/papers/2011_Chang_ACM.pdf:PDF},
  publisher = {Association for Computing Machinery ({ACM})},
}

@InProceedings{Graph2011,
  author   = {Weisfeiler-Lehman Graph and Kernels},
  title    = {shervashidze11a.dvi},
  year     = {2011},
  file     = {:/home/fabrice/Documents/library/papers/2011_Shervashidze_JMLR.pdf:PDF},
  keywords = {graph kernels, graph classification, similarity measures for graphs, Weisfeiler-Lehman algorithm},
}

@InProceedings{Wang2011,
  author   = {Haizhou Wang and Mingzhou Song},
  title    = {Ckmeans.1d.dp: Optimal <em>k</em>-means Clustering in One Dimension by Dynamic Programming},
  year     = {2011},
  abstract = {pose a nuisance for further steps.
The heuristic k-means algorithm, widely used In response to this challenge, our objective is to de-
for cluster analysis, does not guarantee optimal- velop a practical and convenient clustering algorithm
ity. We developed a dynamic programming al- in R to guarantee optimality in a one-dimensional
gorithm for optimal one-dimensional clustering. (1-D) space. The motivation originated from discrete
The algorithm is implemented as an R package system modeling such as Boolean networks (Kauff-
called Ckmeans.1d.dp. We demonstrate its ad- man, 1969, 1993) and generalized logical networks
vantage in optimality and runtime over the stan- (Song et al., 2009), where continuous values must
dard iterative k-means algorithm. be converted to discrete ones before modeling. Our},
  file     = {:/home/fabrice/Documents/library/papers/2011_Wang_CONF.pdf:PDF},
}

@Article{Jovanovic2012,
  author    = {Irena Jovanovi{\'{c}} and Zoran Stani{\'{c}}},
  journal   = {Linear Algebra and its Applications},
  title     = {Spectral distances of graphs},
  year      = {2012},
  month     = {mar},
  number    = {5},
  pages     = {1425--1435},
  volume    = {436},
  abstract  = {Linear Algebra and Its Applications, 436 (2012) 1425-1435. doi:10.1016/j.laa.2011.08.019},
  doi       = {10.1016/j.laa.2011.08.019},
  file      = {:/home/fabrice/Documents/library/papers/2012_Jovanovic_ELSEVIER.pdf:PDF},
  keywords  = {Spectral distance, Adjacency matrix, Graph energy, Cospectrality measure, Spectral diameter},
  publisher = {Elsevier {BV}},
}

@InProceedings{NilsKriege2012,
  author   = {Nils Kriege, Petra Mutzel},
  title    = {Subgraph Matching Kernels for Attributed Graphs},
  year     = {2012},
  abstract = {Proceedings of the International Conference on Machine Learning 2010},
  file     = {:/home/fabrice/Documents/library/papers/2012_Kriege_CONF.pdf:PDF},
  keywords = {graph matching, common subgraph, graph kernel, machine learning, ICML},
}

@InProceedings{Publisher2013,
  author   = {Academy Publisher},
  title    = {template.doc},
  year     = {2013},
  abstract = {This paper presents a new approach to The clustering problems can be categorized into two},
  file     = {:/home/fabrice/Documents/library/papers/2013_Abubaker_CONF.pdf:PDF},
}

@InProceedings{Bruna1312,
  author = {Joan Bruna and Wojciech Zaremba and New York and University New and York University},
  title  = {Spectral Networks and Deep Locally Connected Networks on Graphs},
  year   = {1312},
  file   = {:/home/fabrice/Documents/library/papers/2013_Bruna_CONF.pdf:PDF},
}

@Article{Oury2013,
  author    = {Nicolas Oury and Michael Pedersen and Rasmus Petersen},
  journal   = {Electronic Proceedings in Theoretical Computer Science},
  title     = {Canonical Labelling of Site Graphs},
  year      = {2013},
  month     = {jun},
  pages     = {13--28},
  volume    = {116},
  doi       = {10.4204/eptcs.116.3},
  file      = {:/home/fabrice/Documents/library/papers/2013_Oury_EPTCS.pdf:PDF},
  publisher = {Open Publishing Association},
}

@InProceedings{Graph1210,
  author   = {Improved Graph and Clustering and Yudong Chen and Sujay Sanghavi and Huan Xu},
  year     = {1210},
  abstract = {Graph clustering involves the task of dividing nodes ‚Ä¢ Small density gap: the edge density across clusters is
into clusters, so that the edge density is higher within clusters only a small additive or multiplicative factor different
as opposed to across clusters. A natural, classic and popular from within clusters;
statistical setting for evaluating solutions to this problem is the
stochastic block model, also referred to as the planted partition ‚Ä¢ Sparsity: the graph is overall very sparse even within
model. clusters;},
  file     = {:/home/fabrice/Documents/library/papers/2014_Chen_CONF.pdf:PDF},
}

@InProceedings{Wang1007,
  author   = {Zhe Wang and Xiangyang Xue},
  title    = {Chapter 2 Multi-Class Support Vector Machine},
  year     = {1007},
  abstract = {Support vector machine (SVM) was initially designed for binary
classification. To extend SVM to the multi-class scenario, a number of classification
models were proposed such as the one by Crammer and Singer (J Mach Learn Res
2:265‚Äì292, 2001). However, the number of variables in Crammer and Singer‚Äôs dual
problem is the product of the number of samples (l) by the number of classes
(k), which produces a large computational complexity. This chapter sorts the
existing classical techniques for multi-class SVM into the indirect and direct ones
and further gives the comparison for them in terms of theory and experiments.
Especially, this chapter exhibits a new Simplified Multi-class SVM (SimMSVM)
that reduces the size of the resulting dual problem from l √ó k to l by introducing
a relaxed classification error bound. The experimental discussion demonstrates
that the SimMSVM approach can greatly speed up the training process, while
maintaining a competitive classification accuracy.},
  doi      = {10.1007/978-3-319-02300-7__2,},
  file     = {:/home/fabrice/Documents/library/papers/2014_Wang_BOOK.pdf:PDF},
}

@InProceedings{Xu2014,
  author = {Hangjun Xu and ‚Ä†},
  title  = {An Algorithm for Comparing Similarity Between Two Trees: Edit Distance with Gaps},
  year   = {2014},
  file   = {:/home/fabrice/Documents/library/papers/2014_Xu_CONF.pdf:PDF},
}

@InProceedings{Duvenaud1509,
  author = {David Duvenaud and Dougal Maclaurin and Jorge Aguilera-Iparraguirre and Rafael GoÃÅmez-Bombarelli and Timothy Hirzel and AlaÃÅn Aspuru-Guzik and Ryan P. Adams},
  title  = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
  year   = {1509},
  file   = {:/home/fabrice/Documents/library/papers/2015_Duvenaud_NIPS.pdf:PDF},
}

@InProceedings{Feragen,
  author   = {Aasa Feragen, Francois Lauze, Soren Hauberg},
  title    = {Geodesic Exponential Kernels: When Curvature and Linearity Conflict},
  abstract = {2015 IEEE Conference on Computer Vision and Pattern Recognition},
  file     = {:/home/fabrice/Documents/library/papers/2015_Feragen_IEEE.pdf:PDF},
}

@InProceedings{FUNDAMENTALS2015,
  author = {FUNDAMENTALS OF and MACHINE LEARNING and FOR and PREDICTIVE DATA and ANALYTICS and Algorithms and Worked Examples and Case Studies},
  title  = {FreeChapterÀôInformationBasedLearning 2015/6/12 18:02 Page i},
  year   = {2015},
  file   = {:/home/fabrice/Documents/library/papers/2015_Kelleher_BOOK.pdf:PDF},
}

@InProceedings{Xu1508,
  author = {Hangjun Xu},
  title  = {An Algorithm for Comparing Similarity Between Two Trees},
  year   = {1508},
  file   = {:/home/fabrice/Documents/library/papers/2015_Xu_THESIS.pdf:PDF},
}

@InProceedings{Battaglia2016,
  author   = {Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, koray kavukcuoglu},
  title    = {Interaction Networks for Learning about Objects, Relations and Physics},
  year     = {2016},
  abstract = {Neural Information Processing Systems http://nips.cc/},
  file     = {:/home/fabrice/Documents/library/papers/2016_Battaglia_NIPS.pdf:PDF},
}

@InProceedings{Defferrard2016,
  author   = {Micha√´l Defferrard, Xavier Bresson, Pierre Vandergheynst},
  title    = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
  year     = {2016},
  abstract = {Neural Information Processing Systems http://nips.cc/},
  file     = {:/home/fabrice/Documents/library/papers/2016_Defferrard_NIPS.pdf:PDF},
}

@InProceedings{CLASSIFICATION2017,
  author = {SEMI-SUPERVISED CLASSIFICATION and WITH and GRAPH CONVOLUTIONAL and NETWORKS},
  title  = {Published as a conference paper at ICLR},
  year   = {2017},
  file   = {:/home/fabrice/Documents/library/papers/2016_Kipf_ICLR.pdf:PDF},
}

@InProceedings{Kriege1606,
  author = {Nils M. Kriege and Pierre-Louis Giscard and Department of Computer and Science Department and of Computer and Science},
  title  = {On Valid Optimal Assignment Kernels and Applications to Graph Classification},
  year   = {1606},
  file   = {:/home/fabrice/Documents/library/papers/2016_Kriege_NIPS.pdf:PDF},
}

@InProceedings{Bresson1711,
  author = {Xavier Bresson and Thomas Laurent and School of Computer and Science and Engineering and Department of Mathematics and Nanyang Technological University and Singapore Loyola Marymount University and xbresson@ntu.edu.sg tlaurent@lmu.edu},
  title  = {RESIDUAL GATED GRAPH CONVNETS},
  year   = {1711},
  file   = {:/home/fabrice/Documents/library/papers/2017_Bresson_CONF.pdf:PDF},
}

@InProceedings{Gilmer2017,
  author   = {Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl},
  title    = {Neural Message Passing for Quantum Chemistry},
  year     = {2017},
  abstract = {Proceedings of the International Conference on Machine Learning 2017},
  file     = {:/home/fabrice/Documents/library/papers/2017_Gilmer_PMLR.pdf:PDF},
  keywords = {deep learning, chemistry, DFT},
}

@Article{Griffa2017,
  author    = {Alessandra Griffa and Benjamin Ricaud and Kirell Benzi and Xavier Bresson and Alessandro Daducci and Pierre Vandergheynst and Jean-Philippe Thiran and Patric Hagmann},
  journal   = {{NeuroImage}},
  title     = {Transient networks of spatio-temporal connectivity map communication pathways in brain functional systems},
  year      = {2017},
  month     = {jul},
  pages     = {490--502},
  volume    = {155},
  abstract  = {NeuroImage, 155 (2017) 490-502. doi:10.1016/j.neuroimage.2017.04.015},
  doi       = {10.1016/j.neuroimage.2017.04.015},
  file      = {:/home/fabrice/Documents/library/papers/2017_Griffa.pdf:PDF},
  keywords  = {Resting-state fMRI, Diffusion MRI, Brain connectivity, Multilayer network, Temporal network, Brain dynamics, Point-process, Communication-through-coherence, Spatio-temporal connectome},
  publisher = {Elsevier {BV}},
}

@InProceedings{Hamilton2017,
  author = {William L. Hamilton and Rex Ying and Jure Leskovec and wleif@stanford.edu rexying@stanford.edu jure@cs.stanford.edu},
  title  = {Inductive Representation Learning on Large Graphs},
  year   = {2017},
  file   = {:/home/fabrice/Documents/library/papers/2017_Hamilton_NIPS.pdf:PDF},
}

@InProceedings{Titov1703,
  author   = {Diego Marcheggiani1 Ivan Titov},
  title    = {Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling},
  year     = {1703},
  abstract = {A1
A0},
  file     = {:/home/fabrice/Documents/library/papers/2017_Marcheggiani_CONF.pdf:PDF},
}

@InProceedings{Monti2017,
  author   = {Federico Monti, Michael Bronstein, Xavier Bresson},
  title    = {Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks},
  year     = {2017},
  abstract = {Neural Information Processing Systems http://nips.cc/},
  file     = {:/home/fabrice/Documents/library/papers/2017_Monti_CONF.pdf:PDF},
}

@InProceedings{ATTENTION2018,
  author = {GRAPH ATTENTION and NETWORKS},
  title  = {Published as a conference paper at ICLR},
  year   = {2018},
  file   = {:/home/fabrice/Documents/library/papers/2017_Velickovic_ICLR.pdf:PDF},
}

@InProceedings{Welke2018,
  author   = {Pascal Welke},
  title    = {Probabilistic frequent subtrees for efficient graph classification and retrieval},
  year     = {2018},
  abstract = {Machine Learning, doi:10.1007/s10994-017-5688-7},
  doi      = {.org/10.1007/s10994-017-5688-7},
  file     = {:/home/fabrice/Documents/library/papers/2017_Welke_CONF.pdf:PDF},
  keywords = {Pattern mining,Frequent subgraph mining,Frequent subtree mining,Probabilistic subtrees,Efficient embedding computation,Min-hashing},
}

@Article{Gera2018,
  author    = {Ralucca Gera and L. Alonso and Brian Crawford and Jeffrey House and J. A. Mendez-Bermudez and Thomas Knuth and Ryan Miller},
  journal   = {Applied Network Science},
  title     = {Identifying network structure similarity using spectral graph theory},
  year      = {2018},
  month     = {jan},
  number    = {1},
  volume    = {3},
  abstract  = {Appl Netw Sci, doi:10.1007/s41109-017-0042-3},
  doi       = {10.1007/s41109-017-0042-3},
  file      = {:/home/fabrice/Documents/library/papers/2018_Gera_Springer.pdf:PDF},
  keywords  = {Network topology, Graph comparison metrics, Laplacian, Eigenvalue distribution, Kolmogorov-Smirnov test},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Bernhard¬†Korte,
  author = {Bernhard¬†Korte and Jens¬†Vygen},
  title  = {Algorithms and Combinatorics},
  file   = {:/home/fabrice/Documents/library/papers/2018_Korte_BOOK.pdf:PDF},
}

@InProceedings{Lenssen2019,
  author   = {Jan Eric Lenssen and Gaurav Rattan and Martin Grohe and 1TU Dortmund and University},
  title    = {Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks Christopher Morris1, Martin Ritzert2, Matthias Fey1, William L. Hamilton},
  year     = {2019},
  abstract = {in its neighborhood, and the final feature representation of
a graph is the histogram of the resulting node colors. By},
  file     = {:/home/fabrice/Documents/library/papers/2018_Morris_AAAI.pdf:PDF},
}

@InProceedings{MichaelSchlichtkrull1703,
  author   = {Michael Schlichtkrull, Thomas N.protect unhbox voidb@x penalty @M {}Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, Max Welling},
  title    = {Modeling Relational Data with Graph Convolutional Networks},
  year     = {1703},
  abstract = {:country},
  file     = {:/home/fabrice/Documents/library/papers/2018_Schlichtkrull_LNCS.pdf:PDF},
}

@InProceedings{definite0801,
  author = {definite},
  title  = {The optimal assignment kernel is not positive},
  year   = {0801},
  file   = {:/home/fabrice/Documents/library/papers/2018_Vert_CONF.pdf:PDF},
}

@InProceedings{POWERFUL2019,
  author = {HOW POWERFUL and ARE GRAPH and NEURAL NETWORKS},
  title  = {Published as a conference paper at ICLR},
  year   = {2019},
  file   = {:/home/fabrice/Documents/library/papers/2018_Xu_CONF.pdf:PDF},
}

@InProceedings{Ying*2018,
  author   = {Rex Ying*, Ruining He*, Kaifeng Chen*, Pong Eksombatchai*, William L. Hamilton, Jure Leskovec*},
  title    = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
  year     = {2018},
  abstract = {[9, 12]. The representations learned using deep models can be used},
  doi      = {.org/10.1145/3219819.3219890},
  file     = {:/home/fabrice/Documents/library/papers/2018_Ying_KDD.pdf:PDF},
}

@InProceedings{Chen2019,
  author   = {Zhengdao Chen, Soledad Villar, Lei Chen, Joan Bruna},
  title    = {On the equivalence between graph isomorphism testing and function approximation with GNNs},
  year     = {2019},
  abstract = {Neural Information Processing Systems http://nips.cc/},
  file     = {:/home/fabrice/Documents/library/papers/2019_Chen_NIPS.pdf:PDF},
}

@InProceedings{Cranmer1909,
  author = {Miles D. Cranmer and Rui Xu and Peter Battaglia and Princeton University and Princeton University and DeepMind and Princeton and NJ and USA Princeton and NJ and USA London and UK},
  title  = {Learning Symbolic Physics with Graph Networks},
  year   = {1909},
  file   = {:/home/fabrice/Documents/library/papers/2019_Cranmer_NIPS.pdf:PDF},
}

@InProceedings{Ivanov1910,
  author = {Sergei Ivanov and Sergei Sviridov and Criteo Research and Skoltech Zyfra},
  title  = {Understanding Isomorphism Bias in Graph Data Sets},
  year   = {1910},
  file   = {:/home/fabrice/Documents/library/papers/2019_Ivanov_CONF.pdf:PDF},
}

@InProceedings{Lahn2019,
  author   = {Nathaniel Lahn, Deepika Mulchandani, Sharath Raghvendra},
  title    = {A Graph Theoretic Additive Approximation of Optimal Transport},
  year     = {2019},
  abstract = {Neural Information Processing Systems http://nips.cc/},
  file     = {:/home/fabrice/Documents/library/papers/2019_Lahn_CONF.pdf:PDF},
}

@InProceedings{Yamada1902,
  author = {Tam Le‚Ä† Makoto Yamada and ‚Ä† Kenji Fukumizu and ‚Ä† Marco Cuturi},
  title  = {Tree-Sliced Variants of Wasserstein Distances},
  year   = {1902},
  file   = {:/home/fabrice/Documents/library/papers/2019_Le_NIPS.pdf:PDF},
}

@InProceedings{Petric2019,
  author = {Hermina Petric and Maretic Mireille and EL Gheche and Ecole Polytechnique and F√©d√©rale de Lausanne and Ecole Polytechnique and F√©d√©rale de Lausanne},
  title  = {GOT: An Optimal Transport framework for Graph comparison},
  year   = {2019},
  file   = {:/home/fabrice/Documents/library/papers/2019_Maretic_CONF.pdf:PDF},
}

@InProceedings{Maron2019,
  author   = {Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, Yaron Lipman},
  title    = {Provably Powerful Graph Networks},
  year     = {2019},
  abstract = {Neural Information Processing Systems http://nips.cc/},
  file     = {:/home/fabrice/Documents/library/papers/2019_Maron_NIPS.pdf:PDF},
}

@InProceedings{Monti1902,
  author = {Federico Monti and 2 Fabrizio Frasca and 2 Davide Eynard and 2 Damon Mannion},
  title  = {Fake News Detection on Social Media using Geometric Deep Learning},
  year   = {1902},
  file   = {:/home/fabrice/Documents/library/papers/2019_Monti_CONF.pdf:PDF},
}

@InProceedings{Murphy2019,
  author   = {Ryan L. Murphy and 1 Balasubramaniam and Srinivasan and Vinayak Rao and 1 Bruno and Ribeiro},
  title    = {Relational Pooling for Graph Representations},
  year     = {2019},
  abstract = {ing problems, having a most-powerful framework for graph
This work generalizes graph neural networks representation learning would be a key development in ge-
(GNNs) beyond those based on the Weisfeiler- ometric deep learning (Bronstein et al., 2017).
Lehman (WL) algorithm, graph Laplacians, and
diffusions. Our approach, denoted Relational In this work we introduce Relational Pooling (RP), a novel},
  file     = {:/home/fabrice/Documents/library/papers/2019_Murphy_ICML.pdf:PDF},
}

@InProceedings{Peyre1803,
  author = {Gabriel Peyr√© and Marco Cuturi and and CNRSDMA and ENS Google and CREST and ENSAE},
  title  = {Computational Optimal Transport},
  year   = {1803},
  file   = {:/home/fabrice/Documents/library/papers/2019_Peyre.pdf:PDF},
}

@InProceedings{Rieck2019,
  author   = {Bastian Rieck, Christian Bock, Karsten Borgwardt},
  title    = {A Persistent Weisfeiler‚ÄìLehman Procedure for Graph Classification},
  year     = {2019},
  abstract = {Proceedings of the International Conference on Machine Learning 2019},
  file     = {:/home/fabrice/Documents/library/papers/2019_Rieck_PMLR.pdf:PDF},
  keywords = {Weisfeiler-Lehman, Topological Data Analysis, Persistent Homology, Graph Classification, Cycles},
}

@InProceedings{Schulz,
  author   = {Till Schulz and Pascal Welke},
  title    = {On the Necessity of Graph Kernel Baselines},
  abstract = {Naturally, graph structured data is not easy to learn from. As
opposed to itemsets which can be represented by a table of fixed length,
there is no obvious representation language for graphs which allows for
an easy similarity measure in order to perform e.g. classification tasks on
sets of graphs. There have been introduced numerous graph kernels which
tackle the problem of defining a suitable similarity between graphs by
incorporating structural information. In this article, however, we revert
to the very simplistic approach which is to regard a graph as a (multi-
) itemset made up of node and edge labels. We consider our method
as a baseline and compare it to several established graph kernels on a
wide range of benchmark datasets. Our observations suggest that for the
overwhelming number of available datasets, actually utilizing the graphs‚Äô
structure in graph kernels does not significantly improve the classification
accuracy.},
  file     = {:/home/fabrice/Documents/library/papers/2019_Schulz_CONF.pdf:PDF},
}

@InProceedings{Schulz1038,
  author    = {Till Hendrik Schulz and Pascal Welke},
  title     = {OntheNecessityof GraphKernelBaselines},
  year      = {1038},
  publisher = {IEEE},
  file      = {:/home/fabrice/Documents/library/papers/2019_Schulz_POSTER.pdf:PDF},
}

@InProceedings{ON2020,
  author   = {ON THE and EQUIVALENCE BETWEEN and POSITIONAL NODE and EMBEDDINGS AND and STRUCTURAL GRAPH and REPRESEN- and TATIONS},
  title    = {Published as a conference paper at ICLR},
  year     = {2020},
  abstract = {This work provides the first unifying theoretical framework for node (positional)
embeddings and structural graph representations, bridging methods like matrix
factorization and graph neural networks. Using invariant theory, we show that
the relationship between structural representations and node embeddings is anal-
ogous to that of a distribution and its samples. We prove that all tasks that can
be performed by node embeddings can also be performed by structural represen-
tations and vice-versa. We also show that the concept of transductive and induc-
tive learning is unrelated to node embeddings and graph representations, clearing
another source of confusion in the literature. Finally, we introduce new practi-
cal guidelines to generating and using node embeddings, which fixes significant
shortcomings of standard operating procedures used today.},
  file     = {:/home/fabrice/Documents/library/papers/2019_Srinivasan.pdf:PDF},
}

@InProceedings{Togninalli2019,
  author   = {Matteo Togninalli, Elisabetta Ghisu, Felipe Llinares-L√≥pez, Bastian Rieck, Karsten Borgwardt},
  title    = {Wasserstein Weisfeiler-Lehman Graph Kernels},
  year     = {2019},
  abstract = {Neural Information Processing Systems http://nips.cc/},
  file     = {:/home/fabrice/Documents/library/papers/2019_Togninalli_NIPS.pdf:PDF},
}

@InProceedings{POWERFUL2019a,
  author = {HOW POWERFUL and ARE GRAPH and NEURAL NETWORKS},
  title  = {Published as a conference paper at ICLR},
  year   = {2019},
  file   = {:/home/fabrice/Documents/library/papers/2019_Xu_CONF.pdf:PDF},
}

@InProceedings{Chami2005,
  author   = {Ines Chami and Adva Wolf and Da-Cheng Juan and Frederic Sala and Sujith Ravi and Christopher Re and 1Stanford University and 2Google Research and 3Amazon Alexa},
  title    = {Low-Dimensional Hyperbolic Knowledge Graph Embeddings},
  year     = {2005},
  abstract = {Movie director Steven},
  doi      = {ng},
  file     = {:/home/fabrice/Documents/library/papers/2020_Chami_CONF.pdf:PDF},
}

@InProceedings{Cohen2007,
  author   = {Samuel Cohen, Michael Arbel, Marc Peter Deisenroth},
  title    = {Estimating Barycenters of Measures in High Dimensions},
  year     = {2007},
  abstract = {Proceedings of the International Conference on Machine Learning 2021},
  file     = {:/home/fabrice/Documents/library/papers/2020_Cohen_CONF.pdf:PDF},
  keywords = {Machine Learning, ICML},
}

@InProceedings{2003_Prakash,
  author = {Vijay Prakash and Dwivedi1‚àó Chaitanya and K. Joshi and vijaypra001@e.ntu.edu.sg chaitanya.joshi@ntu.edu.sg},
  title  = {{B}enchmarking {G}raph {N}eural {N}etworks},
  year   = {2003},
  file   = {:/home/fabrice/Documents/library/papers/2020_Dwivedi_CONF.pdf:PDF},
}

@InProceedings{Hamilton2020,
  author = {William L. Hamilton},
  title  = {Graph Representation Learning},
  year   = {2020},
  file   = {:/home/fabrice/Documents/library/papers/2020_Hamilton_BOOK.pdf:PDF},
}

@InProceedings{Hu2020,
  author = {Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
  title  = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
  year   = {2020},
  file   = {:/home/fabrice/Documents/library/papers/2020_Hu_CONF.pdf:PDF},
}

@InProceedings{Mirhoseini2004,
  author   = {Azalia Mirhoseini, Anna Goldie, Mustafa Yazgan, Joe Jiang, Ebrahim Songhori, Shen Wang, Young-Joon Lee, Eric Johnson, Omkar Pathak, Sungmin Bae, Azade Nazi, Jiwoo Pak, Andy Tong, Kavya Srinivasa, William Hang, Emre Tuncer, Anand Babu, Quoc Le, James Laudon, Richard Ho, Roger Carpenter, Jeff Dean},
  title    = {Chip Placement with Deep Reinforcement Learning},
  year     = {2004},
  abstract = {years to design, leaving us with the speculative task of opti-
mizing them for the machine learning (ML) models of 2-5},
  file     = {:/home/fabrice/Documents/library/papers/2020_Mirhoseini_CONF.pdf:PDF},
}

@InProceedings{Morris2020,
  author   = {Christopher Morris, Nils M.Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, Marion Neumann},
  title    = {TUDataset: A collection of benchmark datasets for learning with graphs},
  year     = {2020},
  abstract = {Graph Representation Learning and Beyond (GRL+), ICML 2020 Workshop},
  file     = {:/home/fabrice/Documents/library/papers/2020_Morris_CONF.pdf:PDF},
  keywords = {graph learning, graph kernel, weisfeiler, leman, gnn, graph neural network, benchmark datasets},
}

@InProceedings{SanchezGonzalez2020,
  author   = {Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, Peter W. Battaglia},
  title    = {Learning to Simulate Complex Physics with Graph Networks},
  year     = {2020},
  abstract = {Proceedings of the International Conference on Machine Learning 2020},
  file     = {:/home/fabrice/Documents/library/papers/2020_Sanchey-Gonzalez_PMLR.pdf:PDF},
  keywords = {Machine Learning, Graph Neural Networks, Physical Simulation, Particle-based Fluid Simulation},
}

@InProceedings{GraKeL2020,
  author   = {GraKeL: A and Graph Kernel and Library in Python},
  title    = {Journal of Machine Learning Research 21 (2020) 1-5 Submitted 6/18; Revised 3/20; Published},
  year     = {2020},
  file     = {:/home/fabrice/Documents/library/papers/2020_Siglidis_JMLR.pdf:PDF},
  keywords = {graph similarity, graph kernels, scikit-learn, Python},
}

@InProceedings{GRAPH2021,
  author = {REVISITING GRAPH and NEURAL NETWORKS and FOR LINK and PREDICTION},
  title  = {Under review as a conference paper at ICLR},
  year   = {2021},
  file   = {:/home/fabrice/Documents/library/papers/2020_Zhang_CONF.pdf:PDF},
}

@InProceedings{Python2021,
  author   = {POT: Python and Optimal Transport},
  title    = {Journal of Machine Learning Research 22 (2021) 1-8 Submitted 5/20; Revised 1/21; Published},
  year     = {2021},
  abstract = {Optimal transport has recently been reintroduced to the machine learning community
thanks in part to novel efficient optimization procedures allowing for medium to large scale
applications. We propose a Python toolbox that implements several key optimal transport
ideas for the machine learning community. The toolbox contains implementations of a
number of founding works of OT for machine learning such as Sinkhorn algorithm and
Wasserstein barycenters, but also provides generic solvers that can be used for conducting
novel fundamental research. This toolbox, named POT for Python Optimal Transport, is
open source with an MIT license.
Keywords: Optimal transport, divergence, optimization, domain adaptation},
  file     = {:/home/fabrice/Documents/library/papers/2021_Flamary_JMLR.pdf:PDF},
}

@InProceedings{Hendrik2101,
  author   = {Till Hendrik and Schulz and TamaÃÅs HorvaÃÅth and ‚àó‚Ä† Pascal and Welke and Stefan Wrobel and ‚àó‚Ä†},
  title    = {A Generalized Weisfeiler-Lehman Graph Kernel},
  year     = {2101},
  abstract = {more than it resembles T3, the Weisfeiler-Lehman ker-
The Weisfeiler-Lehman graph kernels are among the most nel [11] simply treats them all as unequal and is thus
prevalent graph kernels due to their remarkable time com- unable to quantify the apparent difference among the
plexity and predictive performance. Their key concept is
based on an implicit comparison of neighborhood repre- pairwise similarities between the unfolding trees.
senting trees with respect to equality (i.e., isomorphism). Motivated by these considerations, we relax the
This binary valued comparison is, however, arguably too above strictness by proposing a method which com-
rigid for defining suitable similarity measures over graphs.
To overcome this limitation, we propose a generalization of pares Weisfeiler-Lehman labels, or equivalently unfold-
Weisfeiler-Lehman graph kernels which takes into account ing trees, with regard to a much finer similarity measure
the similarity between trees rather than equality. We achieve than the binary valued one. More precisely, we employ
this using a specifically fitted variation of the well-known tree
edit distance which can efficiently be calculated. We em- a similarity between Weisfeiler-Lehman labels based on
pirically show that our approach significantly outperforms the concept of tree edit distances between their respec-
state-of-the-art methods in terms of predictive performance tive unfolding trees. These kind of distances provide
on datasets containing structurally more complex graphs be-
yond the typically considered molecular graphs. a natural comparison for trees. On an abstract level,},
  file     = {:/home/fabrice/Documents/library/papers/2021_Schulz_CONF.pdf:PDF},
}

@InProceedings{Schulz2110,
  author   = {Till H. Schulz and Pascal Welke and Stefan Wrobel},
  title    = {Graph Filtration Kernels},
  year     = {2110},
  abstract = {substructures in terms of equivalence. We refer to these ker-
nels as histogram kernels. While they prove to be successful},
  file     = {:/home/fabrice/Documents/library/papers/2021_Schulz_CONFa.pdf:PDF},
}

@Misc{,
  title = {Inspiration Machine Learning.txt},
  file  = {:/home/fabrice/Documents/library/papers/Inspiration Machine Learning.txt:Text},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:/home/fabrice/Documents/library/papers;}

@Comment{jabref-meta: saveActions:disabled;
all-text-fields[identity]
date[normalize_date]
month[normalize_month]
pages[normalize_page_numbers]
;}
