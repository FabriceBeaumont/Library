@Article{1965_Morgan_Article,
  author         = {H. L. Morgan},
  journal        = {Journal of Chemical Documentation},
  title          = {The Generation of a Unique Machine Description for Chemical Structures-A Technique Developed at Chemical Abstracts Service.},
  year           = {1965},
  month          = {may},
  number         = {2},
  pages          = {107--113},
  volume         = {5},
  doi            = {10.1021/c160017a018},
  file           = {:1965_Morgan_CONF.pdf:PDF},
  keywords       = {article doi: 10.1021/c160017a018, Article metadata: Journal of Chemical Documentation_5_2_10.1021/c160017a018_107_113},
  publisher      = {American Chemical Society ({ACS})},
  qualityassured = {qualityAssured},
}

@InProceedings{1990_Perona_InProceedings,
  author         = {Perona, P. and Malik, J.},
  title          = {Scale-space and edge detection using anisotropic diffusion},
  year           = {1990},
  number         = {7},
  pages          = {629-639},
  publisher      = {IEEE},
  volume         = {12},
  doi            = {10.1109/34.56205},
  file           = {:1990_Perona_IEEE.pdf:PDF},
  journal        = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/56205/},
}

@Article{1991_Debnath_Article,
  author         = {Asim Kumar Debnath and Rosa L. Lopez de Compadre and Gargi Debnath and Alan J. Shusterman and Corwin Hansch},
  journal        = {Journal of Medicinal Chemistry},
  title          = {Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity},
  year           = {1991},
  month          = {feb},
  number         = {2},
  pages          = {786--797},
  volume         = {34},
  doi            = {10.1021/jm00106a046},
  file           = {:1991_Debnath_CONF.pdf:PDF},
  keywords       = {article doi: 10.1021/jm00106a046, Article metadata: Journal of Medicinal Chemistry_34_2_10.1021/jm00106a046_786_797},
  publisher      = {American Chemical Society ({ACS})},
  qualityassured = {qualityAssured},
}

@InProceedings{2003_Barla_InProceedingsa,
  author         = {Barla, A. and Odone, F. and Verri, A.},
  booktitle      = {Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429)},
  title          = {Histogram intersection kernel for image classification},
  year           = {2003},
  pages          = {III-513},
  volume         = {3},
  doi            = {10.1109/ICIP.2003.1247294},
  file           = {:2003_Barla_CONF.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/1247294/},
}

@InProceedings{2003_Belkin_InProceedings,
  author         = {Belkin, Mikhail and Niyogi, Partha},
  title          = {{L}aplacian {E}igenmaps for {D}imensionality {R}eduction and {D}ata {R}epresentation},
  year           = {2003},
  number         = {6},
  pages          = {1373-1396},
  volume         = {15},
  doi            = {10.1162/089976603321780317},
  file           = {:2003_Belkin_IEEE.pdf:PDF},
  journal        = {Neural Computation},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/6789755/},
}

@InProceedings{2005_Deshpande_InProceedings,
  author         = {Deshpande, M. and Kuramochi, M. and Wale, N. and Karypis, G.},
  title          = {Frequent substructure-based approaches for classifying chemical compounds},
  year           = {2005},
  number         = {8},
  pages          = {1036-1050},
  volume         = {17},
  doi            = {10.1109/TKDE.2005.127},
  file           = {:2003_Deshpande_TECH_REPORT.pdf:PDF},
  groups         = {Graphtheory},
  journal        = {IEEE Transactions on Knowledge and Data Engineering},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/1458698/},
}

@InProceedings{2003_Gaertner_InProceedingsa,
  author         = {G{\"a}rtner, Thomas and Flach, Peter and Wrobel, Stefan},
  booktitle      = {Learning Theory and Kernel Machines},
  title          = {{O}n {G}raph {K}ernels: {H}ardness {R}esults and {E}fficient {A}lternatives},
  year           = {2003},
  address        = {Berlin, Heidelberg},
  editor         = {Sch{\"o}lkopf, Bernhard and Warmuth, Manfred K.},
  pages          = {129--143},
  publisher      = {Springer Berlin Heidelberg},
  abstract       = {As most `real-world' data is structured, research in kernel methods has begun investigating kernels for various kinds of structured data. One of the most widely used tools for modeling structured data are graphs. An interesting and important challenge is thus to investigate kernels on instances that are represented by graphs. So far, only very specific graphs such as trees and strings have been considered.},
  doi            = {10.1007/978-3-540-45167-9_11},
  file           = {:2003_Gaertner_CONF.pdf:PDF},
  groups         = {GraphKernels},
  isbn           = {978-3-540-45167-9},
  qualityassured = {qualityAssured},
}

@Article{2003_Sutherland_Article,
  author         = {Jeffrey J. Sutherland and Lee A. O{\textquotesingle}Brien and Donald F. Weaver},
  journal        = {Journal of Chemical Information and Computer Sciences},
  title          = {{S}pline-{F}itting with a {G}enetic {A}lgorithm: {A} {M}ethod for {D}eveloping {C}lassification {S}tructure-{A}ctivity {R}elationships},
  year           = {2003},
  month          = {oct},
  number         = {6},
  pages          = {1906--1915},
  volume         = {43},
  doi            = {10.1021/ci034143r},
  file           = {:2003_Sutherland_CONF.pdf:PDF},
  publisher      = {American Chemical Society ({ACS})},
  qualityassured = {qualityAssured},
}

@InProceedings{2004_Kuramochi_InProceedings,
  author         = {Kuramochi, M. and Karypis, G.},
  title          = {An efficient algorithm for discovering frequent subgraphs},
  year           = {2004},
  number         = {9},
  pages          = {1038-1051},
  volume         = {16},
  abstract       = {Over the years, frequent itemset discovery algo- formulating the frequent pattern discovery problem is as that
rithms have been used to find interesting patterns in various of discovering subgraphs that occur frequently over the entire
application areas. However, as data mining techniques are being set of graphs.
increasingly applied to non-traditional domains, existing frequent
pattern discovery approach cannot be used. This is because The power of graphs to model complex datasets has been},
  doi            = {10.1109/TKDE.2004.33},
  file           = {:2004_Kuramochi_IEEE.pdf:PDF},
  groups         = {Graphtheory},
  journal        = {IEEE Transactions on Knowledge and Data Engineering},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/1316833/},
}

@InProceedings{2004_Nijssen_InProceedings,
  author         = {Nijssen, Siegfried and Kok, Joost N.},
  booktitle      = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  title          = {{A} {Q}uickstart in {F}requent {S}tructure {M}ining {C}an {M}ake a {D}ifference},
  year           = {2004},
  address        = {New York, NY, USA},
  pages          = {647–652},
  publisher      = {Association for Computing Machinery},
  series         = {KDD '04},
  abstract       = {Given a database, structure mining algorithms search for substructures that satisfy constraints such as minimum frequency, minimum confidence, minimum interest and maximum frequency. Examples of substructures include graphs, trees and paths. For these substructures many mining algorithms have been proposed. In order to make graph mining more efficient, we investigate the use of the "quickstart principle", which is based on the fact that these classes of structures are contained in each other, thus allowing for the development of structure mining algorithms that split the search into steps of increasing complexity. We introduce the GrAph/Sequence/Tree extractiON (Gaston) algorithm that implements this idea by searching first for frequent paths, then frequent free trees and finally cyclic graphs. We investigate two alternatives for computing the frequency of structures and present experimental results to relate these alternatives.},
  doi            = {10.1145/1014052.1014134},
  file           = {:2004_Nijssen_CONF.pdf:PDF},
  groups         = {Graphtheory},
  isbn           = {1581138881},
  keywords       = {semi-structures, frequent item sets, structures, graphs},
  location       = {Seattle, WA, USA},
  numpages       = {6},
  qualityassured = {qualityAssured},
}

@Article{2004_Schomburg_Article,
  author         = {Schomburg, Ida and Chang, Antje and Ebeling, Christian and Gremse, Marion and Heldt, Christian and Huhn, Gregor and Schomburg, Dietmar},
  journal        = {Nucleic Acids Research},
  title          = {{BRENDA, the enzyme database: updates and major new developments}},
  year           = {2004},
  issn           = {0305-1048},
  month          = {01},
  number         = {suppl_1},
  pages          = {D431-D433},
  volume         = {32},
  abstract       = {{ BRENDA (BRaunschweig ENzyme DAtabase) represents a comprehensive collection of enzyme and metabolic information, based on primary literature. The database contains data from at least 83 000 different enzymes from 9800 different organisms, classified in ∼4200 EC numbers. BRENDA includes biochemical and molecular information on classification and nomenclature, reaction and specificity, functional parameters, occurrence, enzyme structure, application, engineering, stability, disease, isolation and preparation, links and literature references. The data are extracted and evaluated from ∼46 000 references, which are linked to PubMed as long as the reference is cited in PubMed. In the past year BRENDA has undergone major changes including a large increase in updating speed with \\&gt;50\\% of all data updated in 2002 or in the first half of 2003, the development of a new EC‐tree browser, a taxonomy‐tree browser, a chemical substructure search engine for ligand structure, the development of controlled vocabulary, an ontology for some information fields and a thesaurus for ligand names. The database is accessible free of charge to the academic community at http://www.brenda. uni‐koeln.de . }},
  doi            = {10.1093/nar/gkh081},
  eprint         = {https://academic.oup.com/nar/article-pdf/32/suppl\_1/D431/7621720/gkh081.pdf},
  file           = {:2004_Schomburg_CONF.pdf:PDF},
  qualityassured = {qualityAssured},
}

@Article{2005_Nijssen_Article,
  author         = {Siegfried Nijssen and Joost N. Kok},
  journal        = {Electronic Notes in Theoretical Computer Science},
  title          = {{T}he {G}aston {T}ool for {F}requent {S}ubgraph {M}ining},
  year           = {2005},
  issn           = {1571-0661},
  note           = {Proceedings of the International Workshop on Graph-Based Tools (GraBaTs 2004)},
  number         = {1},
  pages          = {77-87},
  volume         = {127},
  abstract       = {Given a database of graphs, structure mining algorithms search for all substructures that satisfy constraints such as minimum frequency, minimum confidence, minimum interest and maximum frequency. In order to make frequent subgraph mining more efficient, we propose to search with steps of increasing complexity. We present the GrAph/Sequence/Tree extractiON (Gaston) tool that implements this idea by searching first for frequent paths, then frequent free trees and finally cyclic graphs. We give results on large molecular databases.},
  doi            = {10.1016/j.entcs.2004.12.039},
  file           = {:2005_Nijssen_ENTCS.pdf:PDF},
  groups         = {Graphtheory},
  keywords       = {Frequent Subgraphs, Data Mining},
  qualityassured = {qualityAssured},
  url            = {https://www.sciencedirect.com/science/article/pii/S1571066105001064},
}

@InProceedings{2008_Bach_InProceedings,
  author         = {Bach, Francis R.},
  booktitle      = {Proceedings of the 25th International Conference on Machine Learning},
  title          = {{G}raph {K}ernels between {P}oint {C}louds},
  year           = {2008},
  address        = {New York, NY, USA},
  pages          = {25–32},
  publisher      = {Association for Computing Machinery},
  series         = {ICML '08},
  abstract       = {Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow one to use kernel methods for such objects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on probabilistic graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.},
  doi            = {10.1145/1390156.1390160},
  file           = {:2008_Bach_ICML.pdf:PDF},
  groups         = {GraphKernels},
  isbn           = {9781605582054},
  location       = {Helsinki, Finland},
  numpages       = {8},
  qualityassured = {qualityAssured},
}

@InProceedings{2003_Shervashidze_InProceedings,
  author         = {Nino Shervashidze and S. V. N. Vishwanathan and Tobias H. Petri},
  title          = {Efficient graphlet kernels for large graph comparison},
  year           = {2003},
  abstract       = {Frequent subgraph mining algorithms, on the other
hand, aim to detect subgraphs that are frequent in
a given dataset of graphs. Afterwards, feature selec-},
  file           = {:2009_Shervashidze_PMLR.pdf:PDF},
  groups         = {GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.mlr.press/v5/shervashidze09a.html},
}

@InProceedings{2009_Vert_InProceedingsa,
  author         = {Vert, Jean-Philippe and Matsui, Tomoko and Satoh, Shin'ichi and Uchiyama, Yuji},
  booktitle      = {2009 IEEE International Conference on Acoustics, Speech and Signal Processing},
  title          = {{H}igh-level feature extraction using {SVM} with walk-based graph kernel},
  year           = {2009},
  month          = {April},
  pages          = {1121-1124},
  abstract       = {We investigate a method using support vector machines (SVMs) with walk-based graph kernels for high-level feature extraction from images. In this method, each image is first segmented into a finite set of homogeneous segments and then represented as a segmentation graph where each vertex is a segment and edges connect adjacent segments. Given a set of features associated with each segment, we then obtain a positive definite kernel between images by comparing walks in the respective segmentation graphs, and image classification is carried out with an SVM based on this kernel. In a benchmark experiment on the MediaMill challenge problem, the mean average precision increased from 0.216 (baseline) to 0.341 when our method was utilized.},
  doi            = {10.1109/ICASSP.2009.4959785},
  file           = {:2009_Vert_IEEE.pdf:PDF},
  groups         = {SupportVectorMachines},
  issn           = {2379-190X},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/4959785/},
}

@MastersThesis{2009_Kriege_MastersThesis,
  author         = {Nils M. Kriege},
  school         = {TU Dortmund},
  title          = {{E}rweiterte {S}ubstruktursuche in {M}olek{\"u}ldatenbanken und ihre {I}ntegration in {S}caffold {H}unter},
  year           = {2009},
  file           = {:2010_Kriege_DISSERTATION.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {http://eprints.cs.univie.ac.at/6761/},
}

@Article{2013_Abubaker_Article,
  author         = {Mohamed Abubaker and Wesam Ashour},
  journal        = {International Journal of Intelligent Systems and Applications},
  title          = {{E}fficient {D}ata {C}lustering {A}lgorithms: {I}mprovements over {K}means},
  year           = {2013},
  month          = {feb},
  number         = {3},
  pages          = {37--49},
  volume         = {5},
  abstract       = {This paper presents a new approach to The clustering problems can be categorized into two},
  doi            = {10.5815/ijisa.2013.03.04},
  file           = {:2013_Abubaker_CONF.pdf:PDF},
  groups         = {ClusteringAlgorithms},
  publisher      = {{MECS} Publisher},
  qualityassured = {qualityAssured},
  readstatus     = {read},
  url            = {https://www.mecs-press.org/ijisa/ijisa-v5-n3/IJISA-V5-N3-4.pdf},
}

@Article{2014_Chen_InProceedings,
  author         = {Chen, Yudong and Sanghavi, Sujay and Xu, Huan},
  journal        = {{IEEE} Transactions on Information Theory},
  title          = {{I}mproved {G}raph {C}lustering},
  year           = {2014},
  month          = {oct},
  number         = {10},
  pages          = {6440--6455},
  volume         = {60},
  abstract       = {Graph clustering involves the task of dividing nodes • Small density gap: the edge density across clusters is
into clusters, so that the edge density is higher within clusters only a small additive or multiplicative factor different
as opposed to across clusters. A natural, classic and popular from within clusters;
statistical setting for evaluating solutions to this problem is the
stochastic block model, also referred to as the planted partition • Sparsity: the graph is overall very sparse even within
model. clusters;},
  doi            = {10.1109/tit.2014.2346205},
  file           = {:2014_Chen_CONF.pdf:PDF},
  groups         = {GraphKernels},
  publisher      = {Institute of Electrical and Electronics Engineers ({IEEE})},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/6873307},
}

@Book{2015_Kelleher_Book,
  author         = {John D. Kelleher and Brian Mac Namee and Aoife D'Arcy},
  title          = {Fundamentals of Machine Learning for Predictive Data Analytics},
  year           = {2015},
  file           = {:2015_Kelleher_BOOK.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://www.mencap.org.uk/sites/default/files/2021-03/pdf-fundamentals-of-machine-learning-for-predictive-data-analytics-a-john-d-kelleher-brian-mac-namee-aoife-darcy-pdf-download-free-book-d21caf4.pdf},
}

@InProceedings{2017_Kipf_InProceedings,
  author         = {Thomas N. Kipf and Max Welling},
  title          = {{S}emi-{S}upervised {C}lassification with {G}raph {C}onvolutional {N}etworks},
  year           = {2017},
  abstract       = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix  = {arXiv},
  eprint         = {1609.02907},
  file           = {:2016_Kipf_ICLR.pdf:PDF},
  primaryclass   = {cs.LG},
  qualityassured = {qualityAssured},
}

@InProceedings{2017_Monti_InProceedings,
  author         = {Monti, Federico and Bronstein, Michael and Bresson, Xavier},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{G}eometric {M}atrix {C}ompletion with {R}ecurrent {M}ulti-{G}raph {N}eural {N}etworks},
  year           = {2017},
  editor         = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {30},
  abstract       = {Neural Information Processing Systems http://nips.cc/},
  file           = {:2017_Monti_CONF.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2eace51d8f796d04991c831a07059758-Paper.pdf},
}

@InProceedings{2018_Velickovic_InProceedings,
  author         = {Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
  title          = {{G}raph {A}ttention {N}etworks},
  year           = {2018},
  archiveprefix  = {arXiv},
  eprint         = {1710.10903},
  eprinttype     = {arxiv},
  file           = {:2017_Velickovic_ICLR.pdf:PDF},
  groups         = {GraphKernels},
  primaryclass   = {stat.ML},
  qualityassured = {qualityAssured},
}

@Article{2017_Welke_Article,
  author         = {Pascal Welke and Tam{\'{a}}s Horv{\'{a}}th and Stefan Wrobel},
  journal        = {Machine Learning},
  title          = {Probabilistic frequent subtrees for efficient graph classification and retrieval},
  year           = {2017},
  month          = {nov},
  number         = {11},
  pages          = {1847--1873},
  volume         = {107},
  abstract       = {Machine Learning, doi:10.1007/s10994-017-5688-7},
  doi            = {10.1007/s10994-017-5688-7},
  file           = {:2017_Welke_CONF.pdf:PDF},
  groups         = {Graphtheory},
  keywords       = {Pattern mining,Frequent subgraph mining,Frequent subtree mining,Probabilistic subtrees,Efficient embedding computation,Min-hashing},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Article{2018_Gera_Article,
  author         = {Ralucca Gera and L. Alonso and Brian Crawford and Jeffrey House and J. A. Mendez-Bermudez and Thomas Knuth and Ryan Miller},
  journal        = {Applied Network Science},
  title          = {Identifying network structure similarity using spectral graph theory},
  year           = {2018},
  month          = {jan},
  number         = {1},
  volume         = {3},
  abstract       = {Appl Netw Sci, doi:10.1007/s41109-017-0042-3},
  doi            = {10.1007/s41109-017-0042-3},
  file           = {:2018_Gera_Springer.pdf:PDF},
  keywords       = {Network topology, Graph comparison metrics, Laplacian, Eigenvalue distribution, Kolmogorov-Smirnov test},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Article{2019_Morris_Article,
  author         = {Christopher Morris and Martin Ritzert and Matthias Fey and William L. Hamilton and Jan Eric Lenssen and Gaurav Rattan and Martin Grohe},
  journal        = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title          = {Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks},
  year           = {2019},
  month          = {jul},
  number         = {01},
  pages          = {4602--4609},
  volume         = {33},
  abstract       = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
  doi            = {10.1609/aaai.v33i01.33014602},
  file           = {:2018_Morris_AAAI.pdf:PDF},
  groups         = {GraphKernels},
  publisher      = {Association for the Advancement of Artificial Intelligence ({AAAI})},
  qualityassured = {qualityAssured},
  url            = {https://ojs.aaai.org/index.php/AAAI/article/view/4384},
}

@InCollection{2018_Schlichtkrull_InCollection,
  author         = {Michael Schlichtkrull and Thomas N. Kipf and Peter Bloem and Rianne van~den Berg and Ivan Titov and Max Welling},
  booktitle      = {The Semantic Web},
  publisher      = {Springer International Publishing},
  title          = {Modeling Relational Data with Graph Convolutional Networks},
  year           = {2018},
  pages          = {593--607},
  abstract       = {:country},
  doi            = {10.1007/978-3-319-93417-4_38},
  file           = {:2018_Schlichtkrull_LNCS.pdf:PDF},
  qualityassured = {qualityAssured},
}

@Misc{2008_Vert_Misc,
  author         = {Jean-Philippe Vert},
  title          = {The optimal assignment kernel is not positive definite},
  year           = {2008},
  archiveprefix  = {arXiv},
  eprint         = {0801.4061},
  eprinttype     = {arxiv},
  file           = {:2018_Vert_CONF.pdf:PDF},
  groups         = {GraphKernels},
  primaryclass   = {cs.LG},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
}

@InProceedings{2019_Xu_InProceedings,
  author         = {Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
  title          = {{H}ow {P}owerful are {G}raph {N}eural {N}etworks?},
  year           = {2019},
  archiveprefix  = {arXiv},
  eprint         = {1810.00826},
  eprinttype     = {arxiv},
  file           = {:2018_Xu_CONF.pdf:PDF},
  groups         = {GraphKernels},
  primaryclass   = {cs.LG},
  qualityassured = {qualityAssured},
}

@InProceedings{2018_Ying_InProceedings,
  author         = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
  booktitle      = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
  title          = {{G}raph {C}onvolutional {N}eural {N}etworks for {W}eb-{S}cale {R}ecommender {S}ystems},
  year           = {2018},
  address        = {New York, NY, USA},
  pages          = {974–983},
  publisher      = {Association for Computing Machinery},
  series         = {KDD '18},
  abstract       = {Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.},
  doi            = {10.1145/3219819.3219890},
  file           = {:2018_Ying_KDD.pdf:PDF},
  groups         = {GraphKernels},
  isbn           = {9781450355520},
  keywords       = {graph convolutional networks, scalability, recommender systems, deep learning},
  location       = {London, United Kingdom},
  numpages       = {10},
  qualityassured = {qualityAssured},
}

@InProceedings{2019_Chen_InProceedings,
  author         = {Chen, Zhengdao and Villar, Soledad and Chen, Lei and Bruna, Joan},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{O}n the equivalence between graph isomorphism testing and function approximation with {GNN}s},
  year           = {2019},
  editor         = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {32},
  abstract       = {Neural Information Processing Systems http://nips.cc/},
  file           = {:2019_Chen_NIPS.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2019/file/71ee911dd06428a96c143a0b135041a4-Paper.pdf},
}

@InProceedings{2019_Cranmer_InProceedings,
  author         = {Miles D. Cranmer and Rui Xu and Peter Battaglia and Shirley Ho},
  title          = {{L}earning {S}ymbolic {P}hysics with {G}raph {N}etworks},
  year           = {2019},
  archiveprefix  = {arXiv},
  eprint         = {1909.05862},
  file           = {:2019_Cranmer_NIPS.pdf:PDF},
  primaryclass   = {cs.LG},
  qualityassured = {qualityAssured},
}

@InProceedings{2019_Ivanov_InProceedings,
  author         = {Sergei Ivanov and Sergei Sviridov and Evgeny Burnaev},
  title          = {{U}nderstanding {I}somorphism {B}ias in {G}raph {D}ata {S}ets},
  year           = {2019},
  archiveprefix  = {arXiv},
  eprint         = {1910.12091},
  file           = {:2019_Ivanov_CONF.pdf:PDF},
  primaryclass   = {cs.LG},
  qualityassured = {qualityAssured},
}

@InProceedings{2019_Lahn_InProceedings,
  author         = {Lahn, Nathaniel and Mulchandani, Deepika and Raghvendra, Sharath},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{A} {G}raph {T}heoretic {A}dditive {A}pproximation of {O}ptimal {T}ransport},
  year           = {2019},
  editor         = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {32},
  abstract       = {Neural Information Processing Systems http://nips.cc/},
  file           = {:2019_Lahn_CONF.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2019/file/9b07f50145902e945a1cc629f729c213-Paper.pdf},
}

@InProceedings{2019_Le_InProceedings,
  author         = {Le, Tam and Yamada, Makoto and Fukumizu, Kenji and Cuturi, Marco},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{T}ree-{S}liced {V}ariants of {W}asserstein {D}istances},
  year           = {2019},
  editor         = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {32},
  file           = {:2019_Le_NIPS.pdf:PDF},
  groups         = {Graphtheory},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2d36b5821f8affc6868b59dfc9af6c9f-Paper.pdf},
}

@InProceedings{2019_PetricMaretic_InProceedings,
  author         = {Petric Maretic, Hermina and El Gheche, Mireille and Chierchia, Giovanni and Frossard, Pascal},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{GOT}: {A}n {O}ptimal {T}ransport framework for {G}raph comparison},
  year           = {2019},
  editor         = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {32},
  file           = {:2019_Maretic_CONF.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2019/file/fdd5b16fc8134339089ef25b3cf0e588-Paper.pdf},
}

@InProceedings{2019_Maron_InProceedings,
  author         = {Maron, Haggai and Ben-Hamu, Heli and Serviansky, Hadar and Lipman, Yaron},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{P}rovably {P}owerful {G}raph {N}etworks},
  year           = {2019},
  editor         = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {32},
  abstract       = {Neural Information Processing Systems http://nips.cc/},
  file           = {:2019_Maron_NIPS.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bb04af0f7ecaee4aae62035497da1387-Paper.pdf},
}

@InProceedings{2019_Monti_InProceedings,
  author         = {Federico Monti and Fabrizio Frasca and Davide Eynard and Damon Mannion and Michael M. Bronstein},
  title          = {{F}ake {N}ews {D}etection on {S}ocial {M}edia using {G}eometric {D}eep {L}earning},
  year           = {2019},
  archiveprefix  = {arXiv},
  eprint         = {1902.06673},
  file           = {:2019_Monti_CONF.pdf:PDF},
  primaryclass   = {cs.SI},
  qualityassured = {qualityAssured},
}

@InProceedings{2019_Murphy_InProceedings,
  author         = {Murphy, Ryan and Srinivasan, Balasubramaniam and Rao, Vinayak and Ribeiro, Bruno},
  booktitle      = {Proceedings of the 36th International Conference on Machine Learning},
  title          = {{R}elational {P}ooling for {G}raph {R}epresentations},
  year           = {2019},
  editor         = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month          = {09--15 Jun},
  pages          = {4663--4673},
  publisher      = {PMLR},
  series         = {Proceedings of Machine Learning Research},
  volume         = {97},
  abstract       = {This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, graph Laplacians, and diffusions. Our approach, denoted Relational Pooling (RP), draws from the theory of finite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation models and, somewhat counterintuitively, can make them even more powerful than the original WL isomorphism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound approach for graph classification. We demonstrate improved performance of RP-based graph representations over state-of-the-art methods on a number of tasks.},
  file           = {:2019_Murphy_ICML.pdf:PDF;murphy19a.pdf:http\:/proceedings.mlr.press/v97/murphy19a/murphy19a.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.mlr.press/v97/murphy19a.html},
}

@TechReport{RePEc:crs:wpaper:2017-86,
  author         = {Gabriel Peyré and Marco Cuturi},
  institution    = {Center for Research in Economics and Statistics},
  title          = {{Computational Optimal Transport}},
  year           = {2017},
  month          = Oct,
  number         = {2017-86},
  type           = {Working Papers},
  abstract       = {Optimal Transport (OT) is a mathematical gem at the interface between probability, analysis and optimization. The goal of that theory is to define geometric tools that are useful to compare probability distributions. Let us briefly sketch some key ideas using a vocabulary that was first introduced by Monge two centuries ago: a probability distribution can be thought of as a pile of sand. Peaks indicate where likely observations are to appear. Given a pair of probability distributions—two different piles of sand—there are, in general, multiple ways to morph, transport or reshape the first pile so that it matches the second. To every such transport we associate an a “global” cost, using the “local” consideration of how much it costs to move a single grain of sand from one location to another. The goal of optimal transport is to find the least costly transport, and use it to derive an entire geometric toolbox for probability distributions. Despite this relatively abstract description, optimal transport theory answers many basic questions related to the way our economy works: In the “mines and factories” problem, the sand is distributed across an entire country, each grain of sand represents a unit of a useful raw resource; the target pile indicates where those resources are needed, typically in factories, where they are meant to be processed. In that scenario, one seeks the least costly way to move all these resources, knowing the entire logistic cost matrix needed to ship resources from any storage point to any factory. Transporting optimally two abstract distributions is also extremely relevant for mathematicians, in the sense that it defines a rich geometric structure on the space of probability distributions. That structure is canonical in the sense that it borrows, in arguably the most natural way, key geometric properties of the underlying “ground” space on which these distributions are defined. For instance, when the underlying space is Euclidean, key concepts s},
  file           = {:2019_Peyre.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://ideas.repec.org/p/crs/wpaper/2017-86.html},
}

@InProceedings{2019_Rieck_InProceedings,
  author         = {Rieck, Bastian and Bock, Christian and Borgwardt, Karsten},
  booktitle      = {Proceedings of the 36th International Conference on Machine Learning},
  title          = {{A} {P}ersistent {W}eisfeiler-{L}ehman {P}rocedure for {G}raph {C}lassification},
  year           = {2019},
  editor         = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  month          = {09--15 Jun},
  pages          = {5448--5458},
  publisher      = {PMLR},
  series         = {Proceedings of Machine Learning Research},
  volume         = {97},
  abstract       = {The Weisfeiler–Lehman graph kernel exhibits competitive performance in many graph classification tasks. However, its subtree features are not able to capture connected components and cycles, topological features known for characterising graphs. To extract such features, we leverage propagated node label information and transform unweighted graphs into metric ones. This permits us to augment the subtree features with topological information obtained using persistent homology, a concept from topological data analysis. Our method, which we formalise as a generalisation of Weisfeiler–Lehman subtree features, exhibits favourable classification accuracy and its improvements in predictive performance are mainly driven by including cycle information.},
  file           = {:2019_Rieck_PMLR.pdf:PDF;rieck19a.pdf:http\:/proceedings.mlr.press/v97/rieck19a/rieck19a.pdf:PDF},
  keywords       = {Weisfeiler-Lehman, Topological Data Analysis, Persistent Homology, Graph Classification, Cycles},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.mlr.press/v97/rieck19a.html},
}

@InProceedings{2019_Schulz_InProceedings,
  author         = {Till Hendrik Schulz and Pascal Welke},
  title          = {On the necessity of graph kernel baselines - poster},
  year           = {2019},
  publisher      = {IEEE},
  file           = {:2019_Schulz_POSTER.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://gem-ecmlpkdd.github.io/archive/2019/papers/GEM2019_paper_17.pdf},
}

@InProceedings{2020_Srinivasan_InProceedings,
  author         = {Balasubramaniam Srinivasan and Bruno Ribeiro},
  title          = {{O}n the {E}quivalence between {P}ositional {N}ode {E}mbeddings and {S}tructural {G}raph {R}epresentations},
  year           = {2020},
  abstract       = {This work provides the first unifying theoretical framework for node (positional)
embeddings and structural graph representations, bridging methods like matrix
factorization and graph neural networks. Using invariant theory, we show that
the relationship between structural representations and node embeddings is anal-
ogous to that of a distribution and its samples. We prove that all tasks that can
be performed by node embeddings can also be performed by structural represen-
tations and vice-versa. We also show that the concept of transductive and induc-
tive learning is unrelated to node embeddings and graph representations, clearing
another source of confusion in the literature. Finally, we introduce new practi-
cal guidelines to generating and using node embeddings, which fixes significant
shortcomings of standard operating procedures used today.},
  archiveprefix  = {arXiv},
  eprint         = {1910.00452},
  eprinttype     = {arxiv},
  file           = {:2019_Srinivasan.pdf:PDF},
  primaryclass   = {cs.LG},
  qualityassured = {qualityAssured},
}

@InProceedings{2019_Togninalli_InProceedings,
  author         = {Togninalli, Matteo and Ghisu, Elisabetta and Llinares-L\'{o}pez, Felipe and Rieck, Bastian and Borgwardt, Karsten},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{W}asserstein {W}eisfeiler-{L}ehman {G}raph {K}ernels},
  year           = {2019},
  editor         = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {32},
  abstract       = {Neural Information Processing Systems http://nips.cc/},
  file           = {:2019_Togninalli_NIPS.pdf:PDF},
  groups         = {GraphKernels},
  qualityassured = {qualityAssured},
  readstatus     = {read},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2019/file/73fed7fd472e502d8908794430511f4d-Paper.pdf},
}

@InProceedings{2020_Chami_InProceedings,
  author         = {Ines Chami and Adva Wolf and Da-Cheng Juan and Frederic Sala and Sujith Ravi and Christopher Ré},
  title          = {{L}ow-{D}imensional {H}yperbolic {K}nowledge {G}raph {E}mbeddings},
  year           = {2020},
  abstract       = {Knowledge graph (KG) embeddings learn low-dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention-based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.},
  archiveprefix  = {arXiv},
  eprint         = {2005.00545},
  file           = {:2020_Chami_CONF.pdf:PDF},
  groups         = {GraphKernels},
  primaryclass   = {cs.LG},
  qualityassured = {qualityAssured},
}

@InProceedings{2021_Cohen_InProceedings,
  author         = {Samuel Cohen and Michael Arbel and Marc Peter Deisenroth},
  title          = {{E}stimating {B}arycenters of {M}easures in {H}igh {D}imensions},
  year           = {2021},
  abstract       = {Proceedings of the International Conference on Machine Learning 2021},
  archiveprefix  = {arXiv},
  eprint         = {2007.07105},
  eprinttype     = {arxiv},
  file           = {:2020_Cohen_CONF.pdf:PDF},
  groups         = {ClusteringAlgorithms},
  keywords       = {Machine Learning, ICML},
  primaryclass   = {stat.ML},
  qualityassured = {qualityAssured},
}

@InProceedings{2003_Prakash_InProceedings,
  author         = {Vijay Prakash and Dwivedi1∗ Chaitanya and K. Joshi and vijaypra001@e.ntu.edu.sg chaitanya.joshi@ntu.edu.sg},
  title          = {{B}enchmarking {G}raph {N}eural {N}etworks},
  year           = {2003},
  file           = {:2020_Dwivedi_CONF.pdf:PDF},
  groups         = {GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://www.jmlr.org/papers/volume24/22-0567/22-0567.pdf},
}

@Book{2020_Hamilton_Book,
  author         = {William L. Hamilton},
  publisher      = {Springer International Publishing},
  title          = {Graph Representation Learning},
  year           = {2020},
  doi            = {10.1007/978-3-031-01588-5},
  file           = {:2020_Hamilton_BOOK.pdf:PDF},
  groups         = {Graphtheory, GraphKernels},
  qualityassured = {qualityAssured},
}

@InProceedings{2020_Hu_InProceedings,
  author         = {Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
  title          = {{O}pen {G}raph {B}enchmark: {D}atasets for {M}achine {L}earning on {G}raphs},
  year           = {2020},
  file           = {:2020_Hu_CONF.pdf:PDF},
  groups         = {GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper/2020/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html},
}

@InProceedings{2020_Mirhoseini_InProceedings,
  author         = {Azalia Mirhoseini and Anna Goldie and Mustafa Yazgan and Joe Jiang and Ebrahim Songhori and Shen Wang and Young-Joon Lee and Eric Johnson and Omkar Pathak and Sungmin Bae and Azade Nazi and Jiwoo Pak and Andy Tong and Kavya Srinivasa and William Hang and Emre Tuncer and Anand Babu and Quoc V. Le and James Laudon and Richard Ho and Roger Carpenter and Jeff Dean},
  title          = {{C}hip {P}lacement with {D}eep {R}einforcement {L}earning},
  year           = {2020},
  abstract       = {In this work, we present a learning-based approach to chip placement, one of the most complex and time-consuming stages of the chip design process. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. To achieve these results, we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to unseen blocks, we ground representation learning in the supervised task of predicting placement quality. By designing a neural architecture that can accurately predict reward across a wide variety of netlists and their placements, we are able to generate rich feature embeddings of the input netlists. We then use this architecture as the encoder of our policy and value networks to enable transfer learning. Our objective is to minimize PPA (power, performance, and area), and we show that, in under 6 hours, our method can generate placements that are superhuman or comparable on modern accelerator netlists, whereas existing baselines require human experts in the loop and take several weeks.},
  archiveprefix  = {arXiv},
  eprint         = {2004.10746},
  eprinttype     = {arxiv},
  file           = {:2020_Mirhoseini_CONF.pdf:PDF},
  primaryclass   = {cs.LG},
  qualityassured = {qualityAssured},
}

@InProceedings{2020_Morris_InProceedings,
  author         = {Christopher Morris and Nils M. Kriege and Franka Bause and Kristian Kersting and Petra Mutzel and Marion Neumann},
  title          = {{TUD}ataset: A collection of benchmark datasets for learning with graphs},
  year           = {2020},
  abstract       = {Graph Representation Learning and Beyond (GRL+), ICML 2020 Workshop},
  eprint         = {2007.08663},
  eprinttype     = {arxiv},
  file           = {:2020_Morris_CONF.pdf:PDF},
  groups         = {GraphKernels},
  keywords       = {graph learning, graph kernel, weisfeiler, leman, gnn, graph neural network, benchmark datasets},
  qualityassured = {qualityAssured},
}

@InProceedings{2020_SanchezGonzalez_InProceedings,
  author         = {Alvaro Sanchez-Gonzalez and Jonathan Godwin and Tobias Pfaff and Rex Ying and Jure Leskovec and Peter W. Battaglia},
  title          = {{L}earning to {S}imulate {C}omplex {P}hysics with {G}raph {N}etworks},
  year           = {2020},
  abstract       = {Proceedings of the International Conference on Machine Learning 2020},
  file           = {:2020_Sanchey-Gonzalez_PMLR.pdf:PDF},
  keywords       = {Machine Learning, Graph Neural Networks, Physical Simulation, Particle-based Fluid Simulation},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.mlr.press/v119/sanchez-gonzalez20a.html},
}

@InProceedings{2020_Siglidis_InProceedings,
  author         = {Giannis Siglidis and Giannis Nikolentzos and Stratis Limnios and Christos Giatsidis and Konstantinos Skianis and Michalis Vazirgiannis},
  title          = {{G}ra{K}e{L}: {A} {G}raph {K}ernel {L}ibrary in {P}ython},
  year           = {2020},
  number         = {54},
  pages          = {1--5},
  volume         = {21},
  file           = {:2020_Siglidis_JMLR.pdf:PDF},
  groups         = {GraphKernels},
  journal        = {Journal of Machine Learning Research},
  keywords       = {graph similarity, graph kernels, scikit-learn, Python},
  qualityassured = {qualityAssured},
  url            = {http://jmlr.org/papers/v21/18-370.html},
}

@InProceedings{2021_Zhang_InProceedings,
  author         = {Muhan Zhang and Pan Li and Yinglong Xia and Kai Wang and Long Jin},
  title          = {{R}evisiting {G}raph {N}eural {N}etworks for {L}ink {P}rediction},
  year           = {2021},
  abstract       = {Graph neural networks (GNNs) have achieved great success in recent years. Three most common applications include node classification, link prediction, and graph classification. While there is rich literature on node classification and graph classification, GNNs for link prediction is relatively less studied and less understood. Two representative classes of methods exist: GAE and SEAL. GAE (Graph Autoencoder) first uses a GNN to learn node embeddings for all nodes, and then aggregates the embeddings of the source and target nodes as their link representation. SEAL extracts a subgraph around the source and target nodes, labels the nodes in the subgraph, and then uses a GNN to learn a link representation from the labeled subgraph. In this paper, we thoroughly discuss the differences between these two classes of methods, and conclude that simply aggregating \textit{node} embeddings does not lead to effective \textit{link} representations, while learning from \textit{properly labeled subgraphs} around links provides highly expressive and generalizable link representations. Experiments on the recent large-scale OGB link prediction datasets show that SEAL has up to 195\% performance gains over GAE methods, achieving new state-of-the-art results on 3 out of 4 datasets.},
  file           = {:2020_Zhang_CONF.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://openreview.net/forum?id=8q_ca26L1fz},
}

@InProceedings{2021_Flamary_InProceedings,
  author         = {Flamary, R\'{e}mi and Courty, Nicolas and Gramfort, Alexandre and Alaya, Mokhtar Z. and Boisbunon, Aur\'{e}lie and Chambon, Stanislas and Chapel, Laetitia and Corenflos, Adrien and Fatras, Kilian and Fournier, Nemo and Gautheron, L\'{e}o and Gayraud, Nathalie T. H. and Janati, Hicham and Rakotomamonjy, Alain and Redko, Ievgen and Rolet, Antoine and Schutz, Antony and Seguy, Vivien and Sutherland, Danica J. and Tavenard, Romain and Tong, Alexander and Vayer, Titouan},
  title          = {{POT}: {P}ython {O}ptimal {T}ransport},
  year           = {2021},
  month          = {jan},
  number         = {1},
  publisher      = {JMLR.org},
  volume         = {22},
  abstract       = {Optimal transport has recently been reintroduced to the machine learning community thanks in part to novel efficient optimization procedures allowing for medium to large scale applications. We propose a Python toolbox that implements several key optimal transport ideas for the machine learning community. The toolbox contains implementations of a number of founding works of OT for machine learning such as Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research. This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.},
  articleno      = {78},
  doi            = {10.5555/3546258.3546336},
  file           = {:2021_Flamary_JMLR.pdf:PDF},
  issn           = {1532-4435},
  issue_date     = {January 2021},
  journal        = {J. Mach. Learn. Res.},
  keywords       = {divergence, domain adaptation, optimal transport, optimization},
  numpages       = {8},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
}

@Misc{2021_Schulz_Misc,
  author         = {Schulz, Till Hendrik and Horváth, Tamás and Welke, Pascal and Wrobel, Stefan},
  title          = {A Generalized Weisfeiler-Lehman Graph Kernel},
  year           = {2021},
  abstract       = {more than it resembles T3, the Weisfeiler-Lehman ker-
The Weisfeiler-Lehman graph kernels are among the most nel [11] simply treats them all as unequal and is thus
prevalent graph kernels due to their remarkable time com- unable to quantify the apparent difference among the
plexity and predictive performance. Their key concept is
based on an implicit comparison of neighborhood repre- pairwise similarities between the unfolding trees.
senting trees with respect to equality (i.e., isomorphism). Motivated by these considerations, we relax the
This binary valued comparison is, however, arguably too above strictness by proposing a method which com-
rigid for defining suitable similarity measures over graphs.
To overcome this limitation, we propose a generalization of pares Weisfeiler-Lehman labels, or equivalently unfold-
Weisfeiler-Lehman graph kernels which takes into account ing trees, with regard to a much finer similarity measure
the similarity between trees rather than equality. We achieve than the binary valued one. More precisely, we employ
this using a specifically fitted variation of the well-known tree
edit distance which can efficiently be calculated. We em- a similarity between Weisfeiler-Lehman labels based on
pirically show that our approach significantly outperforms the concept of tree edit distances between their respec-
state-of-the-art methods in terms of predictive performance tive unfolding trees. These kind of distances provide
on datasets containing structurally more complex graphs be-
yond the typically considered molecular graphs. a natural comparison for trees. On an abstract level,},
  comment        = {Unfolding tree definition},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.2101.08104},
  file           = {:2021_Schulz_CONF.pdf:PDF},
  groups         = {GraphKernels, Weisfeiler-Lehman-Related},
  keywords       = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
  readstatus     = {read},
  url            = {https://ui.adsabs.harvard.edu/abs/2021arXiv210108104H/abstract},
}

@Article{2022_Schulz_Article,
  author         = {Schulz, Till and Welke, Pascal and Wrobel, Stefan},
  journal        = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title          = {{G}raph {F}iltration {K}ernels},
  year           = {2022},
  month          = {Jun.},
  number         = {8},
  pages          = {8196-8203},
  volume         = {36},
  abstract       = {substructures in terms of equivalence. We refer to these ker-
nels as histogram kernels. While they prove to be successful},
  doi            = {10.1609/aaai.v36i8.20793},
  file           = {:2021_Schulz_CONFa.pdf:PDF},
  groups         = {GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://ojs.aaai.org/index.php/AAAI/article/view/20793},
}

@InProceedings{1968_Weisfeiler_InProceedings,
  author         = {Weisfeiler, Boris and Leman, Andrei},
  title          = {The reduction of a graph to canonical form and the algebra which appears therein},
  year           = {1968},
  number         = {9},
  pages          = {12--16},
  volume         = {2},
  abstract       = {We consider an algorithm for the reduction of a given finite multigraph Γ to
canonical form. Therein the new invariant of a graph appears — the algebra A(Γ). The
study of properties of the algebra A(Γ) turns out to be helpful in solving a number of
graph-theoretic problems. We pose and discuss some conjectures on the relation between
properties of the algebra A(Γ) and the automorphism group Aut(Γ) of a graph Γ. We give
an example of undirected graph Γ whose algebra A(Γ) coincides with the group algebra of
some noncommutative group.},
  file           = {:1968_Weisfeiler_CONF.pdf:PDF},
  groups         = {Graphtheory},
  journal        = {nti, Series},
  qualityassured = {qualityAssured},
  url            = {https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf},
}

@Article{1976_Ullmann_Article,
  author         = {Ullmann, Julian R.},
  journal        = {Journal of the ACM (JACM)},
  title          = {An algorithm for subgraph isomorphism},
  year           = {1976},
  month          = {jan},
  number         = {1},
  pages          = {31--42},
  volume         = {23},
  abstract       = {Subgraph isomorphism can be determined by means of a brute-force tree-search enu-
meration procedure. In this paper a new algorithm is introduced that attains efficiencyb y inferentially 
eliminatings uccessor nodes in the tree search. To assess the time actually taken by the new algomthm, 
subgraph isomorphism, chque detection, graph isomorphism, and directed graph isomorphism ex- 
periments have been carried out with random and with various nonrandom graphs.},
  doi            = {10.1145/321921.321925},
  file           = {:1976_Ullmann_CONF.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  publisher      = {ACM New York, NY, USA},
  qualityassured = {qualityAssured},
  readstatus     = {read},
}

@Article{1980_Babai_Article,
  author         = {L{\'{a}}szl{\'{o}} Babai and Paul Erdo{\H{}}s and Stanley M. Selkow},
  journal        = {{SIAM} Journal on Computing},
  title          = {{R}andom {G}raph {I}somorphism},
  year           = {1980},
  month          = {aug},
  number         = {3},
  pages          = {628--635},
  volume         = {9},
  abstract       = {SIAM J. Comput. 1980.9:628-635},
  doi            = {10.1137/0209047},
  file           = {:1980_Babai_SIAM.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  keywords       = {graph,isomorphism testing,canonical labeling,random graph,naive algorithm,average-case analysis,linear time,degree sequence of a graph},
  publisher      = {SIAM},
  qualityassured = {qualityAssured},
}

@Book{2007_Mitchell_Book,
  author         = {Mitchell, Tom Michael and others},
  publisher      = {McGraw-hill New York},
  title          = {Machine learning},
  year           = {2007},
  volume         = {1},
  file           = {:1983_Mitchell_BOOK.pdf:PDF},
  groups         = {MachineLearning},
  qualityassured = {qualityAssured},
  url            = {https://library.iitgn.ac.in/documents/library_files/2016/19032016.pdf},
}

@Article{1992_Cai_Article,
  author         = {Cai, Jin-Yi and F{\"u}rer, Martin and Immerman, Neil},
  journal        = {Combinatorica},
  title          = {An optimal lower bound on the number of variables for graph identification},
  year           = {1992},
  month          = {dec},
  number         = {4},
  pages          = {389--410},
  volume         = {12},
  doi            = {10.1007/bf01305232},
  file           = {:1992_Cai_IEEE.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
  readstatus     = {read},
  url            = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e8151a322b62a37b03c105d0033c3775b00f1ee1},
}

@Article{1992_Wilkinson_Article,
  author         = {Wilkinson, Leland and others},
  journal        = {Retrieved February},
  title          = {{T}ree structured data analysis: {AID}, {CHAID} and {CART}},
  year           = {1992},
  pages          = {2008},
  volume         = {1},
  file           = {:1992_Wilkinson_CONF.pdf:PDF},
  groups         = {Graphtheory},
  qualityassured = {qualityAssured},
  url            = {https://www.researchgate.net/profile/Leland-Wilkinson/publication/228698437_Tree_structured_data_analysis_AID_CHAID_and_CART/links/55e72a2a08ae21d099c148c3/Tree-structured-data-analysis-AID-CHAID-and-CART.pdf},
}

@Article{1992_Zhang_Article,
  author         = {Zhang, Kaizhong and Statman, Rick and Shasha, Dennis},
  journal        = {Information processing letters},
  title          = {On the editing distance between unordered labeled trees},
  year           = {1992},
  number         = {3},
  pages          = {133--139},
  volume         = {42},
  file           = {:1992_Zhang_CONF.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  keywords       = {Computational complexity, unordered trees},
  publisher      = {Elsevier},
  qualityassured = {qualityAssured},
  url            = {https://www.sciencedirect.com/science/article/pii/002001909290136J},
}

@InProceedings{1993_Zhang_InProceedings,
  author         = {Zhang, Kaizhong},
  booktitle      = {Combinatorial Pattern Matching: 4th Annual Symposium, CPM 93 Padova, Italy, June 2--4, 1993 Proceedings 4},
  title          = {A new editing based distance between unordered labeled trees},
  year           = {1993},
  organization   = {Springer},
  pages          = {254--265},
  doi            = {10.1007/BFb0029810},
  file           = {:1993_Zhang_CONF.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  qualityassured = {qualityAssured},
}

@Article{1995_LeCun_Article,
  author         = {LeCun, Yann and Bengio, Yoshua and others},
  journal        = {The handbook of brain theory and neural networks},
  title          = {Convolutional networks for images, speech, and time series},
  year           = {1995},
  number         = {10},
  pages          = {1995},
  volume         = {3361},
  file           = {:1995_LeCun_CONF.pdf:PDF},
  groups         = {Convolutional networks, ConvolutionNetworks},
  publisher      = {Citeseer},
  qualityassured = {qualityAssured},
  url            = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e26cc4a1c717653f323715d751c8dea7461aa105},
}

@Article{1998_Lecun_Article,
  author         = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal        = {Proceedings of the {IEEE}},
  title          = {Gradient-based learning applied to document recognition},
  year           = {1998},
  number         = {11},
  pages          = {2278--2324},
  volume         = {86},
  abstract       = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  doi            = {10.1109/5.726791},
  file           = {:1998_Lecun_IEEE.pdf:PDF},
  groups         = {MachineLearning},
  publisher      = {Institute of Electrical and Electronics Engineers ({IEEE})},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/726791/},
}

@Article{2003_Dobson_Article,
  author         = {Dobson, Paul D. and Doig, Andrew J.},
  journal        = {Journal of molecular biology},
  title          = {Distinguishing enzyme structures from non-enzymes without alignments},
  year           = {2003},
  number         = {4},
  pages          = {771--783},
  volume         = {330},
  doi            = {10.1016/S0022-2836(03)00628-4},
  file           = {:2003_Dobson_CONF.pdf:PDF},
  keywords       = {protein function prediction; structure; enzyme; machine *Corresponding author learning; structural genomics},
  publisher      = {Elsevier},
  qualityassured = {qualityAssured},
  url            = {https://www.sciencedirect.com/science/article/pii/S0022283603006284?casa_token=IX2kOcHWpSgAAAAA:7AxKoWztPZf1e993W32gyVYDnkEoihOl359BDY2EORGTaL4EpE2tZFtd-Y_sWjzkjZkXEikscNY},
}

@Misc{2003_Hsu_Misc,
  author         = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen and others},
  title          = {A practical guide to support vector classification},
  year           = {2003},
  abstract       = {The support vector machine (SVM) is a popular classification technique.
However, beginners who are not familiar with SVM often get unsatisfactory
results since they miss some easy but significant steps. In this guide, we propose
a simple procedure which usually gives reasonable results.},
  file           = {:2003_Hsu_CONF.pdf:PDF},
  groups         = {Support Vector Machines, SupportVectorMachines},
  publisher      = {Taipei, Taiwan},
  qualityassured = {qualityAssured},
  url            = {http://www.datascienceassn.org/sites/default/files/Practical Guide to Support Vector Classification.pdf},
}

@Article{2003_Kashima_Article,
  author         = {Hisashi Kashima and Koji Tsuda and Akihiro Inokuchi},
  title          = {Marginalized kernels between labeled graphs},
  year           = {2003},
  abstract       = {Proceedings of the Twentieth International Conference on Machine Learning},
  file           = {:2003_Kashima_CONF.pdf:PDF},
  groups         = {GraphKernels},
  keywords       = {Compilation copyright ©2003, American Association for Artificial Intelligence. All rights reserved.},
  publisher      = {IEEE},
  qualityassured = {qualityAssured},
  url            = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_3187440},
}

@InProceedings{2003_Nijssen_InProceedings,
  author         = {Siegfried Nijssen and Joost N. Kok},
  title          = {{E}fficient {D}iscovery of {F}requent {U}nordered {T}rees},
  year           = {2003},
  abstract       = {Recently, an algorithm called Freqt was introduced which
enumerates all frequent induced subtrees in an ordered data tree. We
propose a new algorithm for mining unordered frequent induced sub-
trees. We show that the complexity of enumerating unordered trees is
not higher than the complexity of enumerating ordered trees; a strategy
for determining the frequency of unordered trees is introduced.},
  file           = {:2003_Nijssen_CONF.pdf:PDF},
  groups         = {Graphtheory},
  qualityassured = {qualityAssured},
  url            = {https://dial.uclouvain.be/pr/boreal/object/boreal:186570/datastream/PDF_01/view},
}

@InProceedings{2004_Horvath_InProceedings,
  author         = {Horv{\'a}th, Tam{\'a}s and G{\"a}rtner, Thomas and Wrobel, Stefan},
  booktitle      = {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  title          = {Cyclic pattern kernels for predictive graph mining},
  year           = {2004},
  pages          = {158--167},
  abstract       = {With applications in biology, the world-wide web, and sev-
eral other areas, mining of graph-structured objects has re-
ceived signiﬁcant interest recently. One of the major re-
search directions in this ﬁeld is concerned with predictive
data mining in graph databases where each instance is repre-
sented by a graph. Some of the proposed approaches for this
task rely on the excellent classiﬁcation performance of sup-
port vector machines. To control the computational cost of
these approaches, the underlying kernel functions are based
on frequent patterns. In contrast to these approaches, we
propose a kernel function based on a natural set of cyclic and
tree patterns independent of their frequency, and discuss its
computational aspects. To practically demonstrate the ef-
fectiveness of our approach, we use the popular NCI-HIV
molecule dataset. Our experimental results show that cyclic
pattern kernels can be computed quickly and oﬀer predic-
tive performance superior to recent graph kernels based on
frequent patterns.},
  doi            = {10.1145/1014052.1014072},
  file           = {:2004_Horvath_KDD.pdf:PDF},
  groups         = {AppliedGraphtheory, GraphKernels},
  keywords       = {tured graphs while being efficient enough to be applied to},
  qualityassured = {qualityAssured},
}

@InProceedings{2004_Mahe_InProceedings,
  author         = {Mah{\'e}, Pierre and Ueda, Nobuhisa and Akutsu, Tatsuya and Perret, Jean-Luc and Vert, Jean-Philippe},
  booktitle      = {Proceedings of the twenty-first international conference on Machine learning},
  title          = {Extensions of marginalized graph kernels},
  year           = {2004},
  pages          = {70},
  publisher      = {{ACM} Press},
  abstract       = {tial drugs, require the analysis, comparison, and clas-
sification of these graphs. Among the many different},
  doi            = {10.1145/1015330.1015446},
  file           = {:2004_Mahe_ICML.pdf:PDF},
  groups         = {AppliedGraphtheory, GraphKernels},
  qualityassured = {qualityAssured},
}

@Article{2005_Bille_Article,
  author         = {Bille, Philip},
  journal        = {Theoretical computer science},
  title          = {A survey on tree edit distance and related problems},
  year           = {2005},
  month          = {jun},
  number         = {1-3},
  pages          = {217--239},
  volume         = {337},
  doi            = {10.1016/j.tcs.2004.12.030},
  file           = {:2005_Bille_TCS.pdf:PDF},
  groups         = {Graphtheory},
  keywords       = {Tree matching; Tree edit distance; Tree alignment; Tree inclusion},
  publisher      = {Elsevier},
  qualityassured = {qualityAssured},
}

@Article{2005_Borgwardt_Article,
  author         = {K. M. Borgwardt and C. S. Ong and S. Schonauer and S. V. N. Vishwanathan and A. J. Smola and H.-P. Kriegel},
  journal        = {Bioinformatics},
  title          = {Protein function prediction via graph kernels},
  year           = {2005},
  month          = {jun},
  number         = {Suppl 1},
  pages          = {i47--i56},
  volume         = {21},
  abstract       = {known function is consequently the basis of current function
Motivation: Computational approaches to protein function prediction (Whisstock and Lesk, 2003). A newly discovered
prediction infer protein function by finding proteins with sim- protein is predicted to exert the same function as the most
ilar sequence, structure, surface clefts, chemical properties, similar proteins in a database of known proteins. This simil-
amino acid motifs, interaction partners or phylogenetic pro- arity among proteins can be defined in a multitude of ways:
files. We present a new approach that combines sequential, two proteins can be regarded to be similar, if their sequences
structural and chemical information into one graph model of align well [e.g. PSI-BLAST (Altschul et al., 1997)], if their
proteins. We predict functional class membership of enzymes structures match well [e.g. DALI (Holm and Sander, 1996)],
and non-enzymes using graph kernels and support vector if both have common surface clefts or bindings sites [e.g.
machine classification on these protein graphs. CASTp (Binkowski et al., 2003)], similar chemical fea-
Results: Our graph model, derivable from protein sequence tures or common interaction partners [e.g. DIP (Xenarios
and structure only, is competitive with vector models that et al., 2002)], if both contain certain motifs of amino acids
require additional protein information, such as the size of (AAs) [e.g. Evolutionary Trace (Yao et al., 2003)] or if both
surface pockets. If we include this extra information into our appear in the same range of species (Pellegrini et al., 1999).
graph model, our classifier yields significantly higher accuracy An armada of protein function prediction systems that meas-
levels than the vector models. Hyperkernels allow us to select ure protein similarity by one of the conditions above has been
and to optimally combine the most relevant node attributes in developed. Each of these conditions is based on a biological
our protein graphs. We have laid the foundation for a protein hypothesis; e.g. structural similarity implies that two proteins
function prediction system that integrates protein information could share a common ancestor and that they both could per-
from various sources efficiently and effectively. form the same function as this common ancestor (Bartlett
Availability: More information available via www.dbs.ifi.lmu. et al., 2003).
de/Mitarbeiter/borgwardt.html. These assumptions are not universally valid. Hegyi and
Contact: borgwardt@dbs.ifi.lmu.de Gerstein (1999) showed that proteins with similar function},
  doi            = {10.1093/bioinformatics/bti1007},
  file           = {:2005_Borgwardt_CONF.pdf:PDF},
  groups         = {AppliedGraphtheory, GraphKernels},
  publisher      = {Oxford University Press ({OUP})},
  qualityassured = {qualityAssured},
}

@InProceedings{2005_Borgwardt_InProceedings,
  author         = {Borgwardt, K. M. and Kriegel, H. P.},
  booktitle      = {Fifth IEEE International Conference on Data Mining (ICDM'05)},
  title          = {Shortest-path kernels on graphs},
  year           = {2005},
  pages          = {8 pp.-},
  abstract       = {this purpose. However, kernels on these substructures are
either computationally expensive, sometimes even NP-hard},
  doi            = {10.1109/ICDM.2005.132},
  file           = {:2005_Borgwardt_IEEE.pdf:PDF},
  groups         = {GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/1565664/},
}

@InProceedings{2005_Boughorbel_InProceedings,
  author         = {Boughorbel, S. and Tarel, J.-P. and Boujemaa, N.},
  booktitle      = {IEEE International Conference on Image Processing 2005},
  title          = {Generalized histogram intersection kernel for image recognition},
  year           = {2005},
  pages          = {III-161},
  volume         = {3},
  doi            = {10.1109/ICIP.2005.1530353},
  file           = {:2005_Boughorbel_IEEE.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/1530353/},
}

@InProceedings{2005_Froehlich_InProceedings,
  author         = {Fr\"{o}hlich, Holger and Wegner, J\"{o}rg K. and Sieker, Florian and Zell, Andreas},
  booktitle      = {Proceedings of the 22nd International Conference on Machine Learning},
  title          = {{O}ptimal {A}ssignment {K}ernels for {A}ttributed {M}olecular {G}raphs},
  year           = {2005},
  address        = {New York, NY, USA},
  pages          = {225–232},
  publisher      = {Association for Computing Machinery},
  series         = {ICML '05},
  abstract       = {We propose a new kernel function for attributed molecular graphs, which is based on the idea of computing an optimal assignment from the atoms of one molecule to those of another one, including information on neighborhood, membership to a certain structural element and other characteristics for each atom. As a byproduct this leads to a new class of kernel functions. We demonstrate how the necessary computations can be carried out efficiently. Compared to marginalized graph kernels our method in some cases leads to a significant reduction of the prediction error. Further improvement can be gained, if expert knowledge is combined with our method. We also investigate a reduced graph representation of molecules by collapsing certain structural elements, like e.g. rings, into a single node of the molecular graph.},
  doi            = {10.1145/1102351.1102380},
  file           = {:2005_Froehlich_ICML.pdf:PDF},
  groups         = {GraphKernels},
  isbn           = {1595931805},
  location       = {Bonn, Germany},
  numpages       = {8},
  qualityassured = {qualityAssured},
  readstatus     = {read},
}

@InProceedings{2005_Yang_InProceedings,
  author         = {Yang, Rui and Kalnis, Panos and Tung, Anthony K. H.},
  booktitle      = {Proceedings of the 2005 ACM SIGMOD international conference on Management of data},
  title          = {Similarity evaluation on tree-structured data},
  year           = {2005},
  pages          = {754--765},
  abstract       = {and the linear representation of data. They allow the expres-},
  doi            = {10.1145/1066157.1066243},
  file           = {:2005_Yang_CONF.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  qualityassured = {qualityAssured},
}

@Book{2006_Bishop_Book,
  author         = {Bishop, Christopher M. and Nasrabadi, Nasser M.},
  publisher      = {Springer},
  title          = {Pattern recognition and machine learning},
  year           = {2006},
  number         = {4},
  volume         = {4},
  file           = {:2006_Bishop_BOOK.pdf:PDF},
  groups         = {MachineLearning},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
  url            = {https://link.springer.com/book/9780387310732},
}

@PhdThesis{2007_Borgwardt_PhdThesis,
  author         = {Karsten Michael Borgwardt},
  title          = {{G}raph {K}ernels},
  year           = {2007},
  file           = {:2007_Borgwardt_BOOK.pdf:PDF},
  groups         = {AppliedGraphtheory, GraphKernels},
  qualityassured = {qualityAssured},
}

@InProceedings{2007_Harchaoui_InProceedings,
  author         = {Harchaoui, Zaid and Bach, Francis},
  booktitle      = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
  title          = {{I}mage {C}lassification with {S}egmentation {G}raph {K}ernels},
  year           = {2007},
  pages          = {1-8},
  doi            = {10.1109/CVPR.2007.383049},
  file           = {:2007_Harchaoui_IEEE.pdf:PDF},
  groups         = {GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://ieeexplore.ieee.org/abstract/document/4270074/},
}

@Article{2007_Wale_Article,
  author         = {Nikil Wale and Ian A. Watson and George Karypis},
  journal        = {Knowledge and Information Systems},
  title          = {Comparison of descriptor spaces for chemical compound retrieval and classification},
  year           = {2007},
  month          = {aug},
  number         = {3},
  pages          = {347--375},
  volume         = {14},
  abstract       = {In recent years the development of computational techniques that build models
to correctly assign chemical compounds to various classes or to retrieve potential drug-like
compounds has been an active area of research. Many of the best-performing techniques for
these tasks utilize a descriptor-based representation of the compound that captures various
aspects of the underlying molecular graph’s topology. In this paper we compare five dif-
ferent set of descriptors that are currently used for chemical compound classification. We
also introduce four different descriptors derived from all connected fragments present in the
molecular graphs primarily for the purpose of comparing them to the currently used des-
criptor spaces and analyzing what properties of descriptor spaces are helpful in providing
effective representation for molecular graphs. In addition, we introduce an extension to exis-
ting vector-based kernel functions to take into account the length of the fragments present
in the descriptors. We experimentally evaluate the performance of the previously introduced
and the new descriptors in the context of SVM-based classification and ranked-retrieval on
28 classification and retrieval problems derived from 18 datasets. Our experiments show that
for both of these tasks, two of the four descriptors introduced in this paper along with the
extended connectivity fingerprint based descriptors consistently and statistically outperform
previously developed schemes based on the widely used fingerprint- and Maccs keys-based
descriptors, as well as recently introduced descriptors obtained by mining and analyzing the
structure of the molecular graphs.},
  doi            = {10.1007/s10115-007-0103-5},
  file           = {:2007_Wale_CONF.pdf:PDF},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Article{2008_Hofmann_Article,
  author         = {Thomas Hofmann and Bernhard Schölkopf and Alexander J. Smola},
  journal        = {The Annals of Statistics},
  title          = {Kernel methods in machine learning},
  year           = {2008},
  month          = {jun},
  number         = {3},
  volume         = {36},
  abstract       = {The Annals of Statistics, 2008, Vol.36, No.3, 1171-1220},
  doi            = {10.1214/009053607000000677},
  file           = {:2008_Hofmann_CONF.pdf:PDF},
  groups         = {MachineLearning},
  publisher      = {Institute of Mathematical Statistics},
  qualityassured = {qualityAssured},
}

@InProceedings{2008_Kondor_InProceedings,
  author         = {Risi Kondor and Karsten M. Borgwardt},
  booktitle      = {Proceedings of the 25th international conference on Machine learning - {ICML} {\textquotesingle}08},
  title          = {{T}he {S}kew {S}pectrum of {G}raphs},
  year           = {2008},
  publisher      = {{ACM} Press},
  abstract       = {Given a graph G, the two main lines of research that
have emerged to address the above problem focus re-},
  doi            = {10.1145/1390156.1390219},
  file           = {:2008_Kondor_ICML.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  qualityassured = {qualityAssured},
}

@Article{2008_Mahe_Article,
  author         = {Pierre Mah{\'{e}} and Jean-Philippe Vert},
  journal        = {Machine Learning},
  title          = {Graph kernels based on tree patterns for molecules},
  year           = {2008},
  month          = {oct},
  number         = {1},
  pages          = {3--35},
  volume         = {75},
  abstract       = {Motivated by chemical applications, we revisit and extend a family of positive
definite kernels for graphs based on the detection of common subtrees, initially proposed by
Ramon and Gärtner (Proceedings of the first international workshop on mining graphs, trees
and sequences, pp. 65–74, 2003). We propose new kernels with a parameter to control the
complexity of the subtrees used as features to represent the graphs. This parameter allows to
smoothly interpolate between classical graph kernels based on the count of common walks,
on the one hand, and kernels that emphasize the detection of large common subtrees, on the
other hand. We also propose two modular extensions to this formulation. The first extension
increases the number of subtrees that define the feature space, and the second one removes
noisy features from the graph representations. We validate experimentally these new kernels
on problems of toxicity and anti-cancer activity prediction for small molecules with support
vector machines.},
  doi            = {10.1007/s10994-008-5086-2},
  file           = {:2008_Mahé_CONF.pdf:PDF},
  groups         = {AppliedGraphtheory, GraphKernels},
  keywords       = {Graph kernels · Support vector machines · Chemoinformatics},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@InProceedings{2009_Kondor_InProceedings,
  author         = {Risi Kondor and Nino Shervashidze and Karsten M. Borgwardt},
  booktitle      = {Proceedings of the 26th Annual International Conference on Machine Learning - {ICML} {\textquotesingle}09},
  title          = {The graphlet spectrum},
  year           = {2009},
  publisher      = {{ACM} Press},
  abstract       = {In this paper, we overcome these two limitations by},
  doi            = {10.1145/1553374.1553443},
  file           = {:2009_Kondor_ICML.pdf:PDF},
  groups         = {AppliedGraphtheory, MachineLearning},
  qualityassured = {qualityAssured},
}

@Article{2008_Kulis_Article,
  author         = {Brian Kulis and Sugato Basu and Inderjit Dhillon and Raymond Mooney},
  journal        = {Machine Learning},
  title          = {Semi-supervised graph clustering: a kernel approach},
  year           = {2008},
  month          = {sep},
  number         = {1},
  pages          = {1--22},
  volume         = {74},
  abstract       = {Semi-supervised clustering algorithms aim to improve clustering results using
limited supervision. The supervision is generally given as pairwise constraints; such con-
straints are natural for graphs, yet most semi-supervised clustering algorithms are designed
for data represented as vectors. In this paper, we unify vector-based and graph-based ap-
proaches. We first show that a recently-proposed objective function for semi-supervised
clustering based on Hidden Markov Random Fields, with squared Euclidean distance and
a certain class of constraint penalty functions, can be expressed as a special case of the
weighted kernel k-means objective (Dhillon et al., in Proceedings of the 10th International
Conference on Knowledge Discovery and Data Mining, 2004a). A recent theoretical con-
nection between weighted kernel k-means and several graph clustering objectives enables
us to perform semi-supervised clustering of data given either as vectors or as a graph. For
graph data, this result leads to algorithms for optimizing several new semi-supervised graph
clustering objectives. For vector data, the kernel approach also enables us to find clusters
with non-linear boundaries in the input data space. Furthermore, we show that recent work
on spectral learning (Kamvar et al., in Proceedings of the 17th International Joint Confer-
ence on Artificial Intelligence, 2003) may be viewed as a special case of our formulation.
We empirically show that our algorithm is able to outperform current state-of-the-art semi-
supervised algorithms on both vector-based and graph-based data sets.},
  doi            = {10.1007/s10994-008-5084-4},
  file           = {:2009_Kulis_SpringerMachineLearning.pdf:PDF},
  keywords       = {Semi-supervised clustering · Kernel k-means · Graph clustering · Spectral learning},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Article{2009_Scarselli_Article,
  author         = {Franco Scarselli and Marco Gori and Ah Chung Tsoi and Markus Hagenbuchner and Gabriele Monfardini},
  journal        = {{IEEE} Transactions on Neural Networks},
  title          = {{T}he {G}raph {N}eural {N}etwork {M}odel},
  year           = {2009},
  month          = {jan},
  number         = {1},
  pages          = {61--80},
  volume         = {20},
  doi            = {10.1109/tnn.2008.2005605},
  file           = {:2009_Scarselli_IEEE.pdf:PDF},
  groups         = {Graph Neural Networks, AppliedGraphtheory, GraphKernels},
  publisher      = {Institute of Electrical and Electronics Engineers ({IEEE})},
  qualityassured = {qualityAssured},
}

@InProceedings{2009_NinoShervashidze_InProceedings,
  author         = {Nino Shervashidze, Karsten M. Borgwardt},
  title          = {Fast subtree kernels on graphs},
  year           = {2009},
  volume         = {22},
  abstract       = {Neural Information Processing Systems http://nips.cc/},
  file           = {:2009_Shervashidze_NIPS.pdf:PDF},
  groups         = {AppliedGraphtheory, GraphKernels},
  journal        = {Advances in neural information processing systems},
  qualityassured = {qualityAssured},
  readstatus     = {read},
  url            = {https://proceedings.neurips.cc/paper/3813-fast-subtree-kernels-on-graphs},
}

@InProceedings{2010_FabrizioCosta_InProceedings,
  author         = {Fabrizio Costa, Kurt De Grave},
  title          = {{F}ast {N}eighborhood {S}ubgraph {P}airwise {D}istance {K}ernel},
  year           = {2010},
  abstract       = {performance (Ben-David et al., 2002). Possible reme-},
  file           = {:2010_Costa_ICML.pdf:PDF},
  groups         = {Graph Kernels, GraphKernels},
  keywords       = {graph kernel, chemoinformatics, molecular graph},
  qualityassured = {qualityAssured},
}

@InProceedings{2011_Bonneel_InProceedings,
  author         = {Nicolas Bonneel and Michiel van de Panne and Sylvain Paris and Wolfgang Heidrich},
  title          = {{D}isplacement {I}nterpolation {U}sing {L}agrangian {M}ass {T}ransport},
  year           = {2011},
  abstract       = {1 Introduction},
  file           = {:2011_Bonneel_SIGGRAPH.pdf:PDF},
  keywords       = {displacement interpolation, mass transport rection is, subjectively speaking, better defined as having a single},
  qualityassured = {qualityAssured},
}

@Article{2011_Chang_Article,
  author         = {Chih-Chung Chang and Chih-Jen Lin},
  journal        = {{ACM} Transactions on Intelligent Systems and Technology},
  title          = {{LIBSVM}},
  year           = {2011},
  month          = {apr},
  number         = {3},
  pages          = {1--27},
  volume         = {2},
  doi            = {10.1145/1961189.1961199},
  file           = {:2011_Chang_ACM.pdf:PDF},
  publisher      = {Association for Computing Machinery ({ACM})},
  qualityassured = {qualityAssured},
}

@Article{2011_Shervashidze_Article,
  author         = {Shervashidze, Nino and Schweitzer, Pascal and Van Leeuwen, Erik Jan and Mehlhorn, Kurt and Borgwardt, Karsten M.},
  journal        = {Journal of Machine Learning Research},
  title          = {Weisfeiler-lehman graph kernels.},
  year           = {2011},
  number         = {9},
  volume         = {12},
  file           = {:2011_Shervashidze_JMLR.pdf:PDF},
  groups         = {Graph Kernels, GraphKernels},
  keywords       = {graph kernels, graph classification, similarity measures for graphs, Weisfeiler-Lehman algorithm},
  qualityassured = {qualityAssured},
  readstatus     = {read},
  url            = {https://www.jmlr.org/papers/volume12/shervashidze11a/shervashidze11a.pdf},
}

@Article{2011_Wang_Article,
  author         = {Wang, Haizhou and Song, Mingzhou},
  journal        = {The R journal},
  title          = {Ckmeans. 1d. dp: optimal k-means clustering in one dimension by dynamic programming},
  year           = {2011},
  number         = {2},
  pages          = {29},
  volume         = {3},
  file           = {:2011_Wang_CONF.pdf:PDF},
  publisher      = {NIH Public Access},
  qualityassured = {qualityAssured},
  url            = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5148156/},
}

@Article{2012_Jovanovic_Article,
  author         = {Irena Jovanovi{\'{c}} and Zoran Stani{\'{c}}},
  journal        = {Linear Algebra and its Applications},
  title          = {Spectral distances of graphs},
  year           = {2012},
  month          = {mar},
  number         = {5},
  pages          = {1425--1435},
  volume         = {436},
  abstract       = {Linear Algebra and Its Applications, 436 (2012) 1425-1435. doi:10.1016/j.laa.2011.08.019},
  doi            = {10.1016/j.laa.2011.08.019},
  file           = {:2012_Jovanovic_ELSEVIER.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  keywords       = {Spectral distance, Adjacency matrix, Graph energy, Cospectrality measure, Spectral diameter},
  publisher      = {Elsevier {BV}},
  qualityassured = {qualityAssured},
}

@InProceedings{2012_NilsKriege_InProceedings,
  author         = {Nils Kriege, Petra Mutzel},
  title          = {{S}ubgraph {M}atching {K}ernels for {A}ttributed {G}raphs},
  year           = {2012},
  abstract       = {Proceedings of the International Conference on Machine Learning 2010},
  eprint         = {1206.6483},
  file           = {:2012_Kriege_CONF.pdf:PDF},
  groups         = {Graph Kernels, GraphKernels},
  keywords       = {graph matching, common subgraph, graph kernel, machine learning, ICML},
  qualityassured = {qualityAssured},
}

@Article{2013_Bruna_Article,
  author         = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
  journal        = {arXiv preprint arXiv:1312.6203},
  title          = {Spectral networks and locally connected networks on graphs},
  year           = {2013},
  eprint         = {1312.6203},
  eprinttype     = {arxiv},
  file           = {:2013_Bruna_CONF.pdf:PDF},
  groups         = {Graph Neural Networks, Graphtheory},
  qualityassured = {qualityAssured},
}

@Article{2013_Oury_Article,
  author         = {Nicolas Oury and Michael Pedersen and Rasmus Petersen},
  journal        = {Electronic Proceedings in Theoretical Computer Science},
  title          = {{C}anonical {L}abelling of {S}ite {G}raphs},
  year           = {2013},
  month          = {jun},
  pages          = {13--28},
  volume         = {116},
  abstract       = {We investigate algorithms for canonical labelling of site graphs, i.e. graphs in which edges bind vertices on sites with locally unique names. We first show that the problem of canonical labelling of site graphs reduces to the problem of canonical labelling of graphs with edge colourings. We then present two canonical labelling algorithms based on edge enumeration, and a third based on an extension of Hopcroft's partition refinement algorithm. All run in quadratic worst case time individually. However, one of the edge enumeration algorithms runs in sub-quadratic time for graphs with "many" automorphisms, and the partition refinement algorithm runs in sub-quadratic time for graphs with "few" bisimulation equivalences. This suite of algorithms was chosen based on the expectation that graphs fall in one of those two categories. If that is the case, a combined algorithm runs in sub-quadratic worst case time. Whether this expectation is reasonable remains an interesting open problem.},
  doi            = {10.4204/eptcs.116.3},
  file           = {:2013_Oury_EPTCS.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  publisher      = {Open Publishing Association},
  qualityassured = {qualityAssured},
}

@Article{2014_Wang_Article,
  author         = {Wang, Zhe and Xue, Xiangyang},
  journal        = {Support vector machines applications},
  title          = {Multi-class support vector machine},
  year           = {2014},
  pages          = {23--48},
  abstract       = {Support vector machine (SVM) was initially designed for binary
classification. To extend SVM to the multi-class scenario, a number of classification
models were proposed such as the one by Crammer and Singer (J Mach Learn Res
2:265–292, 2001). However, the number of variables in Crammer and Singer’s dual
problem is the product of the number of samples (l) by the number of classes
(k), which produces a large computational complexity. This chapter sorts the
existing classical techniques for multi-class SVM into the indirect and direct ones
and further gives the comparison for them in terms of theory and experiments.
Especially, this chapter exhibits a new Simplified Multi-class SVM (SimMSVM)
that reduces the size of the resulting dual problem from l × k to l by introducing
a relaxed classification error bound. The experimental discussion demonstrates
that the SimMSVM approach can greatly speed up the training process, while
maintaining a competitive classification accuracy.},
  doi            = {10.1007/978-3-319-02300-7__2,},
  file           = {:2014_Wang_BOOK.pdf:PDF},
  groups         = {Support Vector Machines},
  publisher      = {Springer},
  qualityassured = {qualityAssured},
}

@Article{2015_Duvenaud_Article,
  author         = {Duvenaud, David K. and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'a}n and Adams, Ryan P.},
  journal        = {Advances in neural information processing systems},
  title          = {Convolutional networks on graphs for learning molecular fingerprints},
  year           = {2015},
  volume         = {28},
  file           = {:2015_Duvenaud_NIPS.pdf:PDF},
  groups         = {Graph Neural Networks, Convolutional networks, ConvolutionNetworks, GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html},
}

@InProceedings{2015_Feragen_InProceedings,
  author         = {Aasa Feragen and Francois Lauze and Soren Hauberg},
  booktitle      = {2015 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  title          = {Geodesic exponential kernels: When curvature and linearity conflict},
  year           = {2015},
  month          = {jun},
  publisher      = {{IEEE}},
  abstract       = {2015 IEEE Conference on Computer Vision and Pattern Recognition},
  doi            = {10.1109/cvpr.2015.7298922},
  file           = {:2015_Feragen_IEEE.pdf:PDF},
  qualityassured = {qualityAssured},
}

@Article{2015_Xu_Article,
  author         = {Xu, Hangjun},
  journal        = {arXiv preprint arXiv:1508.03381},
  title          = {An algorithm for comparing similarity between two trees},
  year           = {2015},
  eprint         = {1508.03381},
  eprinttype     = {arxiv},
  file           = {:2015_Xu_THESIS.pdf:PDF},
  groups         = {AppliedGraphtheory, Graphtheory},
  qualityassured = {qualityAssured},
}

@InProceedings{2016_Battaglia_InProceedings,
  author         = {Battaglia, Peter and Pascanu, Razvan and Lai, Matthew and Jimenez Rezende, Danilo and kavukcuoglu, koray},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{I}nteraction {N}etworks for {L}earning about {O}bjects, {R}elations and {P}hysics},
  year           = {2016},
  editor         = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {29},
  abstract       = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
  eprint         = {1612.00222},
  file           = {:2016_Battaglia_NIPS.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2016/file/3147da8ab4a0437c15ef51a5cc7f2dc4-Paper.pdf},
}

@InProceedings{2016_Defferrard_InProceedings,
  author         = {Defferrard, Micha\"{e}l and Bresson, Xavier and Vandergheynst, Pierre},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{C}onvolutional {N}eural {N}etworks on {G}raphs with {F}ast {L}ocalized {S}pectral {F}iltering},
  year           = {2016},
  editor         = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {29},
  abstract       = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words’ embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
  eprint         = {1606.09375},
  file           = {:2016_Defferrard_NIPS.pdf:PDF},
  groups         = {ConvolutionNetworks, GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf},
}

@Article{2016_Kriege_Article,
  author         = {Kriege, Nils M. and Giscard, Pierre-Louis and Wilson, Richard},
  journal        = {Advances in neural information processing systems},
  title          = {On valid optimal assignment kernels and applications to graph classification},
  year           = {2016},
  volume         = {29},
  file           = {:2016_Kriege_NIPS.pdf:PDF},
  groups         = {GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper/2016/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html},
}

@Article{2017_Bresson_Article,
  author         = {Bresson, Xavier and Laurent, Thomas},
  journal        = {arXiv preprint arXiv:1711.07553},
  title          = {Residual gated graph convnets},
  year           = {2017},
  eprint         = {1711.07553},
  eprinttype     = {arxiv},
  file           = {:2017_Bresson_CONF.pdf:PDF},
  groups         = {Graphtheory},
  qualityassured = {qualityAssured},
}

@InProceedings{2017_Gilmer_InProceedings,
  author         = {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle      = {Proceedings of the 34th International Conference on Machine Learning},
  title          = {{N}eural {M}essage {P}assing for {Q}uantum {C}hemistry},
  year           = {2017},
  editor         = {Precup, Doina and Teh, Yee Whye},
  month          = {06--11 Aug},
  pages          = {1263--1272},
  publisher      = {PMLR},
  series         = {Proceedings of Machine Learning Research},
  volume         = {70},
  abstract       = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  file           = {:2017_Gilmer_InProceedings.pdf:PDF;gilmer17a.pdf:http\:/proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf:PDF},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.mlr.press/v70/gilmer17a.html},
}

@Article{2017_Griffa_Article,
  author         = {Alessandra Griffa and Benjamin Ricaud and Kirell Benzi and Xavier Bresson and Alessandro Daducci and Pierre Vandergheynst and Jean-Philippe Thiran and Patric Hagmann},
  journal        = {NeuroImage},
  title          = {Transient networks of spatio-temporal connectivity map communication pathways in brain functional systems},
  year           = {2017},
  issn           = {1053-8119},
  pages          = {490-502},
  volume         = {155},
  abstract       = {The study of brain dynamics enables us to characterize the time-varying functional connectivity among distinct neural groups. However, current methods suffer from the absence of structural connectivity information. We propose to integrate infra-slow neural oscillations and anatomical-connectivity maps, as derived from functional and diffusion MRI, in a multilayer-graph framework that captures transient networks of spatio-temporal connectivity. These networks group anatomically wired and temporary synchronized brain regions and encode the propagation of functional activity on the structural connectome. In a group of 71 healthy subjects, we find that these transient networks demonstrate power-law spatial and temporal size, globally organize into well-known functional systems and describe wave-like trajectories of activation across anatomically connected regions. Within the transient networks, activity propagates through polysynaptic paths that include selective ensembles of structural connections and differ from the structural shortest paths. In the light of the communication-through-coherence principle, the identified spatio-temporal networks could encode communication channels' selection and neural assemblies, which deserves further attention. This work contributes to the understanding of brain structure-function relationships by considering the time-varying nature of resting-state interactions on the axonal scaffold, and it offers a convenient framework to study large-scale communication mechanisms and functional dynamics.},
  doi            = {10.1016/j.neuroimage.2017.04.015},
  file           = {:2017_Griffa_Article.pdf:PDF},
  keywords       = {Resting-state fMRI, Diffusion MRI, Brain connectivity, Multilayer network, Temporal network, Brain dynamics, Point-process, Communication-through-coherence, Spatio-temporal connectome},
  qualityassured = {qualityAssured},
  url            = {https://www.sciencedirect.com/science/article/pii/S105381191730304X},
}

@InProceedings{2017_Hamilton_InProceedings,
  author         = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{I}nductive {R}epresentation {L}earning on {L}arge {G}raphs},
  year           = {2017},
  editor         = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher      = {Curran Associates, Inc.},
  volume         = {30},
  abstract       = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  eprint         = {1706.02216},
  file           = {:2017_Hamilton_NIPS.pdf:PDF},
  groups         = {GraphKernels},
  qualityassured = {qualityAssured},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
}

@Article{2017_Marcheggiani_Article,
  author         = {Marcheggiani, Diego and Titov, Ivan},
  journal        = {arXiv preprint arXiv:1703.04826},
  title          = {Encoding sentences with graph convolutional networks for semantic role labeling},
  year           = {2017},
  doi            = {10.18653/v1/d17-1159},
  eprint         = {1703.04826},
  eprinttype     = {arxiv},
  file           = {:2017_Marcheggiani_CONF.pdf:PDF},
  qualityassured = {qualityAssured},
}

@Article{2018_Krakovna_Article,
  author         = {Krakovna, Viktoriya and Orseau, Laurent and Martic, Miljan and Legg, Shane},
  title          = {Measuring and avoiding side effects using relative reachability},
  year           = {2018},
  month          = {06},
  abstract       = {How can we design reinforcement learning agents that avoid causing unnecessary disruptions to their environment? We argue that current approaches to penalizing side effects can introduce bad incentives in tasks that require irreversible actions, and in environments that contain sources of change other than the agent. For example, some approaches give the agent an incentive to prevent any irreversible changes in the environment, including the actions of other agents. We introduce a general definition of side effects, based on relative reachability of states compared to a default state, that avoids these undesirable incentives. Using a set of gridworld experiments illustrating relevant scenarios, we empirically compare relative reachability to penalties based on existing definitions and show that it is the only penalty among those tested that produces the desired behavior in all the scenarios.},
  comment        = {RL Project Bachelor Thesis - Alex Roucka

See also "Penalizing side effects using stepwise relative reachability" by Krakovna et al.},
  file           = {:2018_Krakovna_Article.pdf:PDF},
  groups         = {Alex Roucka},
  qualityassured = {qualityAssured},
  readstatus     = {read},
  url            = {https://www.researchgate.net/publication/325557348_Measuring_and_avoiding_side_effects_using_relative_reachability},
}

@InProceedings{2020_Turner_InProceedings,
  author         = {Turner, Alex and Ratzlaff, Neale and Tadepalli, Prasad},
  booktitle      = {Advances in Neural Information Processing Systems},
  title          = {{A}voiding {S}ide {E}ffects in {C}omplex {E}nvironments},
  year           = {2020},
  editor         = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages          = {21406--21415},
  publisher      = {Curran Associates, Inc.},
  volume         = {33},
  abstract       = {Reward function specification can be difficult. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoided side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway's Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead while leading the agent to complete the specified task and avoid many side effects. Videos and code are available at https://avoiding-side-effects.github.io/.},
  comment        = {RL Project Bachelor Thesis - Alex Roucka},
  file           = {:2020_Turner_InProceedings.pdf:PDF},
  groups         = {Alex Roucka},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
  url            = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f50a6c02a3fc5a3a5d4d9391f05f3efc-Paper.pdf},
}

@Misc{2018_Krakovna_Misc,
  author         = {Krakovna, Victoria and Orseau, Laurent and Kumar, Ramana and Martic, Miljan and Legg, Shane},
  title          = {Penalizing side effects using stepwise relative reachability},
  year           = {2018},
  abstract       = {How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.},
  comment        = {RL Project Bachelor Thesis - Alex Roucka

Goal: RL agents that avoid causing unnecessary disruptions to their environment.

Idea: Use relative reachabiliyt from a baseline state as measure of disruption to the environment.

Gridworld Experiments},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.1806.01186},
  file           = {:2018_Krakovna_Misc.pdf:PDF},
  groups         = {Alex Roucka},
  keywords       = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
  readstatus     = {read},
  url            = {https://arxiv.org/abs/1806.01186},
}

@InProceedings{2020_Turner_InProceedingsa,
  author     = {Alexander Matt Turner and Dylan Hadfield-Menell and Prasad Tadepalli},
  booktitle  = {Proceedings of the {AAAI}/{ACM} Conference on {AI}, Ethics, and Society},
  title      = {{C}onservative {A}gency via {A}ttainable {U}tility {P}reservation},
  year       = {2020},
  month      = {feb},
  publisher  = {{ACM}},
  abstract   = {Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.},
  comment    = {RL Project Bachelor Thesis - Alex Roucka

Implementations in aup.py and model_free_agent_py in "Attainable Utility Preservation" (https://github.com/alexander-turner/attainable-utility-preservation).



Improvements over this approaches in "Penalizing side effects using stepwise relative reachability" by Krakovna et al.},
  doi        = {10.1145/3375627.3375851},
  file       = {:2020_Turner_InProceedingsa.pdf:PDF},
  groups     = {Alex Roucka},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/abs/10.1145/3375627.3375851?casa_token=TOj2-yjPZYEAAAAA:4BKWRa1IBaYiDlTDq_ykSQ48wH0sMMHfzWd_3nN4EF7fqKF9iS7XYAXVkJV_WIpWtoC9wpRiFy4lHw},
}

@Misc{2023_Waeldchen_Misc,
  author         = {Wäldchen, Stephan},
  title          = {{H}ardness of {D}eceptive {C}ertificate {S}election},
  year           = {2023},
  abstract       = {Recent progress towards theoretical interpretability guarantees for AI has been made with classifiers that are based on interactive proof systems. A prover selects a certificate from the datapoint and sends it to a verifier who decides the class. In the context of machine learning, such a certificate can be a feature that is informative of the class. For a setup with high soundness and completeness, the exchanged certificates must have a high mutual information with the true class of the datapoint. However, this guarantee relies on a bound on the Asymmetric Feature Correlation of the dataset, a property that so far is difficult to estimate for high-dimensional data. It was conjectured in Wäldchen et al. that it is computationally hard to exploit the AFC, which is what we prove here.
We consider a malicious prover-verifier duo that aims to exploit the AFC to achieve high completeness and soundness while using uninformative certificates. We show that this task is 𝖭𝖯-hard and cannot be approximated better than $\mathcal{O}(m^{1/8}−ϵ)$, where m is the number of possible certificates, for ϵ>0 under the Dense-vs-Random conjecture. This is some evidence that AFC should not prevent the use of interactive classification for real-world tasks, as it is computationally hard to be exploited.},
  copyright      = {Creative Commons Attribution Share Alike 4.0 International},
  doi            = {10.48550/ARXIV.2306.04505},
  file           = {:2023_Waeldchen_Misc.pdf:PDF},
  groups         = {TU-Berlin-Wäldchen},
  keywords       = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computational Complexity (cs.CC), Cryptography and Security (cs.CR), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.0, 68T01, 91A06},
  priority       = {prio1},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
  url            = {https://arxiv.org/abs/2306.04505},
}

@Misc{2023_Waeldchen_Misca,
  author         = {Stephan Waeldchen and Kartikey Sharma and Max Zimmer and Berkant Turan and Sebastian Pokutta},
  title          = {{F}ormal {I}nterpretability with {M}erlin-{A}rthur {C}lassifiers},
  year           = {2023},
  abstract       = {We propose a new type of multi-agent interactive classifier that provides, for the first time, provable interpretability guarantees even for complex agents such as neural networks.   In our setting, which is inspired by the Merlin-Arthur protocol from Interactive Proof Systems, two agents cooperate to provide a classification: the prover selects a small set of features as a certificate and presents it to the verifier who decides the class. A second, adversarial prover ensures the truthfulness of the system and allows us to connect the game-theoretic equilibrium between the provers and the verifier to guarantees on the exchanged features. We define completeness and soundness metrics to provide a lower bound on the mutual information between the features and the class. Our experiments demonstrate good agreement between theory and practice using neural network classifiers, and we show how our setup practically prevents manipulation.
Anonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.},
  file           = {:2023_Waeldchen_Misca.pdf:PDF},
  groups         = {TU-Berlin-Wäldchen},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
  url            = {https://openreview.net/forum?id=7hvbaJ1AbaM},
}

@Article{2023_Aigner_Article,
  author         = {Kevin-Martin Aigner and Andreas Bärmann and Kristin Braun and Frauke Liers and Sebastian Pokutta and Oskar Schneider and Kartikey Sharma and Sebastian Tschuppik},
  journal        = {{INFORMS} Journal on Optimization},
  title          = {{D}ata-{D}riven {D}istributionally {R}obust {O}ptimization over {T}ime},
  year           = {2023},
  month          = {may},
  doi            = {10.1287/ijoo.2023.0091},
  file           = {:2023_Aigner_Article.pdf:PDF},
  groups         = {TU-Berlin-Wäldchen},
  publisher      = {Institute for Operations Research and the Management Sciences ({INFORMS})},
  qualityassured = {qualityAssured},
}

@Article{2012_Chatterjee_Article,
  author         = {Krishnendu Chatterjee and Thomas A. Henzinger},
  journal        = {Journal of Computer and System Sciences},
  title          = {A survey of stochastic $\upomega$-regular games},
  year           = {2012},
  month          = {mar},
  number         = {2},
  pages          = {394--413},
  volume         = {78},
  doi            = {10.1016/j.jcss.2011.05.002},
  file           = {:2012_Chatterjee_Article.pdf:PDF},
  groups         = {TU-Berlin-Wäldchen},
  priority       = {prio1},
  publisher      = {Elsevier {BV}},
  qualityassured = {qualityAssured},
  url            = {https://www.sciencedirect.com/science/article/pii/S0022000011000511},
}

@Article{2003_Gent_Article,
  author         = {Gent, Ian P. and Rowley, A. G.},
  journal        = {2nd Intl. Work. Modelling and Reform. CSP},
  title          = {{E}ncoding {C}onnect-4 using quantified {B}oolean formulae},
  year           = {2003},
  pages          = {78--93},
  file           = {:2003_Gent_Article.pdf:PDF},
  groups         = {TU-Berlin-Wäldchen},
  priority       = {prio1},
  qualityassured = {qualityAssured},
  url            = {https://d1wqtxts1xzle7.cloudfront.net/30670545/10.1.1.103.1700-libre.pdf?1391790683=&response-content-disposition=inline;+filename=Encoding_Connect_4_using_quantified_Bool.pdf&Expires=1687865352&Signature=cLkaH0TuZy4hJ02YQesgL6TtfZjBh0eEPlWSI4GhSkEQmFNR~BxTIy8H26~FSzIWcMjFzsJlkcYRi2LdG5-GI7B404QX8DVwDzvJuecwLB3k3trryWT6SWyYzCicwrjTbSq~hdI4~KgZhqSaFiDeRhkz-yA7XHqBMqFDCbKAVYnjFbMFWmRWwdFBSeE58nmGehPnzcemL3DXSVVChdsZ9l82cw13VQ8qwzMWaXDV6uqjntwnENwYnZLKpmMjla57Pup1ftpARxzdBSnlUSc8VI7lilmETQJSljBVj--0MepubPTImdxYsRupbCvjU4Vwq2MqiAU1q~dhrI4O~uuUvg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA#page=84},
}

@Article{0_Mares_Article,
  author         = {B. J. Mares},
  title          = {{A} detailed proof that {IP}={PSPACE}},
  file           = {:0_Mares_Article.pdf:PDF},
  groups         = {TU-Berlin-Wäldchen},
  priority       = {prio1},
  qualityassured = {qualityAssured},
  url            = {https://cs.brown.edu/courses/gs019/},
}

@Article{2018_Hamilton_Article,
  author         = {Liberty S. Hamilton and Alexander G. Huth},
  journal        = {Language, Cognition and Neuroscience},
  title          = {The revolution will not be controlled: natural stimuli in speech neuroscience},
  year           = {2018},
  month          = {jul},
  number         = {5},
  pages          = {573--582},
  volume         = {35},
  doi            = {10.1080/23273798.2018.1499946},
  file           = {:2018_Hamilton_Article.pdf:PDF},
  groups         = {Neuroscience&ML},
  publisher      = {Informa {UK} Limited},
  qualityassured = {qualityAssured},
}

@Article{2016_Huth_Article,
  author         = {Alexander G. Huth and Wendy A. de Heer and Thomas L. Griffiths and Fr{\'{e}}d{\'{e}}ric E. Theunissen and Jack L. Gallant},
  journal        = {Nature},
  title          = {Natural speech reveals the semantic maps that tile human cerebral cortex},
  year           = {2016},
  month          = {apr},
  number         = {7600},
  pages          = {453--458},
  volume         = {532},
  doi            = {10.1038/nature17637},
  file           = {:2016_Huth_Article.pdf:PDF},
  groups         = {Neuroscience&ML},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@InProceedings{2018_Fridman_InProceedings,
  author         = {Lex Fridman and Bryan Reimer and Bruce Mehler and William T. Freeman},
  booktitle      = {Proceedings of the 2018 {CHI} Conference on Human Factors in Computing Systems},
  title          = {{C}ognitive {L}oad {E}stimation in the {W}ild},
  year           = {2018},
  month          = {apr},
  publisher      = {{ACM}},
  doi            = {10.1145/3173574.3174226},
  file           = {:2018_Fridman_InProceedings.pdf:PDF},
  groups         = {LexFridman},
  qualityassured = {qualityAssured},
}

@InProceedings{2017_Fridman_InProceedings,
  author         = {Lex Fridman and Heishiro Toyoda and Sean Seaman and Bobbie Seppelt and Linda Angell and Joonbum Lee and Bruce Mehler and Bryan Reimer},
  booktitle      = {Proceedings of the 2017 {CHI} Conference on Human Factors in Computing Systems},
  title          = {{W}hat {C}an {B}e {P}redicted from {S}ix {S}econds of {D}river {G}lances?},
  year           = {2017},
  month          = {may},
  publisher      = {{ACM}},
  doi            = {10.1145/3025453.3025929},
  file           = {:2017_Fridman_InProceedings.pdf:PDF},
  groups         = {LexFridman},
  qualityassured = {qualityAssured},
}

@Misc{2018_Fridman_Misc,
  author         = {Fridman, Lex and Terwilliger, Jack and Jenik, Benedikt},
  title          = {{D}eep{T}raffic: {C}rowdsourced {H}yperparameter {T}uning of {D}eep {R}einforcement {L}earning {S}ystems for {M}ulti-{A}gent {D}ense {T}raffic {N}avigation},
  year           = {2018},
  abstract       = {We present a traffic simulation named DeepTraffic where the planning systems for a subset of the vehicles are handled by a neural network as part of a model-free, off-policy reinforcement learning process. The primary goal of DeepTraffic is to make the hands-on study of deep reinforcement learning accessible to thousands of students, educators, and researchers in order to inspire and fuel the exploration and evaluation of deep Q-learning network variants and hyperparameter configurations through large-scale, open competition. This paper investigates the crowd-sourced hyperparameter tuning of the policy network that resulted from the first iteration of the DeepTraffic competition where thousands of participants actively searched through the hyperparameter space.},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.1801.02805},
  file           = {:2018_Fridman_Misc.pdf:PDF},
  groups         = {LexFridman},
  keywords       = {Neural and Evolutionary Computing (cs.NE), Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Article{2018_Awad_Article,
  author         = {Edmond Awad and Sohan Dsouza and Richard Kim and Jonathan Schulz and Joseph Henrich and Azim Shariff and Jean-Fran{\c{c}}ois Bonnefon and Iyad Rahwan},
  journal        = {Nature},
  title          = {{T}he {M}oral {M}achine experiment},
  year           = {2018},
  month          = {oct},
  number         = {7729},
  pages          = {59--64},
  volume         = {563},
  abstract       = {With the rapid development of artificial intelligence have come concerns about how machines will make moral decisions, and the major challenge of quantifying societal expectations about the ethical principles that should guide machine behaviour. To address this challenge, we deployed the Moral Machine, an online experimental platform designed to explore the moral dilemmas faced by autonomous vehicles. This platform gathered 40 million decisions in ten languages from millions of people in 233 countries and territories. Here we describe the results of this experiment. First, we summarize global moral preferences. Second, we document individual variations in preferences, based on respondents’ demographics. Third, we report cross-cultural ethical variation, and uncover three major clusters of countries. Fourth, we show that these differences correlate with modern institutions and deep cultural traits. We discuss how these preferences can contribute to developing global, socially acceptable principles for machine ethics. All data used in this article are publicly available.},
  doi            = {10.1038/s41586-018-0637-6},
  file           = {:2018_Awad_Article.pdf:PDF},
  groups         = {MaxPlanck},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Article{2016_Bonnefon_Article,
  author         = {Jean-Fran{\c{c}}ois Bonnefon and Azim Shariff and Iyad Rahwan},
  journal        = {Science},
  title          = {The social dilemma of autonomous vehicles},
  year           = {2016},
  month          = {jun},
  number         = {6293},
  pages          = {1573--1576},
  volume         = {352},
  doi            = {10.1126/science.aaf2654},
  file           = {:2016_Bonnefon_Article.pdf:PDF},
  groups         = {MaxPlanck},
  publisher      = {American Association for the Advancement of Science ({AAAS})},
  qualityassured = {qualityAssured},
  url            = {https://www.science.org/doi/abs/10.1126/science.aaf2654},
}

@Misc{2022_Chen_Misc,
  author         = {Chen, Yubei and Bardes, Adrien and Li, Zengyi and LeCun, Yann},
  title          = {{B}ag of {I}mage {P}atch {E}mbedding {B}ehind the {S}uccess of {S}elf-{S}upervised {L}earning},
  year           = {2022},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.2206.08954},
  file           = {:2022_Chen_Misc.pdf:PDF},
  groups         = {YanLeCun, SelfSupervisedLearning},
  keywords       = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@InProceedings{2019_Xian_InProceedings,
  author         = {Yikun Xian and Zuohui Fu and S. Muthukrishnan and Gerard de Melo and Yongfeng Zhang},
  booktitle      = {Proceedings of the 42nd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
  title          = {{R}einforcement {K}nowledge {G}raph {R}easoning for {E}xplainable {R}ecommendation},
  year           = {2019},
  month          = {jul},
  publisher      = {{ACM}},
  abstract       = {Recent advances in personalized recommendation have sparked
great interest in the exploitation of rich structured information
provided by knowledge graphs. Unlike most existing approaches
that only focus on leveraging knowledge graphs for more accurate
recommendation, we perform explicit reasoning with knowledge
for decision making so that the recommendations are generated
and supported by an interpretable causal inference procedure. To
this end, we propose a method called Policy-Guided Path Reason-
ing (PGPR), which couples recommendation and interpretability
by providing actual paths in a knowledge graph. Our contribu-
tions include four aspects. We first highlight the significance of
incorporating knowledge graphs into recommendation to formally
define and interpret the reasoning process. Second, we propose a
reinforcement learning (RL) approach featuring an innovative soft
reward strategy, user-conditional action pruning and a multi-hop
scoring function. Third, we design a policy-guided graph search
algorithm to efficiently and effectively sample reasoning paths for
recommendation. Finally, we extensively evaluate our method on
several large-scale real-world benchmark datasets, obtaining favor-
able results compared with state-of-the-art methods.},
  doi            = {10.1145/3331184.3331203},
  file           = {:2019_Xian_InProceedings.pdf:PDF},
  groups         = {HPI},
  priority       = {prio1},
  qualityassured = {qualityAssured},
  url            = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WCQXaGkAAAAJ&citation_for_view=WCQXaGkAAAAJ:mvPsJ3kp5DgC},
}

@InProceedings{2011_Hoffart_InProceedings,
  author         = {Johannes Hoffart and Fabian M. Suchanek and Klaus Berberich and Edwin Lewis-Kelham and Gerard de Melo and Gerhard Weikum},
  booktitle      = {Proceedings of the 20th international conference companion on World wide web},
  title          = {{YAGO}2},
  year           = {2011},
  month          = {mar},
  publisher      = {{ACM}},
  abstract       = {We present YAGO2, an extension of the YAGO knowledge base with focus on temporal and spatial knowledge. It is automatically built from Wikipedia, GeoNames, and WordNet, and contains nearly 10 million entities and events, as well as 80 million facts representing general world knowledge. An enhanced data representation introduces time and location as first-class citizens. The wealth of spatio-temporal information in YAGO can be explored either graphically or through a special time- and space-aware query language.},
  doi            = {10.1145/1963192.1963296},
  file           = {:2011_Hoffart_InProceedings.pdf:PDF},
  groups         = {HPI},
  priority       = {prio1},
  qualityassured = {qualityAssured},
}

@Article{2021_Hogan_Article,
  author         = {Aidan Hogan and Eva Blomqvist and Michael Cochez and Claudia D'amato and Gerard De Melo and Claudio Gutierrez and Sabrina Kirrane and Jos{\'{e}} Emilio Labra Gayo and Roberto Navigli and Sebastian Neumaier and Axel-Cyrille Ngonga Ngomo and Axel Polleres and Sabbir M. Rashid and Anisa Rula and Lukas Schmelzeisen and Juan Sequeda and Steffen Staab and Antoine Zimmermann},
  journal        = {{ACM} Computing Surveys},
  title          = {{K}nowledge {G}raphs},
  year           = {2021},
  month          = {jul},
  number         = {4},
  pages          = {1--37},
  volume         = {54},
  doi            = {10.1145/3447772},
  file           = {:2021_Hogan_Article.pdf:PDF},
  groups         = {HPI},
  priority       = {prio1},
  publisher      = {Association for Computing Machinery ({ACM})},
  qualityassured = {qualityAssured},
}

@Article{2014_Cambria_Article,
  author         = {Erik Cambria and Bebo White},
  journal        = {{IEEE} Computational Intelligence Magazine},
  title          = {{J}umping {NLP} {C}urves: {A} {R}eview of {N}atural {L}anguage {P}rocessing {R}esearch [{R}eview {A}rticle]},
  year           = {2014},
  month          = {may},
  number         = {2},
  pages          = {48--57},
  volume         = {9},
  abstract       = {Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and representation of human language. NLP research has evolved from the era of punch cards and batch processing (in which the analysis of a sentence could take up to 7 minutes) to the era of Google and the likes of it (in which millions of webpages can be processed in less than a second). This review paper draws on recent developments in NLP research to look at the past, present, and future of NLP technology in a new light. Borrowing the paradigm of `jumping curves' from the field of business management and marketing prediction, this survey article reinterprets the evolution of NLP research as the intersection of three overlapping curves-namely Syntactics, Semantics, and Pragmatics Curveswhich will eventually lead NLP research to evolve into natural language understanding.},
  doi            = {10.1109/mci.2014.2307227},
  file           = {:2014_Cambria_Article.pdf:PDF},
  groups         = {HPI, Language Processing},
  priority       = {prio1},
  publisher      = {Institute of Electrical and Electronics Engineers ({IEEE})},
  qualityassured = {qualityAssured},
}

@Article{2020_Kang_Article,
  author         = {Yue Kang and Zhao Cai and Chee-Wee Tan and Qian Huang and Hefu Liu},
  journal        = {Journal of Management Analytics},
  title          = {Natural language processing ({NLP}) in management research: A literature review},
  year           = {2020},
  month          = {apr},
  number         = {2},
  pages          = {139--172},
  volume         = {7},
  doi            = {10.1080/23270012.2020.1756939},
  file           = {:2020_Kang_Article.pdf:PDF},
  groups         = {HPI, Language Processing},
  priority       = {prio1},
  publisher      = {Informa {UK} Limited},
  qualityassured = {qualityAssured},
}

@Misc{2023_Welke_Misc,
  author         = {Welke, Pascal and Thiessen, Maximilian and Jogl, Fabian and Gärtner, Thomas},
  title          = {{E}xpectation-{C}omplete {G}raph {R}epresentations with {H}omomorphisms},
  year           = {2023},
  abstract       = {We investigate novel random graph embeddings that can be computed in expected polynomial time and that are able to distinguish all non-isomorphic graphs in expectation. Previous graph embeddings have limited expressiveness and either cannot distinguish all graphs or cannot be computed efficiently for every graph. To be able to approximate arbitrary functions on graphs, we are interested in efficient alternatives that become arbitrarily expressive with increasing resources. Our approach is based on Lovász' characterisation of graph isomorphism through an infinite dimensional vector of homomorphism counts. Our empirical evaluation shows competitive results on several benchmark graph learning tasks.},
  copyright      = {Creative Commons Attribution 4.0 International},
  doi            = {10.48550/ARXIV.2306.05838},
  file           = {:2023_Welke_Misc.pdf:PDF},
  groups         = {Graphtheory},
  keywords       = {Machine Learning (cs.LG), Data Structures and Algorithms (cs.DS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio2},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Misc{2023_BenShaul_Misc,
  author         = {Ben-Shaul, Ido and Shwartz-Ziv, Ravid and Galanti, Tomer and Dekel, Shai and LeCun, Yann},
  title          = {{R}everse {E}ngineering {S}elf-{S}upervised {L}earning},
  year           = {2023},
  abstract       = {Self-supervised learning (SSL) is a powerful tool in machine learning, but under-
standing the learned representations and their underlying mechanisms remains
a challenge. This paper presents an in-depth empirical analysis of SSL-trained
representations, encompassing diverse models, architectures, and hyperparameters.
Our study reveals an intriguing aspect of the SSL training process: it inherently
facilitates the clustering of samples with respect to semantic labels, which is surpris-
ingly driven by the SSL objective’s regularization term. This clustering process not
only enhances downstream classification but also compresses the data information.
Furthermore, we establish that SSL-trained representations align more closely with
semantic classes rather than random classes. Remarkably, we show that learned rep-
resentations align with semantic classes across various hierarchical levels, and this
alignment increases during training and when moving deeper into the network. Our
findings provide valuable insights into SSL’s representation learning mechanisms
and their impact on performance across different sets of classes.},
  copyright      = {Creative Commons Attribution 4.0 International},
  doi            = {10.48550/ARXIV.2305.15614},
  file           = {:2023_BenShaul_Misc.pdf:PDF},
  groups         = {YanLeCun, SelfSupervisedLearning},
  keywords       = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio2},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Misc{2023_Anil_Misc,
  author         = {Anil, Rohan and Dai, Andrew M. and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and Chu, Eric and Clark, Jonathan H. and Shafey, Laurent El and Huang, Yanping and Meier-Hellstern, Kathy and Mishra, Gaurav and Moreira, Erica and Omernick, Mark and Robinson, Kevin and Ruder, Sebastian and Tay, Yi and Xiao, Kefan and Xu, Yuanzhong and Zhang, Yujing and Abrego, Gustavo Hernandez and Ahn, Junwhan and Austin, Jacob and Barham, Paul and Botha, Jan and Bradbury, James and Brahma, Siddhartha and Brooks, Kevin and Catasta, Michele and Cheng, Yong and Cherry, Colin and Choquette-Choo, Christopher A. and Chowdhery, Aakanksha and Crepy, Clément and Dave, Shachi and Dehghani, Mostafa and Dev, Sunipa and Devlin, Jacob and Díaz, Mark and Du, Nan and Dyer, Ethan and Feinberg, Vlad and Feng, Fangxiaoyu and Fienber, Vlad and Freitag, Markus and Garcia, Xavier and Gehrmann, Sebastian and Gonzalez, Lucas and Gur-Ari, Guy and Hand, Steven and Hashemi, Hadi and Hou, Le and Howland, Joshua and Hu, Andrea and Hui, Jeffrey and Hurwitz, Jeremy and Isard, Michael and Ittycheriah, Abe and Jagielski, Matthew and Jia, Wenhao and Kenealy, Kathleen and Krikun, Maxim and Kudugunta, Sneha and Lan, Chang and Lee, Katherine and Lee, Benjamin and Li, Eric and Li, Music and Li, Wei and Li, YaGuang and Li, Jian and Lim, Hyeontaek and Lin, Hanzhao and Liu, Zhongtao and Liu, Frederick and Maggioni, Marcello and Mahendru, Aroma and Maynez, Joshua and Misra, Vedant and Moussalem, Maysam and Nado, Zachary and Nham, John and Ni, Eric and Nystrom, Andrew and Parrish, Alicia and Pellat, Marie and Polacek, Martin and Polozov, Alex and Pope, Reiner and Qiao, Siyuan and Reif, Emily and Richter, Bryan and Riley, Parker and Ros, Alex Castro and Roy, Aurko and Saeta, Brennan and Samuel, Rajkumar and Shelby, Renee and Slone, Ambrose and Smilkov, Daniel and So, David R. and Sohn, Daniel and Tokumine, Simon and Valter, Dasha and Vasudevan, Vijay and Vodrahalli, Kiran and Wang, Xuezhi and Wang, Pidong and Wang, Zirui and Wang, Tao and Wieting, John and Wu, Yuhuai and Xu, Kelvin and Xu, Yunhan and Xue, Linting and Yin, Pengcheng and Yu, Jiahui and Zhang, Qiao and Zheng, Steven and Zheng, Ce and Zhou, Weikang and Zhou, Denny and Petrov, Slav and Wu, Yonghui},
  title          = {{PaLM} 2 {T}echnical {R}eport},
  year           = {2023},
  abstract       = {We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.
When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.},
  copyright      = {Creative Commons Attribution Share Alike 4.0 International},
  doi            = {10.48550/ARXIV.2305.10403},
  file           = {:2023_Anil_Misc.pdf:PDF},
  groups         = {Language Processing},
  keywords       = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio1},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@InProceedings{2023_Balestriero_InProceedings,
  author         = {Randall Balestriero and Yann LeCun},
  booktitle      = {{ICASSP} 2023 - 2023 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
  title          = {{F}ast and {E}xact {E}numeration of {D}eep {N}etworks {P}artitions {R}egions},
  year           = {2023},
  month          = {jun},
  publisher      = {{IEEE}},
  abstract       = {One fruitful formulation of Deep Networks (DNs) enabling their theoretical study and providing practical guidelines to practitioners relies on Piecewise Affine Splines. In that realm, a DN’s input-mapping is expressed as per-region affine map-ping where those regions are implicitly determined by the model’s architecture and form a partition of their input space. That partition –which is involved in all the results spanned from this line of research– has so far only been computed on 2/3-dimensional slices of the DN’s input space or estimated by random sampling. In this paper, we provide the first parallel algorithm that does exact enumeration of the DN’s partition regions. The proposed algorithm enables one to finally assess the closeness of the commonly employed approximations methods, e.g. based on random sampling of the DN input space. One of our key finding is that if one is only interested in regions with "large" volume, then uniform sampling of the space is highly efficient, but that if one is also interested in discovering the "small" regions of the partition, then uniform sampling is exponentially costly with the DN’s input space dimension. On the other hand, our proposed method has complexity scaling linearly with input dimension and the number of regions.},
  doi            = {10.1109/icassp49357.2023.10095698},
  file           = {:2023_Balestriero_InProceedings.pdf:PDF},
  groups         = {YanLeCun},
  qualityassured = {qualityAssured},
}

@Misc{2020_Kaplan_Misc,
  author         = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title          = {{S}caling {L}aws for {N}eural {L}anguage {M}odels},
  year           = {2020},
  abstract       = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.2001.08361},
  file           = {:2020_Kaplan_Misc.pdf:PDF},
  groups         = {Language Processing},
  keywords       = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio2},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Article{2017_Vaswani_Article,
  author         = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal        = {Advances in neural information processing systems},
  title          = {Attention is all you need},
  year           = {2017},
  volume         = {30},
  abstract       = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  file           = {:2017_Vaswani_Article.pdf:PDF},
  groups         = {MachineLearning},
  priority       = {prio1},
  qualityassured = {qualityAssured},
}

@Article{2023_Khader_Article,
  author         = {Firas Khader and Jakob Nikolas Kather and Gustav Müller-Franzes and Tianci Wang and Tianyu Han and Soroosh Tayebi Arasteh and Karim Hamesch and Keno Bressem and Christoph Haarburger and Johannes Stegmaier and Christiane Kuhl and Sven Nebelung and Daniel Truhn},
  journal        = {Scientific Reports},
  title          = {Medical transformer for multimodal survival prediction in intensive care: integration of imaging and non-imaging data},
  year           = {2023},
  month          = {jul},
  number         = {1},
  volume         = {13},
  comment        = {MeTra results and ideas.

Survival Prediction improves on a multimodal approach over unimodal approach using radiology images and clinical data.},
  doi            = {10.1038/s41598-023-37835-1},
  file           = {:2023_Khader_Article.pdf:PDF},
  groups         = {COMFORT},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
  readstatus     = {read},
}

@Article{2023_Busch_Article,
  author         = {Felix Busch and Lisa C. Adams and Keno K. Bressem},
  journal        = {Medical Science Educator},
  title          = {{B}iomedical {E}thical {A}spects {T}owards the {I}mplementation of {A}rtificial {I}ntelligence in {M}edical {E}ducation},
  year           = {2023},
  month          = {jun},
  doi            = {10.1007/s40670-023-01815-x},
  file           = {:2023_Busch_Article.pdf:PDF},
  groups         = {COMFORT},
  priority       = {prio1},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Misc{2023_Han_Misc,
  author         = {Han, Tianyu and Adams, Lisa C. and Papaioannou, Jens-Michalis and Grundmann, Paul and Oberhauser, Tom and Löser, Alexander and Truhn, Daniel and Bressem, Keno K.},
  title          = {MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data},
  year           = {2023},
  copyright      = {Creative Commons Attribution 4.0 International},
  doi            = {10.48550/ARXIV.2304.08247},
  file           = {:2023_Han_Misc.pdf:PDF},
  groups         = {COMFORT},
  keywords       = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio1},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Article{2023_Adams_Article,
  author         = {Lisa C. Adams and Felix Busch and Daniel Truhn and Marcus R. Makowski and Hugo J. W. L. Aerts and Keno K. Bressem},
  journal        = {Journal of Medical Internet Research},
  title          = {{W}hat {D}oes {DALL}-{E} 2 {K}now {A}bout {R}adiology?},
  year           = {2023},
  month          = {mar},
  pages          = {e43110},
  volume         = {25},
  doi            = {10.2196/43110},
  file           = {:2023_Adams_Article.pdf:PDF},
  groups         = {COMFORT},
  priority       = {prio1},
  publisher      = {{JMIR} Publications Inc.},
  qualityassured = {qualityAssured},
}

@Article{2022_Vinayahalingam_Article,
  author         = {Shankeeth Vinayahalingam and Niels van Nistelrooij and Bram van Ginneken and Keno Bressem and Daniel Tröltzsch and Max Heiland and Tabea Flügge and Robert Gaudin},
  journal        = {Scientific Reports},
  title          = {Detection of mandibular fractures on panoramic radiographs using deep learning},
  year           = {2022},
  month          = {nov},
  number         = {1},
  volume         = {12},
  doi            = {10.1038/s41598-022-23445-w},
  file           = {:2022_Vinayahalingam_Article.pdf:PDF},
  groups         = {COMFORT},
  priority       = {prio1},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Article{2021_Bressem_Article,
  author         = {Keno K. Bressem and Janis L. Vahldiek and Lisa Adams and Stefan Markus Niehues and Hildrun Haibel and Valeria Rios Rodriguez and Murat Torgutalp and Mikhail Protopopov and Fabian Proft and Judith Rademacher and Joachim Sieper and Martin Rudwaleit and Bernd Hamm and Marcus R. Makowski and Kay-Geert Hermann and Denis Poddubnyy},
  journal        = {Arthritis Research {\&}amp$\mathsemicolon$ Therapy},
  title          = {Deep learning for detection of radiographic sacroiliitis: achieving expert-level performance},
  year           = {2021},
  month          = {apr},
  number         = {1},
  volume         = {23},
  abstract       = {Background:
Radiographs of the sacroiliac joints are commonly used for the diagnosis and classification of axial spondyloarthritis. The aim of this study was to develop and validate an artificial neural network for the detection of definite radiographic sacroiliitis as a manifestation of axial spondyloarthritis (axSpA).

Methods:
Conventional radiographs of the sacroiliac joints obtained in two independent studies of patients with axSpA were used. The first cohort comprised 1553 radiographs and was split into training (n = 1324) and validation (n = 229) sets. The second cohort comprised 458 radiographs and was used as an independent test dataset. All radiographs were assessed in a central reading session, and the final decision on the presence or absence of definite radiographic sacroiliitis was used as a reference. The performance of the neural network was evaluated by calculating areas under the receiver operating characteristic curves (AUCs) as well as sensitivity and specificity. Cohen’s kappa and the absolute agreement were used to assess the agreement between the neural network and the human readers.

Results:
The neural network achieved an excellent performance in the detection of definite radiographic sacroiliitis with an AUC of 0.97 and 0.94 for the validation and test datasets, respectively. Sensitivity and specificity for the cut-off weighting both measurements equally were 88% and 95% for the validation and 92% and 81% for the test set. The Cohen’s kappa between the neural network and the reference judgements were 0.79 and 0.72 for the validation and test sets with an absolute agreement of 90% and 88%, respectively.

Conclusion:
Deep artificial neural networks enable the accurate detection of definite radiographic sacroiliitis relevant for the diagnosis and classification of axSpA.},
  doi            = {10.1186/s13075-021-02484-0},
  file           = {:2021_Bressem_Article.pdf:PDF},
  groups         = {COMFORT},
  priority       = {prio1},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Article{2021_Grauhan_Article,
  author         = {Nils F. Grauhan and Stefan M. Niehues and Robert A. Gaudin and Sarah Keller and Janis L. Vahldiek and Lisa C. Adams and Keno K. Bressem},
  journal        = {Skeletal Radiology},
  title          = {Deep learning for accurately recognizing common causes of shoulder pain on radiographs},
  year           = {2021},
  month          = {feb},
  number         = {2},
  pages          = {355--362},
  volume         = {51},
  abstract       = {Objective:
Training a convolutional neural network (CNN) to detect the most common causes of shoulder pain on plain radiographs and to assess its potential value in serving as an assistive device to physicians.

Materials and methods:
We used a CNN of the ResNet-50 architecture which was trained on 2700 shoulder radiographs from clinical practice of multiple institutions. All radiographs were reviewed and labeled for six findings: proximal humeral fractures, joint dislocation, periarticular calcification, osteoarthritis, osteosynthesis, and joint endoprosthesis. The trained model was then evaluated on a separate test dataset, which was previously annotated by three independent expert radiologists. Both the training and the test datasets included radiographs of highly variable image quality to reflect the clinical situation and to foster robustness of the CNN. Performance of the model was evaluated using receiver operating characteristic (ROC) curves, the thereof derived AUC as well as sensitivity and specificity.

Results:
The developed CNN demonstrated a high accuracy with an area under the curve (AUC) of 0.871 for detecting fractures, 0.896 for joint dislocation, 0.945 for osteoarthritis, and 0.800 for periarticular calcifications. It also detected osteosynthesis and endoprosthesis with near perfect accuracy (AUC 0.998 and 1.0, respectively). Sensitivity and specificity were 0.75 and 0.86 for fractures, 0.95 and 0.65 for joint dislocation, 0.90 and 0.86 for osteoarthrosis, and 0.60 and 0.89 for calcification.

Conclusion:
CNNs have the potential to serve as an assistive device by providing clinicians a means to prioritize worklists or providing additional safety in situations of increased workload.},
  doi            = {10.1007/s00256-021-03740-9},
  file           = {:2021_Grauhan_Article.pdf:PDF},
  groups         = {COMFORT},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Article{2021_Erxleben_Article,
  author         = {Christoph Erxleben and Lisa C. Adams and Jacob Albrecht and Antonia Petersen and Janis L. Vahldiek and Hans-Martin Thie{\ss} and Julia Kremmin and Marcus R. Makowski and Alexandra Niehues and Stefan M. Niehues and Keno K. Bressem},
  journal        = {Clinical Imaging},
  title          = {Improving {CT} accuracy in the diagnosis of {COVID}-19 in a hospital setting},
  year           = {2021},
  month          = {aug},
  pages          = {1--5},
  volume         = {76},
  abstract       = {Objective
This study aimed to improve the accuracy of CT for detection of COVID-19-associated pneumonia and to identify patient subgroups who might benefit most from CT imaging.

Methods
A total of 269 patients who underwent CT for suspected COVID-19 were included in this retrospective analysis. COVID-19 was confirmed by reverse-transcription-polymerase-chain-reaction. Basic demographics (age and sex) and initial vital parameters (O2-saturation, respiratory rate, and body temperature) were recorded. Generalized mixed models were used to calculate the accuracy of vital parameters for detection of COVID-19 and to evaluate the diagnostic accuracy of CT. A clinical score based on vital parameters, age, and sex was established to estimate the pretest probability of COVID-19 and used to define low, intermediate, and high risk groups. A p-value of <0.05 was considered statistically significant.

Results
The sole use of vital parameters for the prediction of COVID-19 was inferior to CT. After correction for confounders, such as age and sex, CT showed a sensitivity of 0.86, specificity of 0.78, and positive predictive value of 0.36. In the subgroup analysis based on pretest probability, positive predictive value and sensitivity increased to 0.53 and 0.89 in the high-risk group, while specificity was reduced to 0.68. In the low-risk group, sensitivity and positive predictive value decreased to 0.76 and 0.33 with a specificity of 0.83. The negative predictive value remained high (0.94 and 0.97) in both groups.

Conclusions
The accuracy of CT for the detection of COVID-19 might be increased by selecting patients with a high-pretest probability of COVID-19.},
  doi            = {10.1016/j.clinimag.2021.01.026},
  file           = {:2021_Erxleben_Article.pdf:PDF},
  groups         = {COMFORT},
  publisher      = {Elsevier {BV}},
  qualityassured = {qualityAssured},
}

@Article{2020_Bressem_ArticleH,
  author         = {Keno K. Bressem and Lisa C. Adams and Robert A. Gaudin and Daniel Tröltzsch and Bernd Hamm and Marcus R. Makowski and Chan-Yong Schüle and Janis L. Vahldiek and Stefan M. Niehues},
  journal        = {Bioinformatics},
  title          = {Highly accurate classification of chest radiographic reports using a deep learning natural language model pre-trained on 3.8 million text reports},
  year           = {2020},
  month          = {jul},
  number         = {21},
  pages          = {5255--5261},
  volume         = {36},
  abstract       = {Motivation
The development of deep, bidirectional transformers such as Bidirectional Encoder Representations from Transformers (BERT) led to an outperformance of several Natural Language Processing (NLP) benchmarks. Especially in radiology, large amounts of free-text data are generated in daily clinical workflow. These report texts could be of particular use for the generation of labels in machine learning, especially for image classification. However, as report texts are mostly unstructured, advanced NLP methods are needed to enable accurate text classification. While neural networks can be used for this purpose, they must first be trained on large amounts of manually labelled data to achieve good results. In contrast, BERT models can be pre-trained on unlabelled data and then only require fine tuning on a small amount of manually labelled data to achieve even better results.

Results
Using BERT to identify the most important findings in intensive care chest radiograph reports, we achieve areas under the receiver operation characteristics curve of 0.98 for congestion, 0.97 for effusion, 0.97 for consolidation and 0.99 for pneumothorax, surpassing the accuracy of previous approaches with comparatively little annotation effort. Our approach could therefore help to improve information extraction from free-text medical reports.

Availability  and implementation
We make the source code for fine-tuning the BERT-models freely available at https://github.com/fast-raidiology/bert-for-radiology.},
  doi            = {10.1093/bioinformatics/btaa668},
  editor         = {Jonathan Wren},
  file           = {:2020_Bressem_ArticleH.pdf:PDF},
  groups         = {COMFORT},
  publisher      = {Oxford University Press ({OUP})},
  qualityassured = {qualityAssured},
}

@Article{2020_Bressem_ArticleC,
  author         = {Keno K. Bressem and Lisa C. Adams and Christoph Erxleben and Bernd Hamm and Stefan M. Niehues and Janis L. Vahldiek},
  journal        = {Scientific Reports},
  title          = {Comparing different deep learning architectures for classification of chest radiographs},
  year           = {2020},
  month          = {aug},
  number         = {1},
  volume         = {10},
  abstract       = {Chest radiographs are among the most frequently acquired images in radiology and are often the subject of computer vision research. However, most of the models used to classify chest radiographs are derived from openly available deep neural networks, trained on large image datasets. These datasets differ from chest radiographs in that they are mostly color images and have substantially more labels. Therefore, very deep convolutional neural networks (CNN) designed for ImageNet and often representing more complex relationships, might not be required for the comparably simpler task of classifying medical image data. Sixteen different architectures of CNN were compared regarding the classification performance on two openly available datasets, the CheXpert and COVID-19 Image Data Collection. Areas under the receiver operating characteristics curves (AUROC) between 0.83 and 0.89 could be achieved on the CheXpert dataset. On the COVID-19 Image Data Collection, all models showed an excellent ability to detect COVID-19 and non-COVID pneumonia with AUROC values between 0.983 and 0.998. It could be observed, that more shallow networks may achieve results comparable to their deeper and more complex counterparts with shorter training times, enabling classification performances on medical image data close to the state-of-the-art methods even when using limited hardware.},
  doi            = {10.1038/s41598-020-70479-z},
  file           = {:2020_Bressem_ArticleC.pdf:PDF},
  groups         = {COMFORT},
  publisher      = {Springer Science and Business Media {LLC}},
  qualityassured = {qualityAssured},
}

@Misc{2016_Jegou_Misc,
  author         = {Jégou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
  title          = {{T}he {O}ne {H}undred {L}ayers {T}iramisu: Fully {C}onvolutional {D}ense{N}ets for {S}emantic {S}egmentation},
  year           = {2016},
  abstract       = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions.
Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train.
In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets.},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.1611.09326},
  file           = {:2016_Jegou_Misc.pdf:PDF},
  groups         = {COMFORT},
  keywords       = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio1},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Misc{2020_Tancik_Misc,
  author         = {Tancik, Matthew and Srinivasan, Pratul P. and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T. and Ng, Ren},
  title          = {{F}ourier {F}eatures {L}et {N}etworks {L}earn {H}igh {F}requency {F}unctions in {L}ow {D}imensional {D}omains},
  year           = {2020},
  abstract       = {We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.2006.10739},
  file           = {:2020_Tancik_Misc.pdf:PDF},
  groups         = {MachineLearning},
  keywords       = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio1},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@InProceedings{2009_Radovanovic_InProceedings,
  author         = {Milo{\v{s}} Radovanovi{\'{c}} and Alexandros Nanopoulos and Mirjana Ivanovi{\'{c}}},
  booktitle      = {Proceedings of the 26th Annual International Conference on Machine Learning},
  title          = {Nearest neighbors in high-dimensional data},
  year           = {2009},
  month          = {jun},
  publisher      = {{ACM}},
  abstract       = {High dimensionality can pose severe difficulties, widely recognized as different aspects of the curse of dimensionality. In this paper we study a new aspect of the curse pertaining to the distribution of k-occurrences, i.e., the number of times a point appears among the k nearest neighbors of other points in a data set. We show that, as dimensionality increases, this distribution becomes considerably skewed and hub points emerge (points with very high k-occurrences). We examine the origin of this phenomenon, showing that it is an inherent property of high-dimensional vector space, and explore its influence on applications based on measuring distances in vector spaces, notably classification, clustering, and information retrieval.},
  doi            = {10.1145/1553374.1553485},
  file           = {:2009_Radovanovic_InProceedings.pdf:PDF},
  groups         = {ClusteringAlgorithms},
  qualityassured = {qualityAssured},
  readstatus     = {skimmed},
}

@Article{2023_Haehnel_Article,
  author         = {Tom Haehnel and Tamara Raschka and Stefano Sapienza and Jochen Klucken and Enrico Glaab and Jean-Christophe Corvol and Bjoern Falkenburger and Holger Froehlich},
  title          = {{P}rogression {S}ubtypes in {P}arkinson{\textquotesingle}s {D}isease: {A} {D}ata-driven {M}ulti-{C}ohort {A}nalysis},
  year           = {2023},
  month          = {oct},
  abstract       = {Background: The progression of Parkinson's disease (PD) is heterogeneous across patients. This heterogeneity complicates patients counseling and inflates the number of patients needed to test potential neuroprotective treatments. Moreover, disease subtypes might require different therapies. This work uses a data-driven approach to investigate how observed heterogeneity in PD can be explained by the existence of distinct PD progression subtypes. Methods: To derive stable PD progression subtypes in an unbiased manner, we analyzed multimodal longitudinal data from three large PD cohorts. A latent time joint mixed-effects model (LTJMM) was used to align patients on a common disease timescale. Progression subtypes were identified by variational deep embedding with recurrence (VaDER). These subtypes were then characterized across the three cohorts using clinical scores, DaTSCAN imaging and digital gait biomarkers. To assign patients to progression subtypes from baseline data, we developed predictive models and performed extensive cross-cohort validation. Results: In each cohort, we identified a fast-progressing and a slow-progressing subtype. These subtypes were reflected by different patterns of motor and non-motor symptoms progression, survival rates, treatment response and features extracted from DaTSCAN imaging and digital gait assessments. Predictive models achieved robust performance with ROC-AUC up to 0.79 for subtype identification. Simulations demonstrated that enriching clinical trials with fast-progressing patients based on predictions from baseline can reduce the required cohort size by 43%. Conclusion: Our results show that heterogeneity in PD can be explained by two distinct subtypes of PD progression that are stable across cohorts and can be predicted from baseline data. These subtypes align with the brain-first vs. body-first concept, which potentially provides a biological explanation for subtype differences. The predictive models will enable clinical trials with significantly lower sample sizes by enriching fast-progressing patients.},
  doi            = {10.1101/2023.10.12.23296943},
  file           = {:2023_Haehnel_Article.pdf:PDF},
  priority       = {prio2},
  publisher      = {Cold Spring Harbor Laboratory},
  qualityassured = {qualityAssured},
}

@Article{2017_Qiu_Article,
  author         = {Jianfeng Qiu and H. Harold Li and Tiezhi Zhang and Fangfang Ma and Deshan Yang},
  journal        = {Journal of Applied Clinical Medical Physics},
  title          = {Automatic x-ray image contrast enhancement based on parameter auto-optimization},
  year           = {2017},
  month          = {sep},
  number         = {6},
  pages          = {218--223},
  volume         = {18},
  abstract       = {Purpose
Insufficient image contrast associated with radiation therapy daily setup x-ray images could negatively affect accurate patient treatment setup. We developed a method to perform automatic and user-independent contrast enhancement on 2D kilo voltage (kV) and megavoltage (MV) x-ray images. The goal was to provide tissue contrast optimized for each treatment site in order to support accurate patient daily treatment setup and the subsequent offline review.

Methods
The proposed method processes the 2D x-ray images with an optimized image processing filter chain, which consists of a noise reduction filter and a high-pass filter followed by a contrast limited adaptive histogram equalization (CLAHE) filter. The most important innovation is to optimize the image processing parameters automatically to determine the required image contrast settings per disease site and imaging modality. Three major parameters controlling the image processing chain, i.e., the Gaussian smoothing weighting factor for the high-pass filter, the block size, and the clip limiting parameter for the CLAHE filter, were determined automatically using an interior-point constrained optimization algorithm.

Results
Fifty-two kV and MV x-ray images were included in this study. The results were manually evaluated and ranked with scores from 1 (worst, unacceptable) to 5 (significantly better than adequate and visually praise worthy) by physicians and physicists. The average scores for the images processed by the proposed method, the CLAHE, and the best window-level adjustment were 3.92, 2.83, and 2.27, respectively. The percentage of the processed images received a score of 5 were 48, 29, and 18%, respectively.

Conclusion
The proposed method is able to outperform the standard image contrast adjustment procedures that are currently used in the commercial clinical systems. When the proposed method is implemented in the clinical systems as an automatic image processing filter, it could be useful for allowing quicker and potentially more accurate treatment setup and facilitating the subsequent offline review and verification.},
  doi            = {10.1002/acm2.12172},
  file           = {:2017_Qiu_Article.pdf:PDF},
  groups         = {Medical},
  publisher      = {Wiley},
  qualityassured = {qualityAssured},
  readstatus     = {read},
}

@InProceedings{2016_He_InProceedings,
  author          = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  booktitle       = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  title           = {{D}eep {R}esidual {L}earning for {I}mage {R}ecognition},
  year            = {2016},
  month           = {jun},
  publisher       = {{IEEE}},
  abstract        = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  comment-fabrice = {The ResNet paper},
  doi             = {10.1109/cvpr.2016.90},
  file            = {:2016_He_InProceedings.pdf:PDF},
  groups          = {ConvolutionNetworks},
  priority        = {prio1},
  qualityassured  = {qualityAssured},
}

@Misc{2023_Arkoudas_Misc,
  author         = {Arkoudas, Konstantine},
  title          = {{GPT}-4 {C}an't {R}eason},
  year           = {2023},
  abstract       = {GPT-4 was released in March 2023 to wide acclaim, marking a very substantial improvement across the board over GPT-3.5 (OpenAI's previously best model, which had powered the initial release of ChatGPT). However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason. This position paper discusses the nature of reasoning; criticizes the current formulation of reasoning problems in the NLP community, as well as the way in which LLM reasoning performance is currently evaluated; introduces a small collection of 21 diverse reasoning problems; and performs a detailed qualitative evaluation of GPT-4's performance on those problems. Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.},
  copyright      = {Creative Commons Attribution 4.0 International},
  doi            = {10.48550/ARXIV.2308.03762},
  keywords       = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio1},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Misc{2018_Howard_Misc,
  author         = {Howard, Jeremy and Ruder, Sebastian},
  title          = {{U}niversal {L}anguage {M}odel {F}ine-tuning for {T}ext {C}lassification},
  year           = {2018},
  abstract       = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.1801.06146},
  keywords       = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio1},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Misc{2023_Deusser_Misc,
  author         = {Deußer, Tobias and Zhao, Cong and Krämer, Wolfgang and Leonhard, David and Bauckhage, Christian and Sifa, Rafet},
  title          = {{C}ontrolled {R}andomness {I}mproves the {P}erformance of {T}ransformer {M}odels},
  year           = {2023},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.2310.13526},
  file           = {:2023_Deusser_Misc.pdf:PDF},
  groups         = {MachineLearning, Language Processing},
  keywords       = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Misc{2023_Hao_Misc,
  author         = {Hao, Zengguang and Zhang, Jie and Xu, Binxia and Wang, Yafang and de Melo, Gerard and Li, Xiaolong},
  title          = {{I}ntent{D}ial: {A}n {I}ntent {G}raph based {M}ulti-{T}urn {D}ialogue {S}ystem with {R}easoning {P}ath {V}isualization},
  year           = {2023},
  abstract       = {Intent detection and identification from multi-turn dialogue has become a widely explored technique in conversational agents, for example, voice assistants and intelligent customer services. The conventional approaches typically cast the intent mining process as a classification task. Although neural classifiers have proven adept at such classification tasks, the issue of neural network models often impedes their practical deployment in real-world settings. We present a novel graph-based multi-turn dialogue system called , which identifies a user's intent by identifying intent elements and a standard query from a dynamically constructed and extensible intent graph using reinforcement learning. In addition, we provide visualization components to monitor the immediate reasoning path for each turn of a dialogue, which greatly facilitates further improvement of the system.},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.2310.11818},
  file           = {:2023_Hao_Misc.pdf:PDF},
  keywords       = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Misc{2017_Leike_Misc,
  author         = {Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A. and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
  title          = {{AI} {S}afety {G}ridworlds},
  year           = {2017},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.1711.09883},
  file           = {:2017_Leike_Misc.pdf:PDF},
  groups         = {ReinforcementLearning, Alex Roucka},
  keywords       = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
  readstatus     = {read},
}

@Misc{2022_Alayrac_Misc,
  author         = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  title          = {{F}lamingo: a {V}isual {L}anguage {M}odel for {F}ew-{S}hot {L}earning},
  year           = {2022},
  abstract       = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.2204.14198},
  file           = {:2022_Alayrac_Misc.pdf:PDF},
  groups         = {Language Processing, MachineLearning, COMFORT, Deep Mind},
  keywords       = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
  readstatus     = {read},
}

@Misc{2018_Devlin_Misc,
  author         = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title          = {{BERT}: {P}re-training of {D}eep {B}idirectional {T}ransformers for {L}anguage {U}nderstanding},
  year           = {2018},
  abstract       = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.1810.04805},
  file           = {:2018_Devlin_Misc.pdf:PDF},
  groups         = {COMFORT, Language Processing},
  keywords       = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio1},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@Article{2024_Bressem_Article,
  author    = {Keno K. Bressem and Jens-Michalis Papaioannou and Paul Grundmann and Florian Borchert and Lisa C. Adams and Leonhard Liu and Felix Busch and Lina Xu and Jan P. Loyen and Stefan M. Niehues and Moritz Augustin and Lennart Grosser and Marcus R. Makowski and Hugo J. W .L. Aerts and Alexander Löser},
  journal   = {Expert Systems with Applications},
  title     = {{medBERT}.de: {A} comprehensive {G}erman {BERT} model for the medical domain},
  year      = {2024},
  month     = {mar},
  pages     = {121598},
  volume    = {237},
  doi       = {10.1016/j.eswa.2023.121598},
  publisher = {Elsevier {BV}},
}

@Misc{2022_Hoffmann_Misc,
  author         = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  title          = {{T}raining {C}ompute-{O}ptimal {L}arge {L}anguage {M}odels},
  year           = {2022},
  abstract       = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4× more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.},
  comment        = {70B Chinchilla language model},
  copyright      = {arXiv.org perpetual, non-exclusive license},
  doi            = {10.48550/ARXIV.2203.15556},
  file           = {:2022_Hoffmann_Misc.pdf:PDF},
  groups         = {Language Processing, MachineLearning},
  keywords       = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  priority       = {prio2},
  publisher      = {arXiv},
  qualityassured = {qualityAssured},
}

@InProceedings{2023_Hagemann_InProceedings,
  author         = {Johannes Hagemann and Samuel Weinbach and Konstantin Dobler and Maximilian Schall and Gerard de Melo},
  booktitle      = {Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@NeurIPS 2023)},
  title          = {{E}fficient {P}arallelization {L}ayouts for {L}arge-{S}cale {D}istributed {M}odel {T}raining},
  year           = {2023},
  file           = {:2023_Hagemann_InProceedings.pdf:PDF},
  groups         = {MachineLearning},
  qualityassured = {qualityAssured},
  url            = {https://openreview.net/forum?id=Y0AHNkVDeu},
}

@Misc{2020_Dosovitskiy_Misc,
  author          = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  title           = {{A}n {I}mage is {W}orth 16x16 {W}ords: {T}ransformers for {I}mage {R}ecognition at {S}cale},
  year            = {2020},
  abstract        = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  comment-fabrice = {https://docs.monai.io/en/stable/networks.html#vit ViT paper Vision Transformer of MONAI},
  copyright       = {arXiv.org perpetual, non-exclusive license},
  doi             = {10.48550/ARXIV.2010.11929},
  groups          = {COMFORT, MachineLearning},
  keywords        = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher       = {arXiv},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Analysis\;0\;1\;0x0000ffff\;MATH_INTEGRAL_BOX\;\;;
1 StaticGroup:Graphtheory\;0\;1\;0x008000ff\;GRAPHQL\;\;;
1 StaticGroup:MachineLearning\;0\;1\;0xffff00ff\;ROBOT_EXCITED_OUTLINE\;\;;
2 StaticGroup:ClusteringAlgorithms\;0\;1\;0xe64d4dff\;UNGROUP\;\;;
2 StaticGroup:ConvolutionNetworks\;0\;1\;0xff9966ff\;LAYERS_TRIPLE_OUTLINE\;\;;
2 StaticGroup:GraphKernels\;0\;1\;0x4d804dff\;GRAPH_OUTLINE\;\;;
2 StaticGroup:Language Processing\;0\;1\;0xff00ffff\;ALPHABETICAL_VARIANT\;NLP, LLM and other\;;
2 StaticGroup:ReinforcementLearning\;0\;0\;0x808000ff\;ROBOT\;\;;
2 StaticGroup:SelfSupervisedLearning\;0\;1\;0x996699ff\;TEACH\;\;;
2 StaticGroup:SupportVectorMachines\;0\;0\;0xe6b34dff\;AB_TESTING\;\;;
1 StaticGroup:Medical\;0\;1\;0x000000ff\;MEDICAL_BAG\;\;;
1 StaticGroup:Projects\;0\;1\;0xffffffff\;ACCOUNT_GROUP\;Subgroups based on Projects\;;
2 StaticGroup:Alex Roucka\;0\;1\;0xffffffff\;\;His Bachelor Thesis based on AI Gridworlds, Reinforcement Learning, Q Learning, Relative Reachability and Attainable Utitlity Preservation\;;
2 StaticGroup:MasterThesis_ComputerScience\;0\;1\;0xffffffff\;BOOK_EDUCATION\;Most relevant papers\;;
2 StaticGroup:Special_Researchers\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:HPI\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:COMFORT\;0\;1\;0xffffffff\;MDI_ADJUST\;\;;
3 StaticGroup:Charité_other\;0\;1\;0xffffffff\;MDI_CALENDAR_PLUS\;\;;
3 StaticGroup:LexFridman\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:MaxPlanck\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Neuroscience&ML\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:TU-Berlin-Wäldchen\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:YanLeCun\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Deep Mind\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Weisfeiler-Lehman-Related\;0\;1\;0xffffffff\;UNGROUP\;\;;
}
